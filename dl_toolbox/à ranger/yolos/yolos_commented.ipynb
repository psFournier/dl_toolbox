{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d9ed1db-1992-4d10-849c-bb7931895e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torchvision\n",
    "from timm.layers import resample_abs_pos_embed     \n",
    "import torch.nn as nn\n",
    "from torchvision.ops import box_convert, generalized_box_iou\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.tv_tensors import BoundingBoxFormat\n",
    "from dl_toolbox.utils import *\n",
    "\n",
    "\n",
    "class SetCriterion(nn.Module):\n",
    "    def __init__(self, num_classes, eos_coef):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            eos_coef: relative classification weight applied to the no-object category\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eos_coef = eos_coef\n",
    "        empty_weight = torch.ones(self.num_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "    def loss_labels(self, outputs, targets, matches, num_boxes, log=True):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            matches: list of batch_size pairs (I, J) of arrays such that output bbox I[n] must be matched with target bbox J[n]\n",
    "        \"\"\"\n",
    "                \n",
    "        # all N tgt labels are reordered following the matches and concatenated  \n",
    "        reordered_labels = [t[\"labels\"][J] for t, (_, J) in zip(targets, matches)]\n",
    "        reordered_labels = torch.cat(reordered_labels) # Nx1\n",
    "        print(f'{reordered_labels.shape =}')\n",
    "        \n",
    "        # batch_idxs[i] is the idx in the batch of the img to which the i-th elem in the new order corresponds\n",
    "        batch_idxs = [torch.full_like(pred, i) for i, (pred, _) in enumerate(matches)]\n",
    "        batch_idxs = torch.cat(batch_idxs) # Nx1\n",
    "        \n",
    "        # src_idxs[i] is the idx of the preds for img batch_idxs[i] to which the i-th elem in the new order corresponds\n",
    "        pred_idxs = torch.cat([pred for (pred, _) in matches]) # Nx1\n",
    "        \n",
    "        # target_classes is of shape batch_size x num det tokens, and is num_classes (=no_obj) everywhere, except for each token that is matched to a tgt, where it is the label of the matched tgt\n",
    "        pred_logits = outputs['pred_logits'] #BxNdetTokxNcls\n",
    "        target_classes = torch.full(\n",
    "            pred_logits.shape[:2], #BxNdetTok\n",
    "            self.num_classes, #Filled with num_cls\n",
    "            dtype=torch.int64, \n",
    "            device=pred_logits.device\n",
    "        )\n",
    "        target_classes[(batch_idxs, pred_idxs)] = reordered_labels\n",
    "        loss_ce = F.cross_entropy(\n",
    "            pred_logits.transpose(1, 2), #BxNclsxd1xd2...\n",
    "            target_classes, #Bxd1xd2...\n",
    "            self.empty_weight\n",
    "        )\n",
    "        \n",
    "        ## If we did as follows, then there would be no incentive for the network to output small logits for non-matched tokens\n",
    "        #reordered_pred_logits = pred_logits[(batch_idxs, pred_idxs)] # NxNcls\n",
    "        #other_loss_ce = F.cross_entropy(\n",
    "        #    reordered_pred_logits,\n",
    "        #    reordered_labels\n",
    "        #)\n",
    "        \n",
    "        losses = {'loss_ce': loss_ce}\n",
    "        return losses\n",
    "\n",
    "    def loss_boxes(self, outputs, targets, matches, num_boxes):\n",
    "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
    "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
    "        \"\"\"\n",
    "        reordered_target_boxes = [t['boxes'][i] for t, (_, i) in zip(targets, matches)]\n",
    "        reordered_target_boxes = torch.cat(reordered_target_boxes) # Nx4\n",
    "        #print(f'{reordered_target_boxes.shape =}')\n",
    "        \n",
    "        # batch_idxs[i] is the idx in the batch of the img to which the i-th elem in the new order corresponds\n",
    "        batch_idxs = [torch.full_like(pred, i) for i, (pred, _) in enumerate(matches)]\n",
    "        batch_idxs = torch.cat(batch_idxs) # Nx1\n",
    "        \n",
    "        # src_idxs[i] is the idx of the preds for img batch_idxs[i] to which the i-th elem in the new order corresponds\n",
    "        pred_idxs = torch.cat([pred for (pred, _) in matches]) # Nx1\n",
    "        \n",
    "        pred_boxes = outputs['pred_boxes'] # BxNdetTokx4\n",
    "        #print(f'{pred_boxes.shape =}')\n",
    "        reordered_pred_boxes = pred_boxes[(batch_idxs, pred_idxs)] # Nx4\n",
    "        #print(f'{reordered_pred_boxes.shape =}')\n",
    "\n",
    "        losses = {}\n",
    "        loss_bbox = F.l1_loss(\n",
    "            reordered_pred_boxes,\n",
    "            reordered_target_boxes,\n",
    "            reduction='none'\n",
    "        )\n",
    "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
    "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
    "            box_convert(reordered_pred_boxes, 'xywh', 'xyxy'),\n",
    "            box_convert(reordered_target_boxes, 'xywh', 'xyxy')))\n",
    "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
    "        return losses\n",
    "\n",
    "    def forward(self, outputs, targets, matches):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "            outputs: dict of tensors, see the output specification of the model for the format\n",
    "            targets: list of dicts, such that len(targets) == batch_size. The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "            matches: list of batch_size pairs (I, J) of arrays such that for pair (I,J) output bbox I[n] must be matched with target bbox J[n]\n",
    "        \"\"\"\n",
    "        # Compute the average (?) number of target boxes accross all nodes, for normalization purposes\n",
    "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
    "        device = next(iter(outputs.values())).device\n",
    "        num_boxes = torch.as_tensor(\n",
    "            [num_boxes],\n",
    "            dtype=torch.float,\n",
    "            device=device\n",
    "        )\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        losses.update(self.loss_labels(outputs, targets, matches, num_boxes))\n",
    "        losses.update(self.loss_boxes(outputs, targets, matches, num_boxes))\n",
    "        loss = sum(losses.values())\n",
    "        return loss    \n",
    "\n",
    "    \n",
    "class Yolos(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_list,\n",
    "        det_token_num,\n",
    "        backbone,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        pred_thresh,\n",
    "        tta=None,\n",
    "        sliding=None,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone,\n",
    "            pretrained=True,\n",
    "            dynamic_img_size=True #Deals with inputs of other size than pretraining\n",
    "        )\n",
    "        self.class_list = class_list\n",
    "        self.num_classes = len(class_list)\n",
    "        self.embed_dim = self.backbone.embed_dim \n",
    "        self.det_token_num = det_token_num\n",
    "        self.add_det_tokens_to_backbone()\n",
    "        self.class_embed = torchvision.ops.MLP(\n",
    "            self.embed_dim,\n",
    "            [self.embed_dim, self.embed_dim, self.num_classes+1]\n",
    "        ) #Num_classes + 1 to deal with no_obj category\n",
    "        self.bbox_embed = torchvision.ops.MLP(\n",
    "            self.embed_dim,\n",
    "            [self.embed_dim, self.embed_dim, 4]\n",
    "        )\n",
    "        self.loss = SetCriterion(\n",
    "            self.num_classes,\n",
    "            0.5 #Coeff of no-obj category\n",
    "        )\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.map_metric = MeanAveragePrecision(\n",
    "            box_format='xywh',\n",
    "            backend='faster_coco_eval'\n",
    "        )\n",
    "        self.sliding = sliding\n",
    "        self.pred_thresh = pred_thresh\n",
    "        \n",
    "    def add_det_tokens_to_backbone(self):\n",
    "        det_token = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                1,\n",
    "                self.det_token_num,\n",
    "                self.backbone.embed_dim\n",
    "            )\n",
    "        )\n",
    "        self.det_token = torch.nn.init.trunc_normal_(\n",
    "            det_token,\n",
    "            std=.02\n",
    "        )\n",
    "        det_pos_embed = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                1,\n",
    "                self.det_token_num,\n",
    "                self.backbone.embed_dim\n",
    "            )\n",
    "        )\n",
    "        self.det_pos_embed = torch.nn.init.trunc_normal_(\n",
    "            det_pos_embed,\n",
    "            std=.02\n",
    "        )\n",
    "        #The ViT needs to know how many input tokens are not for patch embeddings\n",
    "        self.backbone.num_prefix_tokens += self.det_token_num\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        trainable = lambda p: p[1].requires_grad\n",
    "        train_params = list(filter(trainable, self.named_parameters()))\n",
    "        nb_train = sum([int(torch.numel(p[1])) for p in train_params])\n",
    "        nb_tot = sum([int(torch.numel(p)) for p in self.parameters()])\n",
    "        print(f\"Training {nb_train} trainable params out of {nb_tot}\")\n",
    "        #if hasattr(self, 'no_weight_decay'):\n",
    "        #    wd_val = 0. #self.optimizer.weight_decay\n",
    "        #    nwd_params = self.no_weight_decay()\n",
    "        #    train_params = param_groups_weight_decay(train_params, wd_val, nwd_params)\n",
    "        #    print(f\"{len(train_params[0]['params'])} are not affected by weight decay.\")\n",
    "        optimizer = self.optimizer(params=[p[1] for p in train_params])\n",
    "        scheduler = self.scheduler(optimizer)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def raw_logits_and_bboxs(self, x):\n",
    "        \"\"\" This code relies on class_token=True in ViT\n",
    "        \"\"\"\n",
    "        x = self.backbone.patch_embed(x)\n",
    "        \n",
    "        # Inserting position embedding for detection tokens and resampling if dynamic\n",
    "        cls_pos_embed = self.backbone.pos_embed[:, 0, :][:,None] # size 1x1xembed_dim\n",
    "        patch_pos_embed = self.backbone.pos_embed[:, 1:, :] # 1xnum_patchxembed_dim\n",
    "        pos_embed = torch.cat((cls_pos_embed, self.det_pos_embed, patch_pos_embed), dim=1)\n",
    "        if self.backbone.dynamic_img_size:\n",
    "            B, H, W, C = x.shape\n",
    "            pos_embed = resample_abs_pos_embed(\n",
    "                pos_embed,\n",
    "                (H, W),\n",
    "                num_prefix_tokens=self.backbone.num_prefix_tokens,\n",
    "            )\n",
    "            x = x.view(B, -1, C)\n",
    "            \n",
    "        # Inserting detection tokens    \n",
    "        cls_token = self.backbone.cls_token.expand(x.shape[0], -1, -1) \n",
    "        det_token = self.det_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat([cls_token, det_token, x], dim=1)\n",
    "        \n",
    "        # Forward ViT\n",
    "        x += pos_embed\n",
    "        x = self.backbone.pos_drop(x)\n",
    "        x = self.backbone.patch_drop(x)\n",
    "        x = self.backbone.norm_pre(x)\n",
    "        x = self.backbone.blocks(x)\n",
    "        x = self.backbone.norm(x)\n",
    "        \n",
    "        # Extracting processed detection tokens + forward heads\n",
    "        x = x[:,1:1+self.det_token_num,...]\n",
    "        outputs_class = self.class_embed(x)\n",
    "        outputs_coord = self.bbox_embed(x).sigmoid()\n",
    "        \n",
    "        out = {'pred_logits': outputs_class, 'pred_boxes': outputs_coord}\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def forward(self, x, sliding=None):\n",
    "        if sliding is not None:\n",
    "            auxs = [self.forward(aux) for aux in sliding(x)]\n",
    "            return sliding.merge(auxs)\n",
    "        else:\n",
    "            return self.raw_logits_and_bboxs(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def hungarian_matching(self, outputs, targets):\n",
    "        \"\"\" \n",
    "        Params:\n",
    "            outputs=dict:\n",
    "                 \"pred_logits\": Tensor of dim [B, num_queries, num_classes] with the class logits\n",
    "                 \"pred_boxes\": Tensor of dim [B, num_queries, 4] with the pred box coord\n",
    "            targets=list (len(targets) = batch_size) of dicts, each dict:\n",
    "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth objects in the target) containing the class labels\n",
    "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coord\n",
    "\n",
    "        Returns:\n",
    "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "                - index_i is the indices of the selected predictions (in order)\n",
    "                - index_j is the indices of the corresponding selected targets (in order)\n",
    "            For each batch element, it holds:\n",
    "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "        \"\"\"\n",
    "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "        # Also concat the target labels and boxes\n",
    "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "            \n",
    "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "        # but approximate it in 1 - proba[target class].\n",
    "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "        cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "        # Compute the L1 cost between boxes\n",
    "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "        \n",
    "        # Compute the giou cost betwen boxes\n",
    "        out_bbox = box_convert(out_bbox, 'xywh', 'xyxy')\n",
    "        tgt_bbox = box_convert(tgt_bbox, 'xywh', 'xyxy')\n",
    "        cost_giou = -generalized_box_iou(out_bbox, tgt_bbox)\n",
    "\n",
    "        # Final cost matrix\n",
    "        C = cost_bbox + cost_class + cost_giou\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "        \n",
    "        # Finds the minimum cost detection token/target assignment per img\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        int64 = lambda x: torch.as_tensor(x, dtype=torch.int64)\n",
    "        return [(int64(i), int64(j)) for i, j in indices]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def post_process(self, outputs, images):\n",
    "        b,c,h,w = images.shape\n",
    "        target_sizes = torch.Tensor((h,w)).repeat((b,1)) # shape 2 -> bx2\n",
    "        prob = F.softmax(outputs['pred_logits'], -1) # bxNdetTokxNcls\n",
    "        # Most prob cls (except no-obj) and its score per img per token\n",
    "        scores, labels = prob[..., :-1].max(-1) # bxNdetTok\n",
    "        img_h, img_w = target_sizes.unbind(dim=1) # separates heights and widths, shapes b\n",
    "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)[:, None, :] # bx1x4\n",
    "        out_bbox = outputs['pred_boxes'] # bxNdetTokx4\n",
    "        # This scaling should work whether out_bb have meaning in xyxy or xywh format \n",
    "        boxes = out_bbox * scale_fct.to(out_bbox.device)\n",
    "        results = [{'scores': s, 'labels': l, 'boxes': b}\n",
    "                   for s, l, b in zip(scores, labels, boxes)]\n",
    "        return results\n",
    "    \n",
    "    def norm_bb(self, inpt_bb):\n",
    "        \"\"\"\n",
    "        Convert bb to xyxy format for normalization and back to their initial format (xywh?)\n",
    "        \"\"\"\n",
    "        tensor_bb = inpt_bb.as_subclass(torch.Tensor)\n",
    "        bb = tensor_bb.clone() if tensor_bb.is_floating_point() else tensor_bb.float()\n",
    "        xyxy_bb = v2F.convert_bounding_box_format(\n",
    "            bb, old_format=inpt_bb.format, new_format=tv_tensors.BoundingBoxFormat.XYXY, inplace=True\n",
    "        )\n",
    "        xyxy_bb[..., 0::2].div_(inpt_bb.canvas_size[1])\n",
    "        xyxy_bb[..., 1::2].div_(inpt_bb.canvas_size[0])\n",
    "        out_bb = v2F.convert_bounding_box_format(\n",
    "            xyxy_bb, old_format=BoundingBoxFormat.XYXY, new_format=inpt_bb.format, inplace=True\n",
    "        )\n",
    "        return tv_tensors.wrap(out_bb, like=inpt_bb)\n",
    "    \n",
    "    def norm_targets(self, targets):\n",
    "        return [{'labels':t['labels'], 'boxes':self.norm_bb(t['boxes'])} for t in targets]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, targets, paths = batch[\"sup\"]\n",
    "        outputs = self.raw_logits_and_bboxs(x)\n",
    "        # Out bb corners come from a sigmoid layer, so we need tgt bb to be in [0,1] too\n",
    "        norm_tgts = self.norm_targets(targets) \n",
    "        matches = self.hungarian_matching(outputs, norm_tgts)\n",
    "        loss = self.loss(outputs, norm_tgts, matches)\n",
    "        self.log(f\"loss/train\", loss.detach().item())\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, targets, paths = batch\n",
    "        outputs = self.forward(x, sliding=self.sliding)\n",
    "        norm_tgts = self.norm_targets(targets)\n",
    "        matches = self.hungarian_matching(outputs, norm_tgts)\n",
    "        loss = self.loss(outputs, norm_tgts, matches)\n",
    "        self.log(f\"Total loss/val\", loss.detach().item())\n",
    "        # map_metric reuires a certain form for predictions\n",
    "        preds = self.post_process(outputs, x)\n",
    "        # map_metric does not work yet with tv_tensors\n",
    "        targets = [{'labels':t['labels'], 'boxes':t['boxes'].as_subclass(torch.Tensor)} for t in targets]\n",
    "        # self.map_metric works with bb in xywh format\n",
    "        # Make sure your dataset outputs targets in XYWH format\n",
    "        self.map_metric.update(preds, targets)\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        mapmetric = self.map_metric.compute()['map']\n",
    "        self.log(\"map/val\", mapmetric)\n",
    "        print(\"\\nMAP: \", mapmetric)\n",
    "        self.map_metric.reset()\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, targets, paths = batch\n",
    "        outputs = self.forward(x, sliding=self.sliding)\n",
    "        preds = self.post_process(outputs, x)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93db61f5-afb1-4bc2-839e-683d781b074c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_predict_batches=1)` was configured so 1 batch will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.94s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=2.72s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                 | Params\n",
      "------------------------------------------------------\n",
      "0 | backbone     | VisionTransformer    | 22.4 M\n",
      "1 | class_embed  | MLP                  | 296 K \n",
      "2 | bbox_embed   | MLP                  | 297 K \n",
      "3 | loss         | SetCriterion         | 0     \n",
      "4 | map_metric   | MeanAveragePrecision | 0     \n",
      "  | other params | n/a                  | 7.7 K \n",
      "------------------------------------------------------\n",
      "896 K     Trainable params\n",
      "22.1 M    Non-trainable params\n",
      "23.0 M    Total params\n",
      "91.810    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Training 896262 trainable params out of 22952454\n",
      "Sanity Checking DataLoader 0:   0%|                                                                                        | 0/1 [00:00<?, ?it/s]reordered_labels.shape =torch.Size([3])\n",
      "Sanity Checking DataLoader 0: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 66.07it/s]\n",
      "MAP:  tensor(0.)\n",
      "Epoch 0:   0%|                                                                                                             | 0/1 [00:00<?, ?it/s]reordered_labels.shape =torch.Size([10])\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.41it/s, v_num=1]\n",
      "Validation: |                                                                                                              | 0/? [00:00<?, ?it/s]\u001b[Areordered_labels.shape =torch.Size([3])\n",
      "\n",
      "MAP:  tensor(0.)\n",
      "Epoch 1:   0%|                                                                                                    | 0/1 [00:00<?, ?it/s, v_num=1]reordered_labels.shape =torch.Size([0])\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.43it/s, v_num=1]\n",
      "Validation: |                                                                                                              | 0/? [00:00<?, ?it/s]\u001b[Areordered_labels.shape =torch.Size([3])\n",
      "\n",
      "MAP:  tensor(0.)\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.97it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.43it/s, v_num=1]\n",
      "loading annotations into memory...\n",
      "Done (t=3.20s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Predicting: |                                                                                                              | 0/? [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from dl_toolbox.callbacks import ProgressBar\n",
    "from dl_toolbox import datamodules\n",
    "from dl_toolbox import modules\n",
    "import torchvision.transforms.v2 as v2\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from dl_toolbox.callbacks import ProgressBar, Finetuning, Lora\n",
    "from functools import partial\n",
    "import gc\n",
    "\n",
    "\n",
    "train_tf = v2.Compose(\n",
    "    [\n",
    "        v2.RandomCrop(224),\n",
    "        v2.SanitizeBoundingBoxes(),\n",
    "        v2.Normalize([0.5]*3, [0.5]*3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_tf = v2.Compose(\n",
    "    [\n",
    "        v2.CenterCrop(224),\n",
    "        v2.SanitizeBoundingBoxes(),\n",
    "        v2.Normalize([0.5]*3, [0.5]*3)\n",
    "    ]\n",
    ")\n",
    " \n",
    "dm = datamodules.xView(\n",
    "    data_path='/data',\n",
    "    merge='building',\n",
    "    train_tf=train_tf,\n",
    "    test_tf=test_tf,\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "lora = Lora('backbone', 4, True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=2,\n",
    "    limit_train_batches=1,\n",
    "    limit_val_batches=1,\n",
    "    limit_predict_batches=1,\n",
    "    callbacks=[ProgressBar(), lora]\n",
    ")\n",
    "\n",
    "module = Yolos(\n",
    "    class_list=dm.class_list,\n",
    "    det_token_num=10,\n",
    "    backbone='vit_small_patch14_dinov2',\n",
    "    optimizer=partial(torch.optim.Adam, lr=0.001),\n",
    "    scheduler=partial(torch.optim.lr_scheduler.ConstantLR, factor=1),\n",
    "    pred_thresh=0.1,\n",
    "    sliding=None\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "trainer.fit(\n",
    "    module,\n",
    "    datamodule=dm,\n",
    ")\n",
    "\n",
    "trainer.predict(\n",
    "    module,\n",
    "    datamodule=dm,\n",
    "    #ckpt_path='/data/outputs/coco_yolos/2024-06-05_180011/0/checkpoints/last.ckpt',\n",
    "    return_predictions=False\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6386d31-5354-46b1-9e80-710eaf87d4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
