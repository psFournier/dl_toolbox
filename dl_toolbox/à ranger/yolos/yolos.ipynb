{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d9ed1db-1992-4d10-849c-bb7931895e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torchvision\n",
    "from timm.layers import resample_abs_pos_embed     \n",
    "import torch.nn as nn\n",
    "from torchvision.ops import box_convert, generalized_box_iou\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.tv_tensors import BoundingBoxFormat\n",
    "from dl_toolbox.utils import *\n",
    "\n",
    "\n",
    "class SetCriterion(nn.Module):\n",
    "    def __init__(self, num_classes, eos_coef):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            matcher: module able to compute a matching between targets and proposals\n",
    "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
    "            eos_coef: relative classification weight applied to the no-object category\n",
    "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eos_coef = eos_coef\n",
    "        empty_weight = torch.ones(self.num_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n",
    "        src_logits = outputs['pred_logits']\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "        return losses\n",
    "\n",
    "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
    "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
    "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
    "        \"\"\"\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_boxes = outputs['pred_boxes'][idx]\n",
    "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
    "        losses = {}\n",
    "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
    "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
    "            box_convert(src_boxes, 'xywh', 'xyxy'),\n",
    "            box_convert(target_boxes, 'xywh', 'xyxy')))\n",
    "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "    \n",
    "    def norm_targets(self, targets):\n",
    "        return [{'labels':t['labels'], 'boxes':to_xywh(norm(to_xyxy(t['boxes'])))} for t in targets]\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "             outputs: dict of tensors, see the output specification of the model for the format\n",
    "             targets: list of dicts, such that len(targets) == batch_size.\n",
    "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "        \"\"\"\n",
    "        targets = self.norm_targets(targets)\n",
    "        matches = self.hungarian_matching(outputs, targets)\n",
    "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
    "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
    "        dev = next(iter(outputs.values())).device\n",
    "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=dev)\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        losses.update(self.loss_labels(outputs, targets, matches, num_boxes))\n",
    "        losses.update(self.loss_boxes(outputs, targets, matches, num_boxes))\n",
    "        loss = sum(losses.values())\n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def hungarian_matching(self, outputs, targets):\n",
    "        \"\"\" \n",
    "        Params:\n",
    "            outputs=dict:\n",
    "                 \"pred_logits\": Tensor of dim [B, num_queries, num_classes] with the class logits\n",
    "                 \"pred_boxes\": Tensor of dim [B, num_queries, 4] with the pred box coord\n",
    "            targets=list (len(targets) = batch_size) of dicts, each dict:\n",
    "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth objects in the target) containing the class labels\n",
    "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coord\n",
    "\n",
    "        Returns:\n",
    "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "                - index_i is the indices of the selected predictions (in order)\n",
    "                - index_j is the indices of the corresponding selected targets (in order)\n",
    "            For each batch element, it holds:\n",
    "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "        \"\"\"\n",
    "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "        # Also concat the target labels and boxes\n",
    "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "            \n",
    "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "        # but approximate it in 1 - proba[target class].\n",
    "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "        cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "        # Compute the L1 cost between boxes\n",
    "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "        \n",
    "        # Compute the giou cost betwen boxes\n",
    "        out_bbox = box_convert(out_bbox, 'xywh', 'xyxy')\n",
    "        tgt_bbox = box_convert(tgt_bbox, 'xywh', 'xyxy')\n",
    "        cost_giou = -generalized_box_iou(out_bbox, tgt_bbox)\n",
    "\n",
    "        # Final cost matrix\n",
    "        C = cost_bbox + cost_class + cost_giou\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "    \n",
    "class Yolos(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        det_token_num,\n",
    "        backbone,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        pred_thresh,\n",
    "        tta=None,\n",
    "        sliding=None,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained=True, dynamic_img_size=True)\n",
    "        hdim = self.backbone.embed_dim \n",
    "        self.det_token_num = det_token_num\n",
    "        self.add_det_tokens_to_backbone()\n",
    "        self.class_embed = torchvision.ops.MLP(hdim, [hdim,hdim,num_classes+1])\n",
    "        self.bbox_embed = torchvision.ops.MLP(hdim, [hdim,hdim,4])\n",
    "        self.loss = SetCriterion(num_classes, 0.5)\n",
    "        self.num_classes = num_classes\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.map_metric = MeanAveragePrecision(box_format='xywh', backend='faster_coco_eval')\n",
    "        self.sliding = sliding\n",
    "        self.pred_thresh = pred_thresh\n",
    "        \n",
    "    def add_det_tokens_to_backbone(self):\n",
    "        det_token = nn.Parameter(torch.zeros(1, self.det_token_num, self.backbone.embed_dim))\n",
    "        self.det_token = torch.nn.init.trunc_normal_(det_token, std=.02)\n",
    "        det_pos_embed = nn.Parameter(torch.zeros(1, self.det_token_num, self.backbone.embed_dim))\n",
    "        self.det_pos_embed = torch.nn.init.trunc_normal_(det_pos_embed, std=.02)\n",
    "        self.backbone.num_prefix_tokens += self.det_token_num\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        train_params = list(filter(lambda p: p[1].requires_grad, self.named_parameters()))\n",
    "        nb_train = sum([int(torch.numel(p[1])) for p in train_params])\n",
    "        nb_tot = sum([int(torch.numel(p)) for p in self.parameters()])\n",
    "        print(f\"Training {nb_train} params out of {nb_tot}\")\n",
    "        optimizer = self.optimizer(params=[p[1] for p in train_params])\n",
    "        scheduler = self.scheduler(optimizer)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def raw_logits_and_bboxs(self, x):\n",
    "        x = self.backbone.patch_embed(x)\n",
    "        # If cls token in backbone\n",
    "        cls_pos_embed = self.backbone.pos_embed[:, 0, :][:,None] # size 1x1xembed_dim\n",
    "        patch_pos_embed = self.backbone.pos_embed[:, 1:, :] # 1xnum_patchxembed_dim\n",
    "        pos_embed = torch.cat((cls_pos_embed, self.det_pos_embed, patch_pos_embed), dim=1)\n",
    "        if self.backbone.dynamic_img_size:\n",
    "            B, H, W, C = x.shape\n",
    "            pos_embed = resample_abs_pos_embed(\n",
    "                pos_embed,\n",
    "                (H, W),\n",
    "                num_prefix_tokens=self.backbone.num_prefix_tokens,\n",
    "            )\n",
    "            x = x.view(B, -1, C)\n",
    "        # If cls token in backbone\n",
    "        cls_token = self.backbone.cls_token.expand(x.shape[0], -1, -1) \n",
    "        det_token = self.det_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat([cls_token, det_token, x], dim=1)\n",
    "        x += pos_embed\n",
    "        x = self.backbone.pos_drop(x)\n",
    "        x = self.backbone.patch_drop(x)\n",
    "        x = self.backbone.norm_pre(x)\n",
    "        x = self.backbone.blocks(x)\n",
    "        x = self.backbone.norm(x)\n",
    "        x = x[:,1:1+self.det_token_num,...]\n",
    "        outputs_class = self.class_embed(x)\n",
    "        outputs_coord = self.bbox_embed(x).sigmoid()\n",
    "        out = {'pred_logits': outputs_class, 'pred_boxes': outputs_coord}\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x, sliding=None):\n",
    "        if sliding is not None:\n",
    "            auxs = [self.forward(aux) for aux in sliding(x)]\n",
    "            return sliding.merge(auxs)\n",
    "        else:\n",
    "            return self.raw_logits_and_bboxs(x)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def post_process(self, outputs, images):\n",
    "        out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']\n",
    "        b,c,h,w = images.shape\n",
    "        xywh = [tv_tensors.BoundingBoxes(\n",
    "            bb,\n",
    "            format=tv_tensors.BoundingBoxFormat.XYWH,\n",
    "            canvas_size=(h,w)\n",
    "        ) for bb in out_bbox]\n",
    "        xyxy = [unnorm(to_xyxy(bb)) for bb in xywh]\n",
    "        \n",
    "        prob = F.softmax(out_logits, -1)\n",
    "        scores, labels = prob[..., :-1].max(-1)\n",
    "\n",
    "        if self.sliding:\n",
    "            idxs = [torchvision.ops.nms(bb, s, 0.8) for bb, s in zip(xyxy, scores)]\n",
    "            xyxy = [bb[idx] for bb, idx in zip(xyxy, idxs)]\n",
    "            scores = [s[idx] for s, idx in zip(scores, idxs)]\n",
    "            labels = [l[idx] for l, idx in zip(labels, idxs)]\n",
    "\n",
    "        xywh = [to_xywh(bb, old_format='XYXY') for bb in xyxy]\n",
    "        \n",
    "        results = [{'scores': s, 'labels': l, 'boxes': bb} for s, l, bb in zip(scores, labels, xywh)]\n",
    "        return results\n",
    "    \n",
    "    def filter_preds(self, preds):\n",
    "        keep = [torch.where(p['scores']>self.pred_thresh) for p in preds]\n",
    "        filtered = [{\n",
    "            'scores': p['scores'][k], \n",
    "            'labels': p['labels'][k],\n",
    "            'boxes': p['boxes'][k]\n",
    "        } for p, k in zip(preds, keep)]\n",
    "        return filtered\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, targets, paths = batch[\"sup\"]\n",
    "        outputs = self.forward(x)\n",
    "        loss = self.loss(outputs, targets)\n",
    "        self.log(f\"loss/train\", loss.detach().item())\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, targets, paths = batch\n",
    "        outputs = self.forward(x, sliding=self.sliding)\n",
    "        loss = self.loss(outputs, targets)\n",
    "        self.log(f\"Total loss/val\", loss.detach().item())\n",
    "        preds = self.post_process(outputs, x)\n",
    "        self.map_metric.update(preds, targets)\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        mapmetric = self.map_metric.compute()['map']\n",
    "        self.log(\"map/val\", mapmetric)\n",
    "        print(\"\\nMAP: \", mapmetric)\n",
    "        self.map_metric.reset()\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, targets, paths = batch\n",
    "        outputs = self.forward(x, sliding=self.sliding)\n",
    "        preds = self.post_process(outputs, x)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "203530bf-f075-404b-92b6-a48c3d5ded0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dl_toolbox.utils import get_tiles\n",
    "\n",
    "class SlidingDet:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nols,\n",
    "        nrows,\n",
    "        width,\n",
    "        height,\n",
    "        step_w,\n",
    "        step_h\n",
    "    ):\n",
    "        self.nols = nols\n",
    "        self.nrows = nrows\n",
    "        self.tiles = list(get_tiles(nols, nrows, width, height, step_w, step_h))\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        imgs = []\n",
    "        for co, ro, w, h in self.tiles:\n",
    "            imgs.append(img[...,ro:ro+h,co:co+w])\n",
    "        return imgs\n",
    "    \n",
    "    def offset_bb(self, xyxy, co, ro):\n",
    "        xyxy[..., 0::2].add_(co)\n",
    "        xyxy[..., 1::2].add_(ro)\n",
    "        xyxy.canvas_size = (float(self.nrows), float(self.nols))\n",
    "        return xyxy\n",
    "    \n",
    "    def merge(self, outputs):\n",
    "        # outputs is a list of dictionaries, each dict contains logits (bs, nbb, numcls) and bboxs (bs, nbb, 4) for one tile of the initial batch of big images\n",
    "        # bbox format xywh, normalisé par rapport à la tile size\n",
    "        # obj : convert this list into a single dict with one logits tensor (bs, nbb * nb_tiles, numcls) and one bbx tensor (bs, nbb*ntiles, 4) where bbx are normalized xywh with respect to the full img size\n",
    "        logits = torch.cat([out['pred_logits'] for out in outputs], dim=1)\n",
    "        boxes = []\n",
    "        logits = []\n",
    "        for (co, ro, w, h), out in zip(self.tiles, outputs):\n",
    "            logits.append(out['pred_logits'])\n",
    "            tv_bb = [tv_tensors.BoundingBoxes(\n",
    "                bb,\n",
    "                format=tv_tensors.BoundingBoxFormat.XYWH,\n",
    "                canvas_size=(h,w)\n",
    "            ) for bb in out['pred_boxes']]\n",
    "            unnorm_tv_bb = [unnorm(to_xyxy(bb)) for bb in tv_bb]\n",
    "            offset_tv_bb = [self.offset_bb(bb, co, ro) for bb in unnorm_tv_bb]\n",
    "            norm_tv_bb = [norm(bb) for bb in offset_tv_bb]\n",
    "            bb = torch.stack([to_xywh(bb).as_subclass(torch.Tensor) for bb in norm_tv_bb])\n",
    "            boxes.append(bb)\n",
    "        all_logits = torch.cat(logits, dim=1)\n",
    "        all_bb = torch.cat(boxes, dim=1)\n",
    "        return {'pred_logits': all_logits, 'pred_boxes': all_bb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93db61f5-afb1-4bc2-839e-683d781b074c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_predict_batches=1)` was configured so 1 batch will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.46s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=2.71s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                 | Params\n",
      "------------------------------------------------------\n",
      "0 | backbone     | VisionTransformer    | 22.4 M\n",
      "1 | class_embed  | MLP                  | 331 K \n",
      "2 | bbox_embed   | MLP                  | 297 K \n",
      "3 | loss         | SetCriterion         | 0     \n",
      "4 | map_metric   | MeanAveragePrecision | 0     \n",
      "  | other params | n/a                  | 7.7 K \n",
      "------------------------------------------------------\n",
      "930 K     Trainable params\n",
      "22.1 M    Non-trainable params\n",
      "23.0 M    Total params\n",
      "91.948    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 930912 params out of 22987104\n",
      "Sanity Checking DataLoader 0:   0%|                                                                                                       | 0/1 [00:00<?, ?it/s][torch.Size([90, 4]), torch.Size([90, 4])]\n",
      "[torch.Size([4, 4]), torch.Size([4, 4])]\n",
      "Sanity Checking DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.46it/s]\n",
      "MAP:  tensor(-1.)\n",
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.19it/s, v_num=144]\n",
      "Validation: |                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A[torch.Size([90, 4]), torch.Size([90, 4])]\n",
      "[torch.Size([5, 4]), torch.Size([4, 4])]\n",
      "\n",
      "MAP:  tensor(-1.)\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.87it/s, v_num=144]\n",
      "Validation: |                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A[torch.Size([90, 4]), torch.Size([90, 4])]\n",
      "[torch.Size([6, 4]), torch.Size([5, 4])]\n",
      "\n",
      "MAP:  tensor(-1.)\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.90it/s, v_num=144]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.41it/s, v_num=144]\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=2.95s)\n",
      "creating index...\n",
      "index created!\n",
      "Predicting: |                                                                                                                             | 0/? [00:00<?, ?it/s][torch.Size([90, 4]), torch.Size([90, 4])]\n",
      "[torch.Size([5, 4]), torch.Size([6, 4])]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from dl_toolbox.callbacks import ProgressBar\n",
    "from dl_toolbox import datamodules\n",
    "from dl_toolbox import modules\n",
    "import torchvision.transforms.v2 as v2\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from dl_toolbox.callbacks import ProgressBar, Finetuning, Lora, TiffPredsWriter, CalibrationLogger\n",
    "from functools import partial\n",
    "import gc\n",
    "\n",
    "\n",
    "train_tf = v2.Compose(\n",
    "    [\n",
    "        v2.RandomCrop(224),\n",
    "        v2.SanitizeBoundingBoxes(),\n",
    "        v2.Normalize([0.5]*3, [0.5]*3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_tf = v2.Compose(\n",
    "    [\n",
    "        v2.CenterCrop(400),\n",
    "        v2.SanitizeBoundingBoxes(),\n",
    "        v2.Normalize([0.5]*3, [0.5]*3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "sliding = SlidingDet(\n",
    "    nols=400,\n",
    "    nrows=400,\n",
    "    width=224,\n",
    "    height=224,\n",
    "    step_w=112,\n",
    "    step_h=112\n",
    ")\n",
    " \n",
    "dm = datamodules.xView(\n",
    "    data_path='/data',\n",
    "    merge='building',\n",
    "    train_tf=train_tf,\n",
    "    test_tf=test_tf,\n",
    "    batch_tf=None,\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "lora = Lora('backbone', 4, True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=2,\n",
    "    limit_train_batches=1,\n",
    "    limit_val_batches=1,\n",
    "    limit_predict_batches=1,\n",
    "    callbacks=[ProgressBar(), lora]\n",
    ")\n",
    "\n",
    "module = Yolos(\n",
    "    num_classes=91,\n",
    "    det_token_num=10,\n",
    "    backbone='vit_small_patch14_dinov2',\n",
    "    optimizer=partial(torch.optim.Adam, lr=0.001),\n",
    "    scheduler=partial(torch.optim.lr_scheduler.ConstantLR, factor=1),\n",
    "    pred_thresh=0.1,\n",
    "    sliding=sliding\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "trainer.fit(\n",
    "    module,\n",
    "    datamodule=dm,\n",
    ")\n",
    "\n",
    "trainer.predict(\n",
    "    module,\n",
    "    datamodule=dm,\n",
    "    #ckpt_path='/data/outputs/coco_yolos/2024-06-05_180011/0/checkpoints/last.ckpt',\n",
    "    return_predictions=False\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6386d31-5354-46b1-9e80-710eaf87d4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
