{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fceb5c4-69c4-44be-9c01-ff64aafd5c77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=12.07s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from pathlib import Path\n",
    "data_path = Path(\"/data/coco\")\n",
    "\n",
    "ds = torchvision.datasets.CocoDetection(\n",
    "    data_path/\"train2017\",\n",
    "    data_path/\"annotations/instances_train2017.json\", \n",
    "    None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f63622-1062-4275-848e-742d211e367d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(img) = <class 'PIL.Image.Image'>\n",
      "type(target) = <class 'list'>\n",
      "type(target[0]) = <class 'dict'>\n",
      "target[0].keys() = dict_keys(['segmentation', 'area', 'iscrowd', 'image_id', 'bbox', 'category_id', 'id'])\n",
      "1038967\n",
      "type(img) = <class 'PIL.Image.Image'>\n",
      "type(target) = <class 'dict'>\n",
      "target.keys() = dict_keys(['boxes', 'masks', 'labels'])\n",
      "type(target['boxes']) = <class 'torchvision.tv_tensors._bounding_boxes.BoundingBoxes'>\n",
      "type(target['labels']) = <class 'torch.Tensor'>\n",
      "type(target['masks']) = <class 'torchvision.tv_tensors._mask.Mask'>\n",
      "BoundingBoxes([[  1.0800, 187.6900, 612.6700, 473.5300],\n",
      "               [311.7300,   4.3100, 631.0100, 232.9900],\n",
      "               [249.6000, 229.2700, 565.8400, 474.3500],\n",
      "               [  0.0000,  13.5100, 434.4800, 388.6300],\n",
      "               [376.2000,  40.3600, 451.7500,  86.8900],\n",
      "               [465.7800,  38.9700, 523.8500,  85.6400],\n",
      "               [385.7000,  73.6600, 469.7200, 144.1700],\n",
      "               [364.0500,   2.4900, 458.8100,  73.5600]], format=BoundingBoxFormat.XYXY, canvas_size=(480, 640))\n"
     ]
    }
   ],
   "source": [
    "sample = ds[0]\n",
    "img, target = sample\n",
    "print(f\"{type(img) = }\\n{type(target) = }\\n{type(target[0]) = }\\n{target[0].keys() = }\")\n",
    "print(target[0]['id'])\n",
    "\n",
    "dataset = torchvision.datasets.wrap_dataset_for_transforms_v2(ds, target_keys=(\"boxes\", \"labels\", \"masks\"))\n",
    "\n",
    "sample = dataset[0]\n",
    "img, target = sample\n",
    "print(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\n",
    "print(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\\n{type(target['masks']) = }\")\n",
    "print(target['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2339eb2c-f5dc-4dc1-aa8e-5c89d705d407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from pytorch_lightning.utilities import CombinedLoader\n",
    "from functools import partial\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "\n",
    "class Coco(LightningDataModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        train_tf,\n",
    "        test_tf,\n",
    "        batch_size_s,\n",
    "        steps_per_epoch,\n",
    "        num_workers,\n",
    "        pin_memory,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_path = Path(data_path)/\"coco\"\n",
    "        self.train_tf = train_tf\n",
    "        self.test_tf = test_tf\n",
    "        self.batch_size_s = batch_size_s\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        train_s_set = torchvision.datasets.CocoDetection(\n",
    "            self.data_path/\"train2017\",\n",
    "            self.data_path/\"annotations/instances_train2017.json\", \n",
    "            self.train_tf)\n",
    "        self.train_s_set = torchvision.datasets.wrap_dataset_for_transforms_v2(\n",
    "            train_s_set, target_keys=(\"boxes\", \"labels\"))\n",
    "        val_set = torchvision.datasets.CocoDetection(\n",
    "            self.data_path/\"val2017\",\n",
    "            self.data_path/\"annotations/instances_val2017.json\", \n",
    "            self.test_tf)\n",
    "        self.val_set = torchvision.datasets.wrap_dataset_for_transforms_v2(\n",
    "            val_set, target_keys=(\"boxes\", \"labels\"))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _collate(batch):\n",
    "        images_b, targets_b = list(zip(*batch))\n",
    "        boxes = [t['boxes'] for t in targets_b]\n",
    "        labels = [t['labels'] for t in targets_b]\n",
    "        # don't stack bb because each batch elem may not have the same nb of bb\n",
    "        return torch.stack(images_b), boxes, labels\n",
    "                \n",
    "    def _dataloader(self, dataset):\n",
    "        return partial(\n",
    "            DataLoader,\n",
    "            dataset=dataset,\n",
    "            collate_fn=self._collate,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory\n",
    "        )\n",
    "                       \n",
    "    def train_dataloader(self):\n",
    "        return self._dataloader(self.train_s_set)(\n",
    "            sampler=RandomSampler(\n",
    "                self.train_s_set,\n",
    "                replacement=True,\n",
    "                num_samples=self.steps_per_epoch*self.batch_size_s\n",
    "            ),\n",
    "            drop_last=True,\n",
    "            batch_size=self.batch_size_s\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self._dataloader(self.val_set)(\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            batch_size=self.batch_size_s\n",
    "        )\n",
    "    \n",
    "train_tf = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_tf = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize(size=(224, 224), antialias=True),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dm = Coco(\n",
    "    data_path='/data',\n",
    "    train_tf=train_tf,\n",
    "    test_tf=test_tf,\n",
    "    batch_size_s=4,\n",
    "    steps_per_epoch=10,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "#dm.prepare_data()\n",
    "#dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a53af98e-029b-45a7-b979-cb2caa4058fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from torchvision.ops.feature_pyramid_network import ExtraFPNBlock\n",
    "from torchvision.ops.misc import Conv2dNormActivation\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "from torch import nn, Tensor\n",
    "import timm\n",
    "from torchvision.ops.feature_pyramid_network import LastLevelP6P7\n",
    "from dl_toolbox.networks.fcos import Head\n",
    "import torch.nn as nn\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from einops import rearrange\n",
    "\n",
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    \"\"\" LayerNorm for channels of '2D' spatial NCHW tensors \"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6, affine=True):\n",
    "        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.layer_norm(\n",
    "            x.permute(0, 2, 3, 1), self.normalized_shape, self.weight, self.bias, self.eps).permute(0, 3, 1, 2)\n",
    "\n",
    "class SimpleFeaturePyramidNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Module that adds a Simple FPN from on top of a set of feature maps. This is based on\n",
    "    `\"Exploring Plain Vision Transformer Backbones for Object Detection\" <https://arxiv.org/abs/2203.16527>`_.\n",
    "\n",
    "    Unlike regular FPN, Simple FPN expects a single feature map,\n",
    "    on which the Simple FPN will be added.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of channels for the input feature map that\n",
    "            is passed to the module\n",
    "        out_channels (int): number of channels of the Simple FPN representation\n",
    "        extra_blocks (ExtraFPNBlock or None): if provided, extra operations will\n",
    "            be performed. It is expected to take the fpn features, the original\n",
    "            features and the names of the original features as input, and returns\n",
    "            a new list of feature maps and their corresponding names\n",
    "        norm_layer (callable, optional): Module specifying the normalization layer to use. Default: LayerNorm\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = torchvision.ops.SimpleFeaturePyramidNetwork(10, 5)\n",
    "        >>> # get some dummy data\n",
    "        >>> x = torch.rand(1, 10, 64, 64)\n",
    "        >>> # compute the Simple FPN on top of x\n",
    "        >>> output = m(x)\n",
    "        >>> print([(k, v.shape) for k, v in output.items()])\n",
    "        >>> # returns\n",
    "        >>>   [('feat0', torch.Size([1, 5, 64, 64])),\n",
    "        >>>    ('feat2', torch.Size([1, 5, 16, 16])),\n",
    "        >>>    ('feat3', torch.Size([1, 5, 8, 8]))]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        extra_blocks: Optional[ExtraFPNBlock] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        if in_channels <= 0:\n",
    "            raise ValueError(\"in_channels <= 0 is currently not supported\")\n",
    "\n",
    "        for block_index in range(1,4):\n",
    "            layers = []\n",
    "\n",
    "            current_in_channels = in_channels\n",
    "            if block_index == 0:\n",
    "                layers.extend([\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels,\n",
    "                        in_channels // 2,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                    norm_layer(in_channels // 2),\n",
    "                    nn.GELU(),\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels // 2,\n",
    "                        in_channels // 4,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                ])\n",
    "                current_in_channels = in_channels // 4\n",
    "            elif block_index == 1:\n",
    "                layers.append(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels,\n",
    "                        in_channels // 2,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                )\n",
    "                current_in_channels = in_channels // 2\n",
    "            elif block_index == 2:\n",
    "                # nothing to do for this scale\n",
    "                pass\n",
    "            elif block_index == 3:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "            layers.extend([\n",
    "                Conv2dNormActivation(\n",
    "                    current_in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=1,\n",
    "                    padding=0,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=None\n",
    "                ),\n",
    "                Conv2dNormActivation(\n",
    "                    out_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=None\n",
    "                )\n",
    "            ])\n",
    "\n",
    "            self.blocks.append(nn.Sequential(*layers))\n",
    "\n",
    "        if extra_blocks is not None:\n",
    "            if not isinstance(extra_blocks, ExtraFPNBlock):\n",
    "                raise TypeError(f\"extra_blocks should be of type ExtraFPNBlock not {type(extra_blocks)}\")\n",
    "        self.extra_blocks = extra_blocks\n",
    "\n",
    "    def forward(self, x: Tensor) -> Dict[str, Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the Simple FPN for a feature map.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): input feature map.\n",
    "\n",
    "        Returns:\n",
    "            results (list[Tensor]): feature maps after FPN layers.\n",
    "                They are ordered from highest resolution first.\n",
    "        \"\"\"\n",
    "        results = [block(x) for block in self.blocks]\n",
    "        names = [f\"{i}\" for i in range(len(self.blocks))]\n",
    "\n",
    "        if self.extra_blocks is not None:\n",
    "            results, names = self.extra_blocks(results, [x], names)\n",
    "\n",
    "        # make it back an OrderedDict\n",
    "        out = OrderedDict([(k, v) for k, v in zip(names, results)])\n",
    "\n",
    "        return out\n",
    "    \n",
    "x = torch.rand(1, 10, 64, 64)\n",
    "m = SimpleFeaturePyramidNetwork(10, 5, \n",
    "        extra_blocks=LastLevelP6P7(5,5),\n",
    "        norm_layer=LayerNorm2d)\n",
    "output = m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebba51df-e9c9-461c-b055-977f16969f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([32, 32]), torch.Size([16, 16]), torch.Size([8, 8]), torch.Size([4, 4]), torch.Size([2, 2])]\n"
     ]
    }
   ],
   "source": [
    "#from dl_toolbox.networks import FCOS\n",
    "#model = FCOS(num_classes=20)\n",
    "\n",
    "\n",
    "#print(get_graph_node_names(resnet50())[0])\n",
    "\n",
    "class ViTSimpleFPN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=19, out_channels=256):\n",
    "        super(ViTSimpleFPN, self).__init__()\n",
    "        self.backbone = timm.create_model('vit_small_patch14_dinov2', pretrained=True, dynamic_img_size=True)\n",
    "        self.sfpn = SimpleFeaturePyramidNetwork(\n",
    "            in_channels=384,\n",
    "            out_channels=256,\n",
    "            extra_blocks=LastLevelP6P7(256,256),\n",
    "            norm_layer=LayerNorm2d\n",
    "        )\n",
    "        \n",
    "        inp = torch.randn(2, 3, 224, 224)\n",
    "        with torch.no_grad():\n",
    "            out = self.forward_feat(inp)\n",
    "        self.feat_sizes = [o.shape[2:] for o in out.values()]\n",
    "        self.head = Head(out_channels, num_classes, n_feat_levels=6)\n",
    "    \n",
    "    def forward_feat(self, x):\n",
    "        H, W = x.size(2), x.size(3)\n",
    "        GS = H // self.backbone.patch_embed.patch_size[0]\n",
    "        x = self.backbone.forward_features(x)\n",
    "        x = x[:,self.backbone.num_prefix_tokens:,...]\n",
    "        x = rearrange(x, \"b (h w) c -> b c h w\", h=GS)\n",
    "        x = self.sfpn(x)\n",
    "        return x \n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat_dict = self.forward_feat(x)\n",
    "        features = list(feat_dict.values())\n",
    "        box_cls, box_regression, centerness = self.head(features)\n",
    "        # box_reg: lists of n_feat_level tensors BxHW(level)x4\n",
    "        # why not tensor Bxsum_level(HW)x4 ?\n",
    "        return features, box_cls, box_regression, centerness\n",
    "        \n",
    "    \n",
    "network = ViTSimpleFPN(num_classes=19)\n",
    "print(network.feat_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2bef7a0-4dc1-4dc6-977c-5547ad4af1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "INF = 100000000\n",
    "MAXIMUM_DISTANCES_PER_LEVEL = [-1, 64, 128, 256, 512, INF]\n",
    "\n",
    "def _match_reg_distances_shape(MAXIMUM_DISTANCES_PER_LEVEL, num_locs_per_level):\n",
    "    level_reg_distances = []\n",
    "    for m in range(1, len(MAXIMUM_DISTANCES_PER_LEVEL)):\n",
    "        level_distances = torch.tensor([\n",
    "            MAXIMUM_DISTANCES_PER_LEVEL[m - 1], MAXIMUM_DISTANCES_PER_LEVEL[m]\n",
    "        ],\n",
    "                                       dtype=torch.float32)\n",
    "        locs_per_level = num_locs_per_level[m - 1]\n",
    "        level_distances = level_distances.repeat(locs_per_level).view(\n",
    "            locs_per_level, 2)\n",
    "        level_reg_distances.append(level_distances)\n",
    "    # return tensor of size sum of locs_per_level x 2\n",
    "    return torch.cat(level_reg_distances, dim=0)\n",
    "\n",
    "def _compute_centerness_targets(reg_targets):\n",
    "    if len(reg_targets) == 0:\n",
    "        return reg_targets.new_zeros(len(reg_targets))\n",
    "    left_right = reg_targets[:, [0, 2]]\n",
    "    top_bottom = reg_targets[:, [1, 3]]\n",
    "    centerness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\\n",
    "                (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0])\n",
    "    return torch.sqrt(centerness)\n",
    "\n",
    "\n",
    "def _calculate_reg_targets(xs, ys, bbox_targets):\n",
    "    l = xs[:, None] - bbox_targets[:, 0][None] # Lx1 - 1xT -> LxT\n",
    "    t = ys[:, None] - bbox_targets[:, 1][None]\n",
    "    r = bbox_targets[:, 2][None] - xs[:, None]\n",
    "    b = bbox_targets[:, 3][None] - ys[:, None]\n",
    "    return torch.stack([l, t, r, b], dim=2) # LxTx4\n",
    "\n",
    "\n",
    "def _apply_distance_constraints(reg_targets, level_distances):\n",
    "    max_reg_targets, _ = reg_targets.max(dim=2)\n",
    "    return torch.logical_and(max_reg_targets >= level_distances[:, None, 0], \\\n",
    "                             max_reg_targets <= level_distances[:, None, 1])\n",
    "\n",
    "\n",
    "def _prepare_labels(locations, targets_batch):\n",
    "    device = targets_batch[0].device\n",
    "    # nb of locs for bbox in original image size\n",
    "    num_locs_per_level = [len(l) for l in locations]\n",
    "    # L = sum locs per level x 2 : for each loc in all_locs, the max size of bb authorized\n",
    "    level_distances = _match_reg_distances_shape(MAXIMUM_DISTANCES_PER_LEVEL,\n",
    "                                                 num_locs_per_level).to(device)\n",
    "    all_locations = torch.cat(locations, dim=0).to(device) # Lx2\n",
    "    xs, ys = all_locations[:, 0], all_locations[:, 1] # L & L\n",
    "\n",
    "    all_reg_targets = []\n",
    "    all_cls_targets = []\n",
    "    for targets in targets_batch:\n",
    "        bbox_targets = targets[:, :4] # Tx4\n",
    "        cls_targets = targets[:, 4] # T\n",
    "        \n",
    "        # for each loc in L and each target in T, the reg target\n",
    "        reg_targets = _calculate_reg_targets(xs, ys, bbox_targets) # LxTx4\n",
    "\n",
    "        is_in_boxes = reg_targets.min(dim=2)[0] > 0 # min returns values and indices -> LxT\n",
    "\n",
    "        fits_to_feature_level = _apply_distance_constraints(\n",
    "            reg_targets, level_distances).to(device) # LxT\n",
    "\n",
    "        #bbox_areas = _calc_bbox_area(bbox_targets) # T\n",
    "        bbox_areas = torchvision.ops.box_area(bbox_targets) # compared to above, does not deal with 0dim bb\n",
    "        \n",
    "        # area of each target bbox repeated for each loc with inf where the the loc is not \n",
    "        # in the target bbox or if the loc is not at the right level for this bbox size\n",
    "        locations_to_gt_area = bbox_areas[None].repeat(len(all_locations), 1) # LxT\n",
    "        locations_to_gt_area[is_in_boxes == 0] = INF\n",
    "        locations_to_gt_area[fits_to_feature_level == 0] = INF\n",
    "        \n",
    "        # for each loc, area and target idx of the target of min area at that loc\n",
    "        loc_min_area, loc_mind_idxs = locations_to_gt_area.min(dim=1) # val&idx, size L, idx in [0,T-1]\n",
    "\n",
    "        reg_targets = reg_targets[range(len(all_locations)), loc_mind_idxs] # Lx4\n",
    "\n",
    "        cls_targets = cls_targets[loc_mind_idxs] # L\n",
    "        cls_targets[loc_min_area == INF] = 0\n",
    "        \n",
    "        all_cls_targets.append(\n",
    "            torch.split(cls_targets, num_locs_per_level, dim=0))\n",
    "        all_reg_targets.append(\n",
    "            torch.split(reg_targets, num_locs_per_level, dim=0))\n",
    "    # all_cls_targets contains B lists of num levels elem of loc_per_levelsx1\n",
    "    return _match_pred_format(all_cls_targets, all_reg_targets, locations)\n",
    "\n",
    "\n",
    "def _match_pred_format(cls_targets, reg_targets, locations):\n",
    "    cls_per_level = []\n",
    "    reg_per_level = []\n",
    "    for level in range(len(locations)):\n",
    "        cls_per_level.append(torch.cat([ct[level] for ct in cls_targets],\n",
    "                                       dim=0))\n",
    "\n",
    "        reg_per_level.append(torch.cat([rt[level] for rt in reg_targets],\n",
    "                                       dim=0))\n",
    "    # reg_per_level is a list of num_levels tensors of size Bxnum_loc_per_levelx4\n",
    "    return cls_per_level, reg_per_level\n",
    "\n",
    "\n",
    "def _get_positive_samples(cls_labels, reg_labels, box_cls_preds, box_reg_preds,\n",
    "                          centerness_preds, num_classes):\n",
    "    box_cls_flatten = []\n",
    "    box_regression_flatten = []\n",
    "    centerness_flatten = []\n",
    "    labels_flatten = []\n",
    "    reg_targets_flatten = []\n",
    "    for l in range(len(cls_labels)):\n",
    "        box_cls_flatten.append(box_cls_preds[l].permute(0, 2, 3, 1).reshape(\n",
    "            -1, num_classes))\n",
    "        box_regression_flatten.append(box_reg_preds[l].permute(0, 2, 3,\n",
    "                                                               1).reshape(\n",
    "                                                                   -1, 4))\n",
    "        labels_flatten.append(cls_labels[l].reshape(-1))\n",
    "        reg_targets_flatten.append(reg_labels[l].reshape(-1, 4))\n",
    "        centerness_flatten.append(centerness_preds[l].reshape(-1))\n",
    "\n",
    "    cls_preds = torch.cat(box_cls_flatten, dim=0)\n",
    "    cls_targets = torch.cat(labels_flatten, dim=0)\n",
    "    reg_preds = torch.cat(box_regression_flatten, dim=0)\n",
    "    reg_targets = torch.cat(reg_targets_flatten, dim=0)\n",
    "    centerness_preds = torch.cat(centerness_flatten, dim=0)\n",
    "    pos_inds = torch.nonzero(cls_targets > 0).squeeze(1) # dim #loc in all batches where there is one cls to pred not background\n",
    "\n",
    "    reg_preds = reg_preds[pos_inds]\n",
    "    reg_targets = reg_targets[pos_inds]\n",
    "    centerness_preds = centerness_preds[pos_inds]\n",
    "\n",
    "    return reg_preds, reg_targets, cls_preds, cls_targets, centerness_preds, pos_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e24601f-642b-4245-a582-42a6dc91a23e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LossEvaluator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.centerness_loss_func = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "\n",
    "    def _get_cls_loss(self, cls_preds, cls_targets, total_num_pos):\n",
    "        nc = cls_preds.shape[1]\n",
    "        onehot = F.one_hot(cls_targets.long(), nc+1)[:,1:].float()\n",
    "        cls_loss = torchvision.ops.sigmoid_focal_loss(cls_preds, onehot)\n",
    "        return cls_loss.sum() / total_num_pos\n",
    "\n",
    "    def _get_reg_loss(self, reg_preds, reg_targets, centerness_targets):\n",
    "        reg_preds = reg_preds.reshape(-1, 4)\n",
    "        reg_targets = reg_targets.reshape(-1, 4)\n",
    "        reg_losses = torchvision.ops.distance_box_iou_loss(reg_preds, reg_targets, reduction='none')\n",
    "        sum_centerness_targets = centerness_targets.sum()\n",
    "        reg_loss = (reg_losses * centerness_targets).sum() / sum_centerness_targets\n",
    "        return reg_loss\n",
    "\n",
    "    def _get_centerness_loss(self, centerness_preds, centerness_targets,\n",
    "                             total_num_pos):\n",
    "        centerness_loss = self.centerness_loss_func(centerness_preds,\n",
    "                                                    centerness_targets)\n",
    "        return centerness_loss / total_num_pos\n",
    "\n",
    "    def _evaluate_losses(self, reg_preds, cls_preds, centerness_preds,\n",
    "                         reg_targets, cls_targets, centerness_targets,\n",
    "                         pos_inds):\n",
    "        total_num_pos = max(pos_inds.new_tensor([pos_inds.numel()]), 1.0)\n",
    "\n",
    "        cls_loss = self._get_cls_loss(cls_preds, cls_targets, total_num_pos)\n",
    "\n",
    "        if pos_inds.numel() > 0:\n",
    "            reg_loss = self._get_reg_loss(reg_preds, reg_targets,\n",
    "                                          centerness_targets)\n",
    "            centerness_loss = self._get_centerness_loss(centerness_preds,\n",
    "                                                        centerness_targets,\n",
    "                                                        total_num_pos)\n",
    "        else:\n",
    "            reg_loss = reg_preds.sum() # 0 ??\n",
    "            centerness_loss = centerness_preds.sum() # 0 ??\n",
    "\n",
    "        return reg_loss, cls_loss, centerness_loss\n",
    "\n",
    "    def __call__(self, locations, preds, targets_batch, num_classes):\n",
    "        # reg_targets is a list of num_levels tensors of size Bxnum_loc_per_levelx4\n",
    "        cls_targets, reg_targets = _prepare_labels(locations, targets_batch)\n",
    "\n",
    "        cls_preds, reg_preds, centerness_preds = preds\n",
    "\n",
    "        reg_preds, reg_targets, cls_preds, cls_targets, centerness_preds, pos_inds = _get_positive_samples(\n",
    "            cls_targets, reg_targets, cls_preds, reg_preds, centerness_preds,\n",
    "            num_classes)\n",
    "\n",
    "        centerness_targets = _compute_centerness_targets(reg_targets)\n",
    "\n",
    "        reg_loss, cls_loss, centerness_loss = self._evaluate_losses(\n",
    "            reg_preds, cls_preds, centerness_preds, reg_targets, cls_targets,\n",
    "            centerness_targets, pos_inds)\n",
    "\n",
    "        return cls_loss, reg_loss, centerness_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cd41acc-4e80-43d8-9c45-34a3dd319154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "class FCOSPostProcessor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, pre_nms_thresh, pre_nms_top_n, nms_thresh,\n",
    "                 fpn_post_nms_top_n, min_size, num_classes):\n",
    "        super(FCOSPostProcessor, self).__init__()\n",
    "        self.pre_nms_thresh = pre_nms_thresh\n",
    "        self.pre_nms_top_n = pre_nms_top_n\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.fpn_post_nms_top_n = fpn_post_nms_top_n\n",
    "        self.min_size = min_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward_for_single_feature_map(self, locations, cls_preds, reg_preds,\n",
    "                                       cness_preds, image_size):\n",
    "        B, C, _, _ = cls_preds.shape\n",
    "\n",
    "        cls_preds = cls_preds.permute(0, 2, 3, 1).reshape(B, -1, C).sigmoid() # BxHWxC in [0,1]\n",
    "        reg_preds = reg_preds.permute(0, 2, 3, 1).reshape(B, -1, 4)\n",
    "        cness_preds = cness_preds.permute(0, 2, 3, 1).reshape(B, -1).sigmoid()\n",
    "\n",
    "        candidate_inds = cls_preds > self.pre_nms_thresh # BxHWxC\n",
    "        pre_nms_top_n = candidate_inds.reshape(B, -1).sum(1) # B\n",
    "        pre_nms_top_n = pre_nms_top_n.clamp(max=self.pre_nms_top_n)\n",
    "\n",
    "        cls_preds = cls_preds * cness_preds[:, :, None] # BxHWxC\n",
    "        \n",
    "        # Conversion en liste de bbox,scores,cls par image du batch\n",
    "        # POURQUOI le filtre cls_preds > nms_thresh arrive pas après la mul par cness_preds ?\n",
    "        bboxes = []\n",
    "        cls_labels = []\n",
    "        scores = []\n",
    "        for i in range(B):\n",
    "            # Tensor with true where score for loc l and class c > pre_nms_thresh\n",
    "            per_candidate_inds = candidate_inds[i] # HWxC\n",
    "            # tenseur de taille Lx2 avec les indices des elem de cls_preds où > nms_thresh\n",
    "            per_candidate_nonzeros = per_candidate_inds.nonzero() \n",
    "            # L : positions dans [0,HW] des elem dont cls_preds(c) > nms_thresh \n",
    "            per_box_loc = per_candidate_nonzeros[:, 0]\n",
    "            # L : classe dans [1, C] des elem dont cls_preds(h,w) > nms_thresh\n",
    "            per_class = per_candidate_nonzeros[:, 1] + 1\n",
    "\n",
    "            per_reg_preds = reg_preds[i] # HWx4\n",
    "            # liste des bb des elem dont cls_preds(c) > nms_thresh \n",
    "            per_reg_preds = per_reg_preds[per_box_loc] # Lx4\n",
    "            per_locations = locations[per_box_loc] # Lx2\n",
    "\n",
    "            per_pre_nms_top_n = pre_nms_top_n[i]\n",
    "            \n",
    "            per_cls_preds = cls_preds[i] # HWxC\n",
    "            # tenseur de taille L avec les elem de cls_preds*centerness tels que cls_preds > nms_thresh\n",
    "            per_cls_preds = per_cls_preds[per_candidate_inds] \n",
    "            # si y a plus de per_prenms_topn qui passe nms_thresh (si L est trop longue)\n",
    "            if per_candidate_inds.sum().item() > per_pre_nms_top_n.item():\n",
    "                per_cls_preds, top_k_indices = per_cls_preds.topk(\n",
    "                    per_pre_nms_top_n, sorted=False)\n",
    "                per_class = per_class[top_k_indices]\n",
    "                per_reg_preds = per_reg_preds[top_k_indices]\n",
    "                per_locations = per_locations[top_k_indices]\n",
    "            \n",
    "            # Rewrites bbox (x0,y0,x1,y1) from reg targets (l,t,r,b) following eq (1) in paper\n",
    "            per_bboxes = torch.stack([\n",
    "                per_locations[:, 0] - per_reg_preds[:, 0],\n",
    "                per_locations[:, 1] - per_reg_preds[:, 1],\n",
    "                per_locations[:, 0] + per_reg_preds[:, 2],\n",
    "                per_locations[:, 1] + per_reg_preds[:, 3],\n",
    "            ],\n",
    "                                     dim=1)\n",
    "            per_bboxes = torchvision.ops.clip_boxes_to_image(per_bboxes, (image_size, image_size))\n",
    "            #detections = _clip_to_image(detections, (image_size, image_size))\n",
    "            per_bboxes = per_bboxes[torchvision.ops.remove_small_boxes(per_bboxes, self.min_size)]\n",
    "            #detections = remove_small_boxes(detections, self.min_size)\n",
    "            bboxes.append(per_bboxes)\n",
    "            cls_labels.append(per_class)\n",
    "            scores.append(torch.sqrt(per_cls_preds))\n",
    "        \n",
    "        #bboxes is a list of B tensors of size Lx4 (potentially filtered with pre_nms_threshold)\n",
    "        return bboxes, scores, cls_labels\n",
    "\n",
    "    def forward(self, locations, cls_preds, reg_preds, cness_preds, image_size):\n",
    "        # loc: list of n_feat_level tensors of size HW(level)\n",
    "        # reg_preds: list of n_feat_level tensors BxHW(level)x4\n",
    "        \n",
    "        # list of n_feat_level lists of B tensors of size Lx4\n",
    "        sampled_boxes = []\n",
    "        all_scores = []\n",
    "        all_classes = []\n",
    "        for l, o, b, c in list(zip(locations, cls_preds, reg_preds,\n",
    "                                   cness_preds)):\n",
    "            boxes, scores, cls_labels = self.forward_for_single_feature_map(\n",
    "                l, o, b, c, image_size)\n",
    "            # boxes : list of B tensors Lx4\n",
    "            sampled_boxes.append(boxes)\n",
    "            all_scores.append(scores)\n",
    "            all_classes.append(cls_labels)\n",
    "        \n",
    "        # list of B lists of n_feat_level bbox preds\n",
    "        all_bboxes = list(zip(*sampled_boxes))\n",
    "        all_scores = list(zip(*all_scores))\n",
    "        all_classes = list(zip(*all_classes))\n",
    "    \n",
    "        # list of B tensors with all feature level bbox preds grouped\n",
    "        all_bboxes = [torch.cat(bboxes, dim=0) for bboxes in all_bboxes]\n",
    "        all_scores = [torch.cat(scores, dim=0) for scores in all_scores]\n",
    "        all_classes = [torch.cat(classes, dim=0) for classes in all_classes]\n",
    "        boxes, scores, classes = self.select_over_all_levels(\n",
    "            all_bboxes, all_scores, all_classes)\n",
    "\n",
    "        return boxes, scores, classes\n",
    "\n",
    "    def select_over_all_levels(self, boxlists, scores, classes):\n",
    "        num_images = len(boxlists)\n",
    "        all_picked_boxes, all_confidence_scores, all_classes = [], [], []\n",
    "        for i in range(num_images):\n",
    "            picked_indices = torchvision.ops.nms(boxlists[i], scores[i], self.nms_thresh)\n",
    "            picked_boxes = boxlists[i][picked_indices]\n",
    "            confidence_scores = scores[i][picked_indices]\n",
    "            picked_classes = classes[i][picked_indices]\n",
    "\n",
    "            number_of_detections = len(picked_indices)\n",
    "            if number_of_detections > self.fpn_post_nms_top_n > 0:\n",
    "                image_thresh, _ = torch.kthvalue(\n",
    "                    confidence_scores.cpu(),\n",
    "                    number_of_detections - self.fpn_post_nms_top_n + 1)\n",
    "                keep = confidence_scores >= image_thresh.item()\n",
    "\n",
    "                keep = torch.nonzero(keep).squeeze(1)\n",
    "                picked_boxes, confidence_scores, picked_classes = picked_boxes[\n",
    "                    keep], confidence_scores[keep], picked_classes[keep]\n",
    "\n",
    "            keep = confidence_scores >= self.pre_nms_thresh\n",
    "            picked_boxes, confidence_scores, picked_classes = picked_boxes[\n",
    "                keep], confidence_scores[keep], picked_classes[keep]\n",
    "\n",
    "            all_picked_boxes.append(picked_boxes)\n",
    "            all_confidence_scores.append(confidence_scores)\n",
    "            all_classes.append(picked_classes)\n",
    "        \n",
    "        # all_picked_boxes : list of B tensors with all feature level bbox preds filtered by nms\n",
    "        return all_picked_boxes, all_confidence_scores, all_classes\n",
    "\n",
    "import schedulefree\n",
    "\n",
    "class FCOS(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        num_classes,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.le = LossEvaluator()\n",
    "        self.post_processor = FCOSPostProcessor(\n",
    "            pre_nms_thresh=0.3,\n",
    "            pre_nms_top_n=1000,\n",
    "            nms_thresh=0.45,\n",
    "            fpn_post_nms_top_n=50,\n",
    "            min_size=0,\n",
    "            num_classes=num_classes)\n",
    "        self.fpn_strides = [8, 16, 32, 64, 128]\n",
    "        self.feat_sizes = network.feat_sizes\n",
    "        self.num_classes = num_classes\n",
    "        self.network = network\n",
    "        self.map_metric = MeanAveragePrecision()\n",
    "        # locations is a list of num_feat_level elem, where each elem indicates the tensor of \n",
    "        # locations in the original image corresponding to each location in the feature map at this level\n",
    "        self.locations = self._compute_locations()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=1e-3,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decay=5e-2,\n",
    "            eps=1e-8,\n",
    "        )\n",
    "        #opt = schedulefree.AdamWScheduleFree(self.parameters(), lr=0.0025)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=1e-3,\n",
    "            steps_per_epoch=10,\n",
    "            epochs=20\n",
    "        )\n",
    "        #return opt\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\"\n",
    "            },\n",
    "        }\n",
    "    \n",
    "    def _compute_locations(self):\n",
    "        locations = []\n",
    "        \n",
    "        def _locations_per_level(h, w, s):\n",
    "            locs_x = [i for i in range(w)]\n",
    "            locs_y = [i for i in range(h)]\n",
    "            locs_x = [s / 2 + x * s for x in locs_x]\n",
    "            locs_y = [s / 2 + y * s for y in locs_y]\n",
    "            locs = [(y, x) for x in locs_x for y in locs_y]\n",
    "            return torch.tensor(locs)\n",
    "        \n",
    "        for level, (h,w) in enumerate(self.feat_sizes):\n",
    "            locs = _locations_per_level(h, w, self.fpn_strides[level])\n",
    "            locations.append(locs)\n",
    "        return locations\n",
    "    \n",
    "    def forward(self, images, targets_batch=None):\n",
    "        features, box_cls, box_regression, centerness = self.network(images)\n",
    "        locations = [l.to(features[0].device) for l in self.locations]\n",
    "        image_size = images.shape[-1]\n",
    "        outputs = {}\n",
    "        predicted_boxes, scores, all_classes = self.post_processor(\n",
    "            locations, box_cls, box_regression, centerness, image_size)\n",
    "        \n",
    "        \n",
    "        if targets_batch != None:\n",
    "            cls_loss, reg_loss, centerness_loss = self.le(\n",
    "                locations, (box_cls, box_regression, centerness),\n",
    "                targets_batch,\n",
    "                num_classes=self.num_classes)\n",
    "            outputs[\"cls_loss\"] = cls_loss\n",
    "            outputs[\"reg_loss\"] = reg_loss\n",
    "            outputs[\"centerness_loss\"] = centerness_loss\n",
    "            outputs[\"combined_loss\"] = cls_loss + reg_loss + centerness_loss\n",
    "\n",
    "\n",
    "\n",
    "        outputs[\"predicted_boxes\"] = predicted_boxes\n",
    "        outputs[\"scores\"] = scores\n",
    "        outputs[\"pred_classes\"] = all_classes\n",
    "        return outputs\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, bboxes, labels = batch\n",
    "        y = [torch.cat([bb, l[:,None]], dim=1) for bb, l in zip(bboxes, labels)]\n",
    "        results = self.forward(x, y)\n",
    "        loss = results[\"combined_loss\"]\n",
    "        self.log(f\"loss/train\", loss.detach().item())\n",
    "        self.log(f\"cls_loss/train\", results[\"cls_loss\"].detach().item())\n",
    "        self.log(f\"reg_loss/train\", results[\"reg_loss\"].detach().item())\n",
    "        self.log(f\"centerness_loss/train\", results[\"centerness_loss\"].detach().item())\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, bboxes, labels = batch\n",
    "        y = [torch.cat([bb, l[:,None]], dim=1) for bb, l in zip(bboxes, labels)]\n",
    "        results = self.forward(x, y)\n",
    "        loss = results[\"combined_loss\"]\n",
    "        preds = [{'boxes': bb, 'scores': s, 'labels': l} for bb,s,l in zip(\n",
    "            results[\"predicted_boxes\"], results[\"scores\"], results[\"pred_classes\"]\n",
    "        )]\n",
    "        target_bb = [t[:, :4] for t in y]\n",
    "        target_l = [t[:, 4] for t in y]\n",
    "        targets = [{'boxes': bb, 'labels': l} for bb,l in zip(target_bb, target_l)]\n",
    "        self.map_metric.update(preds, targets)\n",
    "        self.log(f\"loss/val\", loss.detach().item())\n",
    "        self.log(f\"cls_loss/val\", results[\"cls_loss\"].detach().item())\n",
    "        self.log(f\"reg_loss/val\", results[\"reg_loss\"].detach().item())\n",
    "        self.log(f\"centerness_loss/val\", results[\"centerness_loss\"].detach().item())\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        mapmetric = self.map_metric.compute()['map']\n",
    "        self.log(\"map/val\", mapmetric)\n",
    "        print(\"\\nMAP: \", mapmetric)\n",
    "        self.map_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d01e6b-5b98-4aae-a18b-a87aa7c0f8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type                 | Params\n",
      "--------------------------------------------------------\n",
      "0 | post_processor | FCOSPostProcessor    | 0     \n",
      "1 | network        | ViTSimpleFPN         | 28.0 M\n",
      "2 | map_metric     | MeanAveragePrecision | 0     \n",
      "--------------------------------------------------------\n",
      "28.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.0 M    Total params\n",
      "111.870   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Sanity Checking DataLoader 0:   0%|                                                                                                       | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Class values must be smaller than num_classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     21\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1031\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1031\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1060\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1057\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1060\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 260\u001b[0m, in \u001b[0;36mFCOS.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    258\u001b[0m x, bboxes, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    259\u001b[0m y \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mcat([bb, l[:,\u001b[38;5;28;01mNone\u001b[39;00m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m bb, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(bboxes, labels)]\n\u001b[0;32m--> 260\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m loss \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    262\u001b[0m preds \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m: bb, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m: s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: l} \u001b[38;5;28;01mfor\u001b[39;00m bb,s,l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    263\u001b[0m     results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m], results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m], results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    264\u001b[0m )]\n",
      "Cell \u001b[0;32mIn[8], line 222\u001b[0m, in \u001b[0;36mFCOS.forward\u001b[0;34m(self, images, targets_batch)\u001b[0m\n\u001b[1;32m    217\u001b[0m predicted_boxes, scores, all_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_processor(\n\u001b[1;32m    218\u001b[0m     locations, box_cls, box_regression, centerness, image_size)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets_batch \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     cls_loss, reg_loss, centerness_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_regression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenterness\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtargets_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m cls_loss\n\u001b[1;32m    227\u001b[0m     outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m reg_loss\n",
      "Cell \u001b[0;32mIn[7], line 59\u001b[0m, in \u001b[0;36mLossEvaluator.__call__\u001b[0;34m(self, locations, preds, targets_batch, num_classes)\u001b[0m\n\u001b[1;32m     53\u001b[0m reg_preds, reg_targets, cls_preds, cls_targets, centerness_preds, pos_inds \u001b[38;5;241m=\u001b[39m _get_positive_samples(\n\u001b[1;32m     54\u001b[0m     cls_targets, reg_targets, cls_preds, reg_preds, centerness_preds,\n\u001b[1;32m     55\u001b[0m     num_classes)\n\u001b[1;32m     57\u001b[0m centerness_targets \u001b[38;5;241m=\u001b[39m _compute_centerness_targets(reg_targets)\n\u001b[0;32m---> 59\u001b[0m reg_loss, cls_loss, centerness_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_losses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreg_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenterness_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenterness_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_inds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cls_loss, reg_loss, centerness_loss\n",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m, in \u001b[0;36mLossEvaluator._evaluate_losses\u001b[0;34m(self, reg_preds, cls_preds, centerness_preds, reg_targets, cls_targets, centerness_targets, pos_inds)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate_losses\u001b[39m(\u001b[38;5;28mself\u001b[39m, reg_preds, cls_preds, centerness_preds,\n\u001b[1;32m     29\u001b[0m                      reg_targets, cls_targets, centerness_targets,\n\u001b[1;32m     30\u001b[0m                      pos_inds):\n\u001b[1;32m     31\u001b[0m     total_num_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(pos_inds\u001b[38;5;241m.\u001b[39mnew_tensor([pos_inds\u001b[38;5;241m.\u001b[39mnumel()]), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m     cls_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cls_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_num_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pos_inds\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     36\u001b[0m         reg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reg_loss(reg_preds, reg_targets,\n\u001b[1;32m     37\u001b[0m                                       centerness_targets)\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mLossEvaluator._get_cls_loss\u001b[0;34m(self, cls_preds, cls_targets, total_num_pos)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_cls_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, cls_preds, cls_targets, total_num_pos):\n\u001b[1;32m      9\u001b[0m     nc \u001b[38;5;241m=\u001b[39m cls_preds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m     onehot \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_targets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnc\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[:,\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     11\u001b[0m     cls_loss \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msigmoid_focal_loss(cls_preds, onehot)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cls_loss\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m total_num_pos\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Class values must be smaller than num_classes."
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from dl_toolbox.callbacks import ProgressBar\n",
    "import gc\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='cpu',\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    limit_train_batches=5,\n",
    "    limit_val_batches=5,\n",
    "    callbacks=[ProgressBar()]\n",
    ")\n",
    "\n",
    "module = FCOS(\n",
    "    network,\n",
    "    num_classes=19\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "trainer.fit(\n",
    "    module,\n",
    "    datamodule=dm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae76ce9e-94e1-4999-a6fb-5005b2a03e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073db9f-985d-4995-8f19-73d842212f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
