{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c186ba07-5a59-4a0d-9c0d-b17a397e1405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.v2 as v2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image2 = torch.clone(image)\n",
    "        if len(image2.shape) == 4:\n",
    "            # batched\n",
    "            image2 = image2.permute(1, 0, 2, 3)\n",
    "        for t, m, s in zip(image2, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return image2.permute(1, 0, 2, 3)\n",
    "    \n",
    "norm = v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "unnorm = UnNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "class TorchPCA(object):\n",
    "\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean_ = X.mean(dim=0)\n",
    "        unbiased = X - self.mean_.unsqueeze(0)\n",
    "        U, S, V = torch.pca_lowrank(unbiased, q=self.n_components, center=False, niter=4)\n",
    "        self.components_ = V.T\n",
    "        self.singular_values_ = S\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        t0 = X - self.mean_.unsqueeze(0)\n",
    "        projected = t0 @ self.components_.T\n",
    "        return projected\n",
    "\n",
    "\n",
    "def pca(image_feats_list, dim=3, fit_pca=None, max_samples=None):\n",
    "    device = image_feats_list[0].device\n",
    "\n",
    "    def flatten(tensor, target_size=None):\n",
    "        if target_size is not None and fit_pca is None:\n",
    "            tensor = F.interpolate(tensor, (target_size, target_size), mode=\"bilinear\")\n",
    "        B, C, H, W = tensor.shape\n",
    "        return tensor.permute(1, 0, 2, 3).reshape(C, B * H * W).permute(1, 0).detach().cpu()\n",
    "\n",
    "    if len(image_feats_list) > 1 and fit_pca is None:\n",
    "        target_size = image_feats_list[0].shape[2]\n",
    "    else:\n",
    "        target_size = None\n",
    "\n",
    "    flattened_feats = []\n",
    "    for feats in image_feats_list:\n",
    "        flattened_feats.append(flatten(feats, target_size))\n",
    "    x = torch.cat(flattened_feats, dim=0)\n",
    "\n",
    "    # Subsample the data if max_samples is set and the number of samples exceeds max_samples\n",
    "    if max_samples is not None and x.shape[0] > max_samples:\n",
    "        indices = torch.randperm(x.shape[0])[:max_samples]\n",
    "        x = x[indices]\n",
    "\n",
    "    if fit_pca is None:\n",
    "        fit_pca = TorchPCA(n_components=dim).fit(x)\n",
    "\n",
    "    reduced_feats = []\n",
    "    for feats in image_feats_list:\n",
    "        x_red = fit_pca.transform(flatten(feats))\n",
    "        if isinstance(x_red, np.ndarray):\n",
    "            x_red = torch.from_numpy(x_red)\n",
    "        x_red -= x_red.min(dim=0, keepdim=True).values\n",
    "        x_red /= x_red.max(dim=0, keepdim=True).values\n",
    "        B, C, H, W = feats.shape\n",
    "        reduced_feats.append(x_red.reshape(B, H, W, dim).permute(0, 3, 1, 2).to(device))\n",
    "\n",
    "    return reduced_feats, fit_pca\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "def _remove_axes(ax):\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "\n",
    "def remove_axes(axes):\n",
    "    if len(axes.shape) == 2:\n",
    "        for ax1 in axes:\n",
    "            for ax in ax1:\n",
    "                _remove_axes(ax)\n",
    "    else:\n",
    "        for ax in axes:\n",
    "            _remove_axes(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6340ec7f-c96d-4a33-8fd7-2d99d2be6f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from dl_toolbox.datasets import Rellis3d\n",
    "#from torchvision import tv_tensors\n",
    "#\n",
    "#tf = v2.Compose([\n",
    "#    v2.RandomCrop(size=(672, 672)),\n",
    "#    v2.ToDtype(\n",
    "#        dtype={tv_tensors.Image: torch.float32, tv_tensors.Mask: torch.int64, \"others\":None}, \n",
    "#        scale=True\n",
    "#    ),\n",
    "#    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#])\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "#rellis = '/data/Rellis-3D'\n",
    "#imgs = [rellis+'/00000/pylon_camera_node/frame000000-1581624652_750.jpg']\n",
    "#msks = [rellis+'/00000/pylon_camera_node_label_id/frame000000-1581624652_750.png']\n",
    "#dataset = Rellis3d(\n",
    "#    imgs=imgs,\n",
    "#    msks=msks,\n",
    "#    merge='all19',\n",
    "#    transforms=tf\n",
    "#)\n",
    "#elem = dataset[0]\n",
    "#image, mask = elem['image'].to(device).unsqueeze(0), elem['label']\n",
    "#h = 672 // 14\n",
    "#w = 672 // 14\n",
    "#encoder.to(device)\n",
    "#lr_feats = encoder.forward_features(image)\n",
    "#lr_feats = lr_feats[:,encoder.num_prefix_tokens:,...]\n",
    "#lr_feats = lr_feats.reshape(-1, h, w, 384).permute(0,3,1,2).detach().cpu()\n",
    "#hr_feats_bili = v2.functional.resize(lr_feats, (672, 672), Image.BILINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8755c249-dc89-41bb-bb2b-3b40f80ba6fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from concurrent.futures import ProcessPoolExecutor\n",
    "#\n",
    "#ref = image[0].permute(1,2,0).to('cpu').numpy()\n",
    "#source_upsampled = hr_feats_bili[0].permute(1,2,0).to('cpu').numpy()\n",
    "#\n",
    "#NB_CHANNELS = 384\n",
    "#\n",
    "#scale = 48 / 672\n",
    "#radius = 2\n",
    "#diameter = 2 * radius + 1\n",
    "#step = int(np.ceil(1 / scale))\n",
    "#padding = radius * step\n",
    "#sigma_spatial = 2.5\n",
    "#sigma_range = np.std(ref)\n",
    "#\n",
    "#reference = np.pad(ref, ((padding, padding), (padding, padding), (0, 0)), 'symmetric').astype(np.float32)\n",
    "#source_upsampled = np.pad(source_upsampled, ((padding, padding), (padding, padding), (0, 0)), 'symmetric').astype(np.float32)\n",
    "#\n",
    "## Spatial Gaussian function.\n",
    "#x, y = np.meshgrid(np.arange(diameter) - radius, np.arange(diameter) - radius)\n",
    "#kernel_spatial = np.exp(-1.0 * (x**2 + y**2) /  (2 * sigma_spatial**2))\n",
    "##kernel_spatial = np.repeat(kernel_spatial, 3).reshape(-1, 3)\n",
    "#kernel_spatial = np.reshape(-1,1)\n",
    "#\n",
    "## Lookup table for range kernel.\n",
    "#lut_range = np.exp(-1.0 * np.arange(256)**2 / (2 * sigma_range**2))\n",
    "#\n",
    "#def process_row(y):\n",
    "#    result = np.zeros((ref.shape[1], NB_CHANNELS))\n",
    "#    y += padding\n",
    "#    for x in range(padding, reference.shape[1] - padding):\n",
    "#        I_p = reference[y, x]\n",
    "#        patch_reference = reference[y - padding:y + padding + 1:step, x - padding:x + padding + 1:step].reshape(-1, 3)\n",
    "#        patch_source_upsampled = source_upsampled[y - padding:y + padding + 1:step, x - padding:x + padding + 1:step].reshape(-1, NB_CHANNELS)\n",
    "#\n",
    "#        kernel_range = lut_range[np.abs(patch_reference - I_p).astype(int)]\n",
    "#        norm = np.linalg.norm(patch_reference-I_p, axis=1, keepdims=True)\n",
    "#        kernel_range = np.exp(-1.0 * norm**2 / (2 * sigma_range**2))\n",
    "#        weight = kernel_range * kernel_spatial\n",
    "#        prod = weight * patch_source_upsampled\n",
    "#        k_p = weight.sum(axis=0)\n",
    "#        res = np.round(np.sum(weight * patch_source_upsampled, axis=0) / k_p)\n",
    "#        result[x - padding] = res\n",
    "#    return result\n",
    "#\n",
    "#executor = ProcessPoolExecutor()\n",
    "#result = executor.map(process_row, range(ref.shape[0]))\n",
    "#executor.shutdown(True)\n",
    "#hr_feats = torch.Tensor(np.array(list(result))) \n",
    "#print(hr_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "519d7304-0dc9-4e53-a239-7e3b2eb84c78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#[lr_feats_pca], _ = pca([lr_feats])\n",
    "#[hr_feats_bili_pca], _ = pca([hr_feats_bili])\n",
    "#[hr_feats_jbu_pca], _ = pca([hr_feats.permute(2,0,1).unsqueeze(0)])\n",
    "#\n",
    "#fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "#ax[0,0].imshow(v2.functional.to_pil_image(unnorm(image).squeeze(0)))\n",
    "#ax[0,0].set_title(\"Ref Image\")\n",
    "#ax[0,1].imshow(v2.functional.to_pil_image(lr_feats_pca[0]))\n",
    "#ax[0,1].set_title(\"low res features\")\n",
    "#ax[1,0].imshow(v2.functional.to_pil_image(hr_feats_bili_pca[0]))\n",
    "#ax[1,0].set_title(\"Bilinear upsampled features\")\n",
    "#ax[1,1].imshow(v2.functional.to_pil_image(hr_feats_jbu_pca[0]))\n",
    "#ax[1,1].set_title(\"JBU features\")\n",
    "#remove_axes(ax)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6545e26b-437d-4dca-a41a-680bb3242b07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics as M\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "from timm.models.layers import trunc_normal_\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "class DecoderLinear(nn.Module):\n",
    "    def __init__(self, n_cls, patch_size, d_encoder):\n",
    "        super().__init__()\n",
    "        self.d_encoder = d_encoder\n",
    "        self.patch_size = patch_size\n",
    "        self.n_cls = n_cls\n",
    "        self.head = nn.Linear(self.d_encoder, n_cls)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return set()\n",
    "\n",
    "    def forward(self, x, im_size):\n",
    "        H, W = im_size\n",
    "        GS = H // self.patch_size\n",
    "        x = self.head(x)\n",
    "        x = rearrange(x, \"b (h w) c -> b c h w\", h=GS)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "def param_groups_weight_decay(\n",
    "        params,\n",
    "        weight_decay=1e-5,\n",
    "        no_weight_decay_list=()\n",
    "):\n",
    "    no_weight_decay_list = set(no_weight_decay_list)\n",
    "    decay = []\n",
    "    no_decay = []\n",
    "    for name, param in params:\n",
    "        if param.ndim <= 1 or name.endswith(\".bias\") or name in no_weight_decay_list:\n",
    "            no_decay.append(param)\n",
    "            print(name)\n",
    "        else:\n",
    "            decay.append(param)\n",
    "    return [\n",
    "        {'params': no_decay, 'weight_decay': 0.},\n",
    "        {'params': decay, 'weight_decay': weight_decay}\n",
    "    ]\n",
    "            \n",
    "class Segmenter(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        n_cls\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_cls = n_cls\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.acc = M.Accuracy(task='multiclass', num_classes=20)\n",
    "        self.loss =  nn.CrossEntropyLoss()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        weight_decay=5e-2\n",
    "        parameters = list(filter(lambda p: p[1].requires_grad, self.named_parameters()))\n",
    "        print(\n",
    "            f\"The model will start training with only {sum([int(torch.numel(p)) for n,p in parameters])} \"\n",
    "            f\"trainable parameters out of {sum([int(torch.numel(p)) for p in self.parameters()])}.\"\n",
    "        )\n",
    "        if hasattr(self, 'no_weight_decay'):\n",
    "            parameters = param_groups_weight_decay(parameters, weight_decay, self.no_weight_decay())\n",
    "            print(f\"{len(parameters[0]['params'])} no decay params\")\n",
    "        opt = schedulefree.AdamWScheduleFree(parameters, weight_decay=weight_decay, lr=0.0025)\n",
    "        return opt\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        if hasattr(self.optimizers(), 'train'):\n",
    "            self.optimizers().train()\n",
    "        \n",
    "    def on_validation_start(self):\n",
    "        if hasattr(self.optimizers(), 'eval'):\n",
    "            self.optimizers().eval()  \n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        def append_prefix_no_weight_decay(prefix, module):\n",
    "            return set(map(lambda x: prefix + x, module.no_weight_decay()))\n",
    "        nwd_params = append_prefix_no_weight_decay(\"encoder.\", self.encoder).union(\n",
    "            append_prefix_no_weight_decay(\"decoder.\", self.decoder)\n",
    "        )\n",
    "        print(nwd_params)\n",
    "        return nwd_params\n",
    "\n",
    "    def forward(self, im):\n",
    "        H_ori, W_ori = im.size(2), im.size(3)\n",
    "        H, W = im.size(2), im.size(3)\n",
    "        x = self.encoder.forward_features(im)\n",
    "        x = x[:,self.encoder.num_prefix_tokens:,...]\n",
    "        masks = self.decoder(x, (H, W))\n",
    "        masks = F.interpolate(masks, size=(H, W), mode=\"bilinear\")\n",
    "        return masks\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[\"sup\"]\n",
    "        outputs = self.forward(x)\n",
    "        loss = self.loss(outputs, y['masks'])\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        outputs = self.forward(x)\n",
    "        loss = self.loss(outputs, y['masks'])\n",
    "        acc = self.acc.update(outputs, y['masks'])\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        acc = self.acc.compute()\n",
    "        print(\"\\nAcc: \", acc)\n",
    "        self.acc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc757141-3d97-4622-96bc-91b0f1bc5f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type               | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder | VisionTransformer  | 22.4 M\n",
      "1 | decoder | DecoderLinear      | 7.7 K \n",
      "2 | acc     | MulticlassAccuracy | 0     \n",
      "3 | loss    | CrossEntropyLoss   | 0     \n",
      "-----------------------------------------------\n",
      "302 K     Trainable params\n",
      "22.1 M    Non-trainable params\n",
      "22.4 M    Total params\n",
      "89.435    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will start training with only 302612 trainable parameters out of 22358804.\n",
      "{'encoder.pos_embed', 'encoder.dist_token', 'encoder.cls_token'}\n",
      "decoder.head.bias\n",
      "1 no decay params\n",
      "Sanity Checking DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.38it/s]\n",
      "Acc:  tensor(0.1514, device='cuda:0')\n",
      "Epoch 0:   1%|█▏                                                                                                    | 14/1247 [00:08<12:35,  1.63it/s, v_num=27]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "[rank: 0] Received SIGTERM: 15\n"
     ]
    }
   ],
   "source": [
    "from dl_toolbox import datamodules\n",
    "import schedulefree\n",
    "from dl_toolbox.callbacks import ProgressBar, FeatureFt, Lora\n",
    "from torchvision import tv_tensors\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "\n",
    "tf = v2.Compose([\n",
    "    v2.RandomCrop(size=(672, 672)),\n",
    "    v2.ToDtype(\n",
    "        dtype={tv_tensors.Image: torch.float32, tv_tensors.Mask: torch.int64, \"others\":None}, \n",
    "        scale=True\n",
    "    ),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dm = datamodules.Rellis3d(\n",
    "    data_path='/data',\n",
    "    merge='all20',\n",
    "    sup=1,\n",
    "    unsup=0,\n",
    "    train_tf=tf,\n",
    "    test_tf=tf,\n",
    "    batch_size=4,\n",
    "    num_workers=6,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    limit_train_batches=1.,\n",
    "    limit_val_batches=1.,\n",
    "    callbacks=[ProgressBar(), Lora('encoder', 4)]\n",
    ")\n",
    "\n",
    "encoder = timm.create_model('vit_small_patch14_dinov2', pretrained=True, dynamic_img_size=True)\n",
    "decoder = DecoderLinear(n_cls=20, d_encoder=encoder.embed_dim, patch_size=14)\n",
    "model = Segmenter(encoder,decoder,n_cls=20)\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    datamodule=dm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f373a6ec-6e6c-47d4-9780-03c55743be08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
