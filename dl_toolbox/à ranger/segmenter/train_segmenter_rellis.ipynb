{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b61bc75-88a0-4b5b-8f06-03fcc1d8df9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from functools import partial\n",
    "\n",
    "encoder = timm.create_model('vit_small_patch14_dinov2', pretrained=True, dynamic_img_size=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c186ba07-5a59-4a0d-9c0d-b17a397e1405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.v2 as v2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image2 = torch.clone(image)\n",
    "        if len(image2.shape) == 4:\n",
    "            # batched\n",
    "            image2 = image2.permute(1, 0, 2, 3)\n",
    "        for t, m, s in zip(image2, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return image2.permute(1, 0, 2, 3)\n",
    "    \n",
    "norm = v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "unnorm = UnNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "class TorchPCA(object):\n",
    "\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean_ = X.mean(dim=0)\n",
    "        unbiased = X - self.mean_.unsqueeze(0)\n",
    "        U, S, V = torch.pca_lowrank(unbiased, q=self.n_components, center=False, niter=4)\n",
    "        self.components_ = V.T\n",
    "        self.singular_values_ = S\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        t0 = X - self.mean_.unsqueeze(0)\n",
    "        projected = t0 @ self.components_.T\n",
    "        return projected\n",
    "\n",
    "\n",
    "def pca(image_feats_list, dim=3, fit_pca=None, max_samples=None):\n",
    "    device = image_feats_list[0].device\n",
    "\n",
    "    def flatten(tensor, target_size=None):\n",
    "        if target_size is not None and fit_pca is None:\n",
    "            tensor = F.interpolate(tensor, (target_size, target_size), mode=\"bilinear\")\n",
    "        B, C, H, W = tensor.shape\n",
    "        return tensor.permute(1, 0, 2, 3).reshape(C, B * H * W).permute(1, 0).detach().cpu()\n",
    "\n",
    "    if len(image_feats_list) > 1 and fit_pca is None:\n",
    "        target_size = image_feats_list[0].shape[2]\n",
    "    else:\n",
    "        target_size = None\n",
    "\n",
    "    flattened_feats = []\n",
    "    for feats in image_feats_list:\n",
    "        flattened_feats.append(flatten(feats, target_size))\n",
    "    x = torch.cat(flattened_feats, dim=0)\n",
    "\n",
    "    # Subsample the data if max_samples is set and the number of samples exceeds max_samples\n",
    "    if max_samples is not None and x.shape[0] > max_samples:\n",
    "        indices = torch.randperm(x.shape[0])[:max_samples]\n",
    "        x = x[indices]\n",
    "\n",
    "    if fit_pca is None:\n",
    "        fit_pca = TorchPCA(n_components=dim).fit(x)\n",
    "\n",
    "    reduced_feats = []\n",
    "    for feats in image_feats_list:\n",
    "        x_red = fit_pca.transform(flatten(feats))\n",
    "        if isinstance(x_red, np.ndarray):\n",
    "            x_red = torch.from_numpy(x_red)\n",
    "        x_red -= x_red.min(dim=0, keepdim=True).values\n",
    "        x_red /= x_red.max(dim=0, keepdim=True).values\n",
    "        B, C, H, W = feats.shape\n",
    "        reduced_feats.append(x_red.reshape(B, H, W, dim).permute(0, 3, 1, 2).to(device))\n",
    "\n",
    "    return reduced_feats, fit_pca\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "def _remove_axes(ax):\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "\n",
    "def remove_axes(axes):\n",
    "    if len(axes.shape) == 2:\n",
    "        for ax1 in axes:\n",
    "            for ax in ax1:\n",
    "                _remove_axes(ax)\n",
    "    else:\n",
    "        for ax in axes:\n",
    "            _remove_axes(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6340ec7f-c96d-4a33-8fd7-2d99d2be6f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from dl_toolbox.datasets import Rellis3d\n",
    "#from torchvision import tv_tensors\n",
    "#\n",
    "#tf = v2.Compose([\n",
    "#    v2.RandomCrop(size=(672, 672)),\n",
    "#    v2.ToDtype(\n",
    "#        dtype={tv_tensors.Image: torch.float32, tv_tensors.Mask: torch.int64, \"others\":None}, \n",
    "#        scale=True\n",
    "#    ),\n",
    "#    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#])\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "#rellis = '/data/Rellis-3D'\n",
    "#imgs = [rellis+'/00000/pylon_camera_node/frame000000-1581624652_750.jpg']\n",
    "#msks = [rellis+'/00000/pylon_camera_node_label_id/frame000000-1581624652_750.png']\n",
    "#dataset = Rellis3d(\n",
    "#    imgs=imgs,\n",
    "#    msks=msks,\n",
    "#    merge='all19',\n",
    "#    transforms=tf\n",
    "#)\n",
    "#elem = dataset[0]\n",
    "#image, mask = elem['image'].to(device).unsqueeze(0), elem['label']\n",
    "#h = 672 // 14\n",
    "#w = 672 // 14\n",
    "#encoder.to(device)\n",
    "#lr_feats = encoder.forward_features(image)\n",
    "#lr_feats = lr_feats[:,encoder.num_prefix_tokens:,...]\n",
    "#lr_feats = lr_feats.reshape(-1, h, w, 384).permute(0,3,1,2).detach().cpu()\n",
    "#hr_feats_bili = v2.functional.resize(lr_feats, (672, 672), Image.BILINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8755c249-dc89-41bb-bb2b-3b40f80ba6fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from concurrent.futures import ProcessPoolExecutor\n",
    "#\n",
    "#ref = image[0].permute(1,2,0).to('cpu').numpy()\n",
    "#source_upsampled = hr_feats_bili[0].permute(1,2,0).to('cpu').numpy()\n",
    "#\n",
    "#NB_CHANNELS = 384\n",
    "#\n",
    "#scale = 48 / 672\n",
    "#radius = 2\n",
    "#diameter = 2 * radius + 1\n",
    "#step = int(np.ceil(1 / scale))\n",
    "#padding = radius * step\n",
    "#sigma_spatial = 2.5\n",
    "#sigma_range = np.std(ref)\n",
    "#\n",
    "#reference = np.pad(ref, ((padding, padding), (padding, padding), (0, 0)), 'symmetric').astype(np.float32)\n",
    "#source_upsampled = np.pad(source_upsampled, ((padding, padding), (padding, padding), (0, 0)), 'symmetric').astype(np.float32)\n",
    "#\n",
    "## Spatial Gaussian function.\n",
    "#x, y = np.meshgrid(np.arange(diameter) - radius, np.arange(diameter) - radius)\n",
    "#kernel_spatial = np.exp(-1.0 * (x**2 + y**2) /  (2 * sigma_spatial**2))\n",
    "##kernel_spatial = np.repeat(kernel_spatial, 3).reshape(-1, 3)\n",
    "#kernel_spatial = np.reshape(-1,1)\n",
    "#\n",
    "## Lookup table for range kernel.\n",
    "#lut_range = np.exp(-1.0 * np.arange(256)**2 / (2 * sigma_range**2))\n",
    "#\n",
    "#def process_row(y):\n",
    "#    result = np.zeros((ref.shape[1], NB_CHANNELS))\n",
    "#    y += padding\n",
    "#    for x in range(padding, reference.shape[1] - padding):\n",
    "#        I_p = reference[y, x]\n",
    "#        patch_reference = reference[y - padding:y + padding + 1:step, x - padding:x + padding + 1:step].reshape(-1, 3)\n",
    "#        patch_source_upsampled = source_upsampled[y - padding:y + padding + 1:step, x - padding:x + padding + 1:step].reshape(-1, NB_CHANNELS)\n",
    "#\n",
    "#        kernel_range = lut_range[np.abs(patch_reference - I_p).astype(int)]\n",
    "#        norm = np.linalg.norm(patch_reference-I_p, axis=1, keepdims=True)\n",
    "#        kernel_range = np.exp(-1.0 * norm**2 / (2 * sigma_range**2))\n",
    "#        weight = kernel_range * kernel_spatial\n",
    "#        prod = weight * patch_source_upsampled\n",
    "#        k_p = weight.sum(axis=0)\n",
    "#        res = np.round(np.sum(weight * patch_source_upsampled, axis=0) / k_p)\n",
    "#        result[x - padding] = res\n",
    "#    return result\n",
    "#\n",
    "#executor = ProcessPoolExecutor()\n",
    "#result = executor.map(process_row, range(ref.shape[0]))\n",
    "#executor.shutdown(True)\n",
    "#hr_feats = torch.Tensor(np.array(list(result))) \n",
    "#print(hr_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "519d7304-0dc9-4e53-a239-7e3b2eb84c78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#[lr_feats_pca], _ = pca([lr_feats])\n",
    "#[hr_feats_bili_pca], _ = pca([hr_feats_bili])\n",
    "#[hr_feats_jbu_pca], _ = pca([hr_feats.permute(2,0,1).unsqueeze(0)])\n",
    "#\n",
    "#fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "#ax[0,0].imshow(v2.functional.to_pil_image(unnorm(image).squeeze(0)))\n",
    "#ax[0,0].set_title(\"Ref Image\")\n",
    "#ax[0,1].imshow(v2.functional.to_pil_image(lr_feats_pca[0]))\n",
    "#ax[0,1].set_title(\"low res features\")\n",
    "#ax[1,0].imshow(v2.functional.to_pil_image(hr_feats_bili_pca[0]))\n",
    "#ax[1,0].set_title(\"Bilinear upsampled features\")\n",
    "#ax[1,1].imshow(v2.functional.to_pil_image(hr_feats_jbu_pca[0]))\n",
    "#ax[1,1].set_title(\"JBU features\")\n",
    "#remove_axes(ax)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6545e26b-437d-4dca-a41a-680bb3242b07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['encoder.blocks.11.mlp.fc1.parametrizations.weight.0.lora_B', 'encoder.blocks.11.mlp.fc2.bias', 'encoder.blocks.11.mlp.fc2.parametrizations.weight.original', 'encoder.blocks.11.mlp.fc2.parametrizations.weight.0.lora_A', 'encoder.blocks.11.mlp.fc2.parametrizations.weight.0.lora_B', 'encoder.blocks.11.ls2.gamma', 'encoder.norm.weight', 'encoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias']\n",
      "The model will start training with only 302612 trainable parameters out of 22358804.\n"
     ]
    }
   ],
   "source": [
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "class DecoderLinear(nn.Module):\n",
    "    def __init__(self, n_cls, patch_size, d_encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_encoder = d_encoder\n",
    "        self.patch_size = patch_size\n",
    "        self.n_cls = n_cls\n",
    "\n",
    "        self.head = nn.Linear(self.d_encoder, n_cls)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return set()\n",
    "\n",
    "    def forward(self, x, im_size):\n",
    "        H, W = im_size\n",
    "        GS = H // self.patch_size\n",
    "        x = self.head(x)\n",
    "        x = rearrange(x, \"b (h w) c -> b c h w\", h=GS)\n",
    "\n",
    "        return x\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import minlora \n",
    "import torchmetrics as M\n",
    "import pytorch_lightning as pl\n",
    "from dl_toolbox import losses\n",
    "\n",
    "def get_lora_config(rank):\n",
    "    return {  # specify which layers to add lora to, by default only add to linear layers\n",
    "        nn.Linear: {\n",
    "            \"weight\": partial(minlora.LoRAParametrization.from_linear, rank=rank),\n",
    "        },\n",
    "    }\n",
    "\n",
    "class Segmenter(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        n_cls,\n",
    "        freeze,\n",
    "        lora,\n",
    "        rank\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_cls = n_cls\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        if freeze:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if lora:\n",
    "            cfg = get_lora_config(rank)\n",
    "            minlora.add_lora(self.encoder, lora_config=cfg)\n",
    "        self.acc = M.Accuracy(task='multiclass', num_classes=20)\n",
    "        self.loss =  losses.CrossEntropy()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = schedulefree.AdamWScheduleFree(self.parameters(), lr=0.0025)\n",
    "        return opt\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        print('\\n opt train')\n",
    "        self.optimizers().train()\n",
    "        \n",
    "    def on_validation_start(self):\n",
    "        print('\\n opt eval')\n",
    "        self.optimizers().eval()  \n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        def append_prefix_no_weight_decay(prefix, module):\n",
    "            return set(map(lambda x: prefix + x, module.no_weight_decay()))\n",
    "\n",
    "        nwd_params = append_prefix_no_weight_decay(\"encoder.\", self.encoder).union(\n",
    "            append_prefix_no_weight_decay(\"decoder.\", self.decoder)\n",
    "        )\n",
    "        return nwd_params\n",
    "\n",
    "    def forward(self, im):\n",
    "        H_ori, W_ori = im.size(2), im.size(3)\n",
    "        H, W = im.size(2), im.size(3)\n",
    "\n",
    "        #x = self.encoder(im, return_features=True)\n",
    "        x = self.encoder.forward_features(im)\n",
    "        \n",
    "\n",
    "        # remove CLS/DIST tokens for decoding\n",
    "        #num_extra_tokens = 1 + self.encoder.distilled\n",
    "        #x = x[:, num_extra_tokens:]\n",
    "        x = x[:,self.encoder.num_prefix_tokens:,...]\n",
    "\n",
    "        masks = self.decoder(x, (H, W))\n",
    "\n",
    "        masks = F.interpolate(masks, size=(H, W), mode=\"bilinear\")\n",
    "        #masks = unpadding(masks, (H_ori, W_ori))\n",
    "\n",
    "        return masks\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch = batch[\"sup\"]\n",
    "        x = batch[\"image\"]\n",
    "        y = batch[\"label\"]\n",
    "        outputs = self.forward(x)\n",
    "        loss = self.loss(outputs, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[\"image\"]\n",
    "        y = batch[\"label\"]\n",
    "        outputs = self.forward(x)\n",
    "        loss = self.loss(outputs, y)\n",
    "        acc = self.acc.update(outputs, y)\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        acc = self.acc.compute()\n",
    "        print(\"\\nAcc: \", acc)\n",
    "        self.acc.reset()\n",
    "\n",
    "decoder = DecoderLinear(n_cls=20, d_encoder=encoder.embed_dim, patch_size=14)\n",
    "model = Segmenter(encoder,decoder,n_cls=20, freeze=True, lora=True, rank=4)\n",
    "\n",
    "def name_is_lora(name):\n",
    "    return (\n",
    "        len(name.split(\".\")) >= 4\n",
    "        and (name.split(\".\")[-4]) == \"parametrizations\"\n",
    "        and name.split(\".\")[-1] in [\"lora_A\", \"lora_B\"]\n",
    "    )\n",
    "\n",
    "def name_is_decoder(name):\n",
    "    return (name.split(\".\")[0]) == \"decoder\"\n",
    "\n",
    "def lora_or_decoder(name):\n",
    "    return name_is_lora(name) or name_is_decoder(name)\n",
    "\n",
    "def get_params_by_name(model, print_shapes=False, name_filter=None):\n",
    "    for n, p in model.named_parameters():\n",
    "        if name_filter is None or name_filter(n):\n",
    "            if print_shapes:\n",
    "                print(n, p.shape)\n",
    "            yield p\n",
    "\n",
    "print([p[0] for p in list(model.named_parameters())][-10:])\n",
    "parameters = list(model.parameters())\n",
    "trainable_parameters = list(get_params_by_name(model, name_filter=lora_or_decoder))\n",
    "print(\n",
    "    f\"The model will start training with only {sum([int(torch.numel(p)) for p in trainable_parameters])} \"\n",
    "    f\"trainable parameters out of {sum([int(torch.numel(p)) for p in parameters])}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc757141-3d97-4622-96bc-91b0f1bc5f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "Missing logger folder: /d/pfournie/dl_toolbox/dl_toolbox/à ranger/segmenter/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type               | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder | VisionTransformer  | 22.4 M\n",
      "1 | decoder | DecoderLinear      | 7.7 K \n",
      "2 | acc     | MulticlassAccuracy | 0     \n",
      "3 | loss    | CrossEntropy       | 0     \n",
      "-----------------------------------------------\n",
      "302 K     Trainable params\n",
      "22.1 M    Non-trainable params\n",
      "22.4 M    Total params\n",
      "89.435    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\n",
      " opt eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "Acc:  tensor(0.0060, device='cuda:0')\n",
      "                                                                                                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                            | 0/1 [00:00<?, ?it/s]\n",
      " opt train\n",
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.14s/it, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.4648, device='cuda:0')\n",
      "Epoch 1:   0%|                                                                                                                   | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.23it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.6410, device='cuda:0')\n",
      "Epoch 2:   0%|                                                                                                                   | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.02s/it, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.7775, device='cuda:0')\n",
      "Epoch 3:   0%|                                                                                                                   | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.18it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.8145, device='cuda:0')\n",
      "Epoch 4:   0%|                                                                                                                   | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.33it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.8642, device='cuda:0')\n",
      "Epoch 5:   0%|                                                                                                                   | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.07it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.8380, device='cuda:0')\n",
      "Epoch 6:   0%|                                                                                                                   | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 6: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.04it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.9003, device='cuda:0')\n",
      "Epoch 7:   0%|                                                                                                                   | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 7: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.23it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.8209, device='cuda:0')\n",
      "Epoch 8:   0%|                                                                                                                   | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.17it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.8420, device='cuda:0')\n",
      "Epoch 9:   0%|                                                                                                                   | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 9: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.27it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.8582, device='cuda:0')\n",
      "Epoch 10:   0%|                                                                                                                  | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.24it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.8468, device='cuda:0')\n",
      "Epoch 11:   0%|                                                                                                                  | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 11: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.28it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.8753, device='cuda:0')\n",
      "Epoch 12:   0%|                                                                                                                  | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.31it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.8539, device='cuda:0')\n",
      "Epoch 13:   0%|                                                                                                                  | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 13: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.28it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.8296, device='cuda:0')\n",
      "Epoch 14:   0%|                                                                                                                  | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 14: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.09it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.8694, device='cuda:0')\n",
      "Epoch 15:   0%|                                                                                                                  | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 15: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.24it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.9067, device='cuda:0')\n",
      "Epoch 16:   0%|                                                                                                                  | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n",
      "Epoch 16: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.20it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.8366, device='cuda:0')\n",
      "Epoch 17:   0%|                                                                                                                  | 0/1 [00:00<?, ?it/s, v_num=0]\n",
      " opt train\n"
     ]
    }
   ],
   "source": [
    "from dl_toolbox import datamodules\n",
    "import schedulefree\n",
    "from dl_toolbox.callbacks import ProgressBar\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "\n",
    "tf = v2.Compose([\n",
    "    v2.RandomCrop(size=(672, 672)),\n",
    "    v2.ToDtype(\n",
    "        dtype={tv_tensors.Image: torch.float32, tv_tensors.Mask: torch.int64, \"others\":None}, \n",
    "        scale=True\n",
    "    ),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dm = datamodules.Rellis3d(\n",
    "    data_path='/data',\n",
    "    merge='all19',\n",
    "    sup=1,\n",
    "    unsup=5,\n",
    "    train_tf=tf,\n",
    "    test_tf=tf,\n",
    "    batch_size_s=4,\n",
    "    batch_size_u=4,\n",
    "    steps_per_epoch=10,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    limit_train_batches=1,\n",
    "    limit_val_batches=1,\n",
    "    callbacks=[ProgressBar()]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    datamodule=dm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d656883-ed54-4200-912e-e196df77f5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
