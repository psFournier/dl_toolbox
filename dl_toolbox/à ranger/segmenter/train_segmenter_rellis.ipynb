{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b61bc75-88a0-4b5b-8f06-03fcc1d8df9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from functools import partial\n",
    "\n",
    "encoder = timm.create_model('vit_small_patch14_dinov2', pretrained=True, dynamic_img_size=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c186ba07-5a59-4a0d-9c0d-b17a397e1405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.v2 as v2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image2 = torch.clone(image)\n",
    "        if len(image2.shape) == 4:\n",
    "            # batched\n",
    "            image2 = image2.permute(1, 0, 2, 3)\n",
    "        for t, m, s in zip(image2, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return image2.permute(1, 0, 2, 3)\n",
    "    \n",
    "norm = v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "unnorm = UnNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "class TorchPCA(object):\n",
    "\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean_ = X.mean(dim=0)\n",
    "        unbiased = X - self.mean_.unsqueeze(0)\n",
    "        U, S, V = torch.pca_lowrank(unbiased, q=self.n_components, center=False, niter=4)\n",
    "        self.components_ = V.T\n",
    "        self.singular_values_ = S\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        t0 = X - self.mean_.unsqueeze(0)\n",
    "        projected = t0 @ self.components_.T\n",
    "        return projected\n",
    "\n",
    "\n",
    "def pca(image_feats_list, dim=3, fit_pca=None, max_samples=None):\n",
    "    device = image_feats_list[0].device\n",
    "\n",
    "    def flatten(tensor, target_size=None):\n",
    "        if target_size is not None and fit_pca is None:\n",
    "            tensor = F.interpolate(tensor, (target_size, target_size), mode=\"bilinear\")\n",
    "        B, C, H, W = tensor.shape\n",
    "        return tensor.permute(1, 0, 2, 3).reshape(C, B * H * W).permute(1, 0).detach().cpu()\n",
    "\n",
    "    if len(image_feats_list) > 1 and fit_pca is None:\n",
    "        target_size = image_feats_list[0].shape[2]\n",
    "    else:\n",
    "        target_size = None\n",
    "\n",
    "    flattened_feats = []\n",
    "    for feats in image_feats_list:\n",
    "        flattened_feats.append(flatten(feats, target_size))\n",
    "    x = torch.cat(flattened_feats, dim=0)\n",
    "\n",
    "    # Subsample the data if max_samples is set and the number of samples exceeds max_samples\n",
    "    if max_samples is not None and x.shape[0] > max_samples:\n",
    "        indices = torch.randperm(x.shape[0])[:max_samples]\n",
    "        x = x[indices]\n",
    "\n",
    "    if fit_pca is None:\n",
    "        fit_pca = TorchPCA(n_components=dim).fit(x)\n",
    "\n",
    "    reduced_feats = []\n",
    "    for feats in image_feats_list:\n",
    "        x_red = fit_pca.transform(flatten(feats))\n",
    "        if isinstance(x_red, np.ndarray):\n",
    "            x_red = torch.from_numpy(x_red)\n",
    "        x_red -= x_red.min(dim=0, keepdim=True).values\n",
    "        x_red /= x_red.max(dim=0, keepdim=True).values\n",
    "        B, C, H, W = feats.shape\n",
    "        reduced_feats.append(x_red.reshape(B, H, W, dim).permute(0, 3, 1, 2).to(device))\n",
    "\n",
    "    return reduced_feats, fit_pca\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "def _remove_axes(ax):\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "\n",
    "def remove_axes(axes):\n",
    "    if len(axes.shape) == 2:\n",
    "        for ax1 in axes:\n",
    "            for ax in ax1:\n",
    "                _remove_axes(ax)\n",
    "    else:\n",
    "        for ax in axes:\n",
    "            _remove_axes(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6340ec7f-c96d-4a33-8fd7-2d99d2be6f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from dl_toolbox.datasets import Rellis3d\n",
    "#from torchvision import tv_tensors\n",
    "#\n",
    "#tf = v2.Compose([\n",
    "#    v2.RandomCrop(size=(672, 672)),\n",
    "#    v2.ToDtype(\n",
    "#        dtype={tv_tensors.Image: torch.float32, tv_tensors.Mask: torch.int64, \"others\":None}, \n",
    "#        scale=True\n",
    "#    ),\n",
    "#    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#])\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "#rellis = '/data/Rellis-3D'\n",
    "#imgs = [rellis+'/00000/pylon_camera_node/frame000000-1581624652_750.jpg']\n",
    "#msks = [rellis+'/00000/pylon_camera_node_label_id/frame000000-1581624652_750.png']\n",
    "#dataset = Rellis3d(\n",
    "#    imgs=imgs,\n",
    "#    msks=msks,\n",
    "#    merge='all19',\n",
    "#    transforms=tf\n",
    "#)\n",
    "#elem = dataset[0]\n",
    "#image, mask = elem['image'].to(device).unsqueeze(0), elem['label']\n",
    "#h = 672 // 14\n",
    "#w = 672 // 14\n",
    "#encoder.to(device)\n",
    "#lr_feats = encoder.forward_features(image)\n",
    "#lr_feats = lr_feats[:,encoder.num_prefix_tokens:,...]\n",
    "#lr_feats = lr_feats.reshape(-1, h, w, 384).permute(0,3,1,2).detach().cpu()\n",
    "#hr_feats_bili = v2.functional.resize(lr_feats, (672, 672), Image.BILINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8755c249-dc89-41bb-bb2b-3b40f80ba6fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from concurrent.futures import ProcessPoolExecutor\n",
    "#\n",
    "#ref = image[0].permute(1,2,0).to('cpu').numpy()\n",
    "#source_upsampled = hr_feats_bili[0].permute(1,2,0).to('cpu').numpy()\n",
    "#\n",
    "#NB_CHANNELS = 384\n",
    "#\n",
    "#scale = 48 / 672\n",
    "#radius = 2\n",
    "#diameter = 2 * radius + 1\n",
    "#step = int(np.ceil(1 / scale))\n",
    "#padding = radius * step\n",
    "#sigma_spatial = 2.5\n",
    "#sigma_range = np.std(ref)\n",
    "#\n",
    "#reference = np.pad(ref, ((padding, padding), (padding, padding), (0, 0)), 'symmetric').astype(np.float32)\n",
    "#source_upsampled = np.pad(source_upsampled, ((padding, padding), (padding, padding), (0, 0)), 'symmetric').astype(np.float32)\n",
    "#\n",
    "## Spatial Gaussian function.\n",
    "#x, y = np.meshgrid(np.arange(diameter) - radius, np.arange(diameter) - radius)\n",
    "#kernel_spatial = np.exp(-1.0 * (x**2 + y**2) /  (2 * sigma_spatial**2))\n",
    "##kernel_spatial = np.repeat(kernel_spatial, 3).reshape(-1, 3)\n",
    "#kernel_spatial = np.reshape(-1,1)\n",
    "#\n",
    "## Lookup table for range kernel.\n",
    "#lut_range = np.exp(-1.0 * np.arange(256)**2 / (2 * sigma_range**2))\n",
    "#\n",
    "#def process_row(y):\n",
    "#    result = np.zeros((ref.shape[1], NB_CHANNELS))\n",
    "#    y += padding\n",
    "#    for x in range(padding, reference.shape[1] - padding):\n",
    "#        I_p = reference[y, x]\n",
    "#        patch_reference = reference[y - padding:y + padding + 1:step, x - padding:x + padding + 1:step].reshape(-1, 3)\n",
    "#        patch_source_upsampled = source_upsampled[y - padding:y + padding + 1:step, x - padding:x + padding + 1:step].reshape(-1, NB_CHANNELS)\n",
    "#\n",
    "#        kernel_range = lut_range[np.abs(patch_reference - I_p).astype(int)]\n",
    "#        norm = np.linalg.norm(patch_reference-I_p, axis=1, keepdims=True)\n",
    "#        kernel_range = np.exp(-1.0 * norm**2 / (2 * sigma_range**2))\n",
    "#        weight = kernel_range * kernel_spatial\n",
    "#        prod = weight * patch_source_upsampled\n",
    "#        k_p = weight.sum(axis=0)\n",
    "#        res = np.round(np.sum(weight * patch_source_upsampled, axis=0) / k_p)\n",
    "#        result[x - padding] = res\n",
    "#    return result\n",
    "#\n",
    "#executor = ProcessPoolExecutor()\n",
    "#result = executor.map(process_row, range(ref.shape[0]))\n",
    "#executor.shutdown(True)\n",
    "#hr_feats = torch.Tensor(np.array(list(result))) \n",
    "#print(hr_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "519d7304-0dc9-4e53-a239-7e3b2eb84c78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#[lr_feats_pca], _ = pca([lr_feats])\n",
    "#[hr_feats_bili_pca], _ = pca([hr_feats_bili])\n",
    "#[hr_feats_jbu_pca], _ = pca([hr_feats.permute(2,0,1).unsqueeze(0)])\n",
    "#\n",
    "#fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "#ax[0,0].imshow(v2.functional.to_pil_image(unnorm(image).squeeze(0)))\n",
    "#ax[0,0].set_title(\"Ref Image\")\n",
    "#ax[0,1].imshow(v2.functional.to_pil_image(lr_feats_pca[0]))\n",
    "#ax[0,1].set_title(\"low res features\")\n",
    "#ax[1,0].imshow(v2.functional.to_pil_image(hr_feats_bili_pca[0]))\n",
    "#ax[1,0].set_title(\"Bilinear upsampled features\")\n",
    "#ax[1,1].imshow(v2.functional.to_pil_image(hr_feats_jbu_pca[0]))\n",
    "#ax[1,1].set_title(\"JBU features\")\n",
    "#remove_axes(ax)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6545e26b-437d-4dca-a41a-680bb3242b07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['encoder.blocks.11.mlp.fc1.parametrizations.weight.0.lora_B', 'encoder.blocks.11.mlp.fc2.bias', 'encoder.blocks.11.mlp.fc2.parametrizations.weight.original', 'encoder.blocks.11.mlp.fc2.parametrizations.weight.0.lora_A', 'encoder.blocks.11.mlp.fc2.parametrizations.weight.0.lora_B', 'encoder.blocks.11.ls2.gamma', 'encoder.norm.weight', 'encoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias']\n",
      "The model will start training with only 302612 trainable parameters out of 22358804.\n"
     ]
    }
   ],
   "source": [
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "class DecoderLinear(nn.Module):\n",
    "    def __init__(self, n_cls, patch_size, d_encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_encoder = d_encoder\n",
    "        self.patch_size = patch_size\n",
    "        self.n_cls = n_cls\n",
    "\n",
    "        self.head = nn.Linear(self.d_encoder, n_cls)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return set()\n",
    "\n",
    "    def forward(self, x, im_size):\n",
    "        H, W = im_size\n",
    "        GS = H // self.patch_size\n",
    "        x = self.head(x)\n",
    "        x = rearrange(x, \"b (h w) c -> b c h w\", h=GS)\n",
    "\n",
    "        return x\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import minlora \n",
    "import torchmetrics as M\n",
    "import pytorch_lightning as pl\n",
    "from dl_toolbox import losses\n",
    "\n",
    "def get_lora_config(rank):\n",
    "    return {  # specify which layers to add lora to, by default only add to linear layers\n",
    "        nn.Linear: {\n",
    "            \"weight\": partial(minlora.LoRAParametrization.from_linear, rank=rank),\n",
    "        },\n",
    "    }\n",
    "\n",
    "class Segmenter(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        n_cls,\n",
    "        freeze,\n",
    "        lora,\n",
    "        rank\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_cls = n_cls\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        if freeze:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if lora:\n",
    "            cfg = get_lora_config(rank)\n",
    "            minlora.add_lora(self.encoder, lora_config=cfg)\n",
    "        self.acc = M.Accuracy(task='multiclass', num_classes=20)\n",
    "        self.loss =  losses.CrossEntropy()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = schedulefree.AdamWScheduleFree(self.parameters(), lr=0.0025)\n",
    "        return opt\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        print('\\n opt train')\n",
    "        self.optimizers().train()\n",
    "        \n",
    "    def on_validation_start(self):\n",
    "        print('\\n opt eval')\n",
    "        self.optimizers().eval()  \n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        def append_prefix_no_weight_decay(prefix, module):\n",
    "            return set(map(lambda x: prefix + x, module.no_weight_decay()))\n",
    "\n",
    "        nwd_params = append_prefix_no_weight_decay(\"encoder.\", self.encoder).union(\n",
    "            append_prefix_no_weight_decay(\"decoder.\", self.decoder)\n",
    "        )\n",
    "        return nwd_params\n",
    "\n",
    "    def forward(self, im):\n",
    "        H_ori, W_ori = im.size(2), im.size(3)\n",
    "        H, W = im.size(2), im.size(3)\n",
    "\n",
    "        #x = self.encoder(im, return_features=True)\n",
    "        x = self.encoder.forward_features(im)\n",
    "        \n",
    "\n",
    "        # remove CLS/DIST tokens for decoding\n",
    "        #num_extra_tokens = 1 + self.encoder.distilled\n",
    "        #x = x[:, num_extra_tokens:]\n",
    "        x = x[:,self.encoder.num_prefix_tokens:,...]\n",
    "\n",
    "        masks = self.decoder(x, (H, W))\n",
    "\n",
    "        masks = F.interpolate(masks, size=(H, W), mode=\"bilinear\")\n",
    "        #masks = unpadding(masks, (H_ori, W_ori))\n",
    "\n",
    "        return masks\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch = batch[\"sup\"]\n",
    "        x = batch[\"image\"]\n",
    "        y = batch[\"label\"]\n",
    "        outputs = self.forward(x)\n",
    "        loss = self.loss(outputs, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[\"image\"]\n",
    "        y = batch[\"label\"]\n",
    "        outputs = self.forward(x)\n",
    "        loss = self.loss(outputs, y)\n",
    "        acc = self.acc.update(outputs, y)\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        acc = self.acc.compute()\n",
    "        print(\"\\nAcc: \", acc)\n",
    "        self.acc.reset()\n",
    "\n",
    "decoder = DecoderLinear(n_cls=20, d_encoder=encoder.embed_dim, patch_size=14)\n",
    "model = Segmenter(encoder,decoder,n_cls=20, freeze=True, lora=True, rank=4)\n",
    "\n",
    "def name_is_lora(name):\n",
    "    return (\n",
    "        len(name.split(\".\")) >= 4\n",
    "        and (name.split(\".\")[-4]) == \"parametrizations\"\n",
    "        and name.split(\".\")[-1] in [\"lora_A\", \"lora_B\"]\n",
    "    )\n",
    "\n",
    "def name_is_decoder(name):\n",
    "    return (name.split(\".\")[0]) == \"decoder\"\n",
    "\n",
    "def lora_or_decoder(name):\n",
    "    return name_is_lora(name) or name_is_decoder(name)\n",
    "\n",
    "def get_params_by_name(model, print_shapes=False, name_filter=None):\n",
    "    for n, p in model.named_parameters():\n",
    "        if name_filter is None or name_filter(n):\n",
    "            if print_shapes:\n",
    "                print(n, p.shape)\n",
    "            yield p\n",
    "\n",
    "print([p[0] for p in list(model.named_parameters())][-10:])\n",
    "parameters = list(model.parameters())\n",
    "trainable_parameters = list(get_params_by_name(model, name_filter=lora_or_decoder))\n",
    "print(\n",
    "    f\"The model will start training with only {sum([int(torch.numel(p)) for p in trainable_parameters])} \"\n",
    "    f\"trainable parameters out of {sum([int(torch.numel(p)) for p in parameters])}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc757141-3d97-4622-96bc-91b0f1bc5f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type               | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder | VisionTransformer  | 22.4 M\n",
      "1 | decoder | DecoderLinear      | 7.7 K \n",
      "2 | acc     | MulticlassAccuracy | 0     \n",
      "3 | loss    | CrossEntropy       | 0     \n",
      "-----------------------------------------------\n",
      "302 K     Trainable params\n",
      "22.1 M    Non-trainable params\n",
      "22.4 M    Total params\n",
      "89.435    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\n",
      " opt eval\n",
      "Sanity Checking DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.29it/s]\n",
      "Acc:  tensor(0.0026, device='cuda:0')\n",
      "Epoch 0:   0%|                                                                                                                          | 0/100 [00:00<?, ?it/s]\n",
      " opt train\n",
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:58<00:00,  1.72it/s, v_num=2]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.8910, device='cuda:0')\n",
      "Epoch 1:   0%|                                                                                                                 | 0/100 [00:00<?, ?it/s, v_num=2]\n",
      " opt train\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:58<00:00,  1.70it/s, v_num=2]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      " opt eval\n",
      "\n",
      "Acc:  tensor(0.9050, device='cuda:0')\n",
      "Epoch 2:   0%|                                                                                                                 | 0/100 [00:00<?, ?it/s, v_num=2]\n",
      " opt train\n",
      "Epoch 2:  51%|█████████████████████████████████████████████████████                                                   | 51/100 [00:30<00:29,  1.67it/s, v_num=2]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 3.\nOriginal Traceback (most recent call last):\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/d/pfournie/dl_toolbox/dl_toolbox/utils/collate_fn.py\", line 16, in __call__\n    collated_batch = default_collate(to_collate)\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 129, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 129, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m\n\u001b[1;32m     16\u001b[0m dm \u001b[38;5;241m=\u001b[39m datamodules\u001b[38;5;241m.\u001b[39mRellis3d(\n\u001b[1;32m     17\u001b[0m     data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     merge\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall19\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     31\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[ProgressBar()]\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdm\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    518\u001b[0m model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 520\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    550\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    553\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    555\u001b[0m     ckpt_path,\n\u001b[1;32m    556\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    557\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m )\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:978\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m--> 978\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:201\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:354\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py:189\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loop\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    188\u001b[0m batch_idx \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mfetched \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_fetcher, _DataLoaderIterDataFetcher) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 189\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    192\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py:136\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;66;03m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py:150\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_profiler()\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py:276\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py:64\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m         out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumed[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/dl_toolbox/venv/lib/python3.8/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 3.\nOriginal Traceback (most recent call last):\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/d/pfournie/dl_toolbox/dl_toolbox/utils/collate_fn.py\", line 16, in __call__\n    collated_batch = default_collate(to_collate)\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 129, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 129, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "from dl_toolbox import datamodules\n",
    "import schedulefree\n",
    "from dl_toolbox.callbacks import ProgressBar\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "\n",
    "tf = v2.Compose([\n",
    "    v2.RandomCrop(size=(672, 672)),\n",
    "    v2.ToDtype(\n",
    "        dtype={tv_tensors.Image: torch.float32, tv_tensors.Mask: torch.int64, \"others\":None}, \n",
    "        scale=True\n",
    "    ),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dm = datamodules.Rellis3d(\n",
    "    data_path='/data',\n",
    "    merge='all19',\n",
    "    sup=1,\n",
    "    unsup=5,\n",
    "    train_tf=tf,\n",
    "    test_tf=tf,\n",
    "    batch_size_s=4,\n",
    "    batch_size_u=4,\n",
    "    steps_per_epoch=100,\n",
    "    num_workers=6,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    limit_train_batches=1.,\n",
    "    limit_val_batches=1.,\n",
    "    callbacks=[ProgressBar()]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    datamodule=dm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d656883-ed54-4200-912e-e196df77f5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
