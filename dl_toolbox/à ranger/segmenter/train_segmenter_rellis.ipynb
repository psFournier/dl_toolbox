{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc757141-3d97-4622-96bc-91b0f1bc5f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dl_toolbox import datamodules\n",
    "from torchvision import tv_tensors\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "tf_train = v2.Compose([\n",
    "    v2.RandomCrop(size=(504, 504)),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "tf_test = v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "dm = datamodules.Flair(\n",
    "    data_path='/data',\n",
    "    merge='main13',\n",
    "    bands=[1,2,3],\n",
    "    sup=1,\n",
    "    unsup=0,\n",
    "    train_tf=tf_train,\n",
    "    test_tf=tf_test,\n",
    "    batch_size=4,\n",
    "    num_workers=6,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18474101-35b9-4fef-98fa-6eb23069121d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing domains\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /tmp exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                      | Params\n",
      "-----------------------------------------------------------\n",
      "0 | encoder      | VisionTransformer         | 22.4 M\n",
      "1 | decoder      | DecoderLinear             | 5.0 K \n",
      "2 | loss         | CrossEntropy              | 0     \n",
      "3 | val_accuracy | MulticlassAccuracy        | 0     \n",
      "4 | val_cm       | MulticlassConfusionMatrix | 0     \n",
      "5 | val_jaccard  | MulticlassJaccardIndex    | 0     \n",
      "-----------------------------------------------------------\n",
      "299 K     Trainable params\n",
      "22.1 M    Non-trainable params\n",
      "22.4 M    Total params\n",
      "89.424    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will start training with only 299917 trainable parameters out of 22356109.\n",
      "1 params do not undergo weight decay\n",
      "Sanity Checking DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  0.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: 26 NaN values found in confusion matrix have been replaced with zeros.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  0.74it/s, v_num=35]\n",
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  0.45it/s, v_num=35]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  0.40it/s, v_num=35]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from dl_toolbox.callbacks import ProgressBar, FeatureFt, Lora, TiffPredsWriter\n",
    "from dl_toolbox.modules import Segmenter\n",
    "from functools import partial\n",
    "from dl_toolbox.losses import CrossEntropy\n",
    "from dl_toolbox.transforms import Sliding\n",
    "\n",
    "module = Segmenter(\n",
    "    num_classes=13,\n",
    "    backbone='vit_small_patch14_dinov2',\n",
    "    optimizer=partial(torch.optim.Adam, lr=0.001),\n",
    "    scheduler=partial(torch.optim.lr_scheduler.ConstantLR, factor=1),\n",
    "    loss=CrossEntropy(),\n",
    "    batch_tf=None,\n",
    "    metric_ignore_index=None,\n",
    "    tta=None,\n",
    "    sliding=Sliding(\n",
    "        nols=512,\n",
    "        nrows=512,\n",
    "        width=504,\n",
    "        height=504,\n",
    "        step_w=500,\n",
    "        step_h=500\n",
    "    )\n",
    ")\n",
    "\n",
    "ckpt = ModelCheckpoint(\n",
    "    dirpath='/tmp',\n",
    "    filename=\"epoch_{epoch:03d}\",\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "lora = Lora('encoder', 4)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=1,\n",
    "    limit_train_batches=1,\n",
    "    limit_val_batches=1,\n",
    "    callbacks=[ProgressBar(), lora, ckpt]\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    module,\n",
    "    datamodule=dm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f373a6ec-6e6c-47d4-9780-03c55743be08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing domains\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /data/outputs/flair_segmenter/remote_ckpt/2024-05-16_115135/0/checkpoints/last.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /data/outputs/flair_segmenter/remote_ckpt/2024-05-16_115135/0/checkpoints/last.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:34<00:00,  2.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>pred_path</th>\n",
       "      <th>conf</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/data/FLAIR_1/train/D058_2020/Z3_UU/img/IMG_03...</td>\n",
       "      <td>/tmp/preds/FLAIR_1/train/D058_2020/Z3_UU/img/I...</td>\n",
       "      <td>0.877644</td>\n",
       "      <td>tensor(0.8700)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/data/FLAIR_1/train/D017_2018/Z17_AA/img/IMG_0...</td>\n",
       "      <td>/tmp/preds/FLAIR_1/train/D017_2018/Z17_AA/img/...</td>\n",
       "      <td>0.761988</td>\n",
       "      <td>tensor(0.8269)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/data/FLAIR_1/train/D072_2019/Z6_UU/img/IMG_04...</td>\n",
       "      <td>/tmp/preds/FLAIR_1/train/D072_2019/Z6_UU/img/I...</td>\n",
       "      <td>0.826615</td>\n",
       "      <td>tensor(0.7786)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/data/FLAIR_1/train/D074_2020/Z1_NN/img/IMG_05...</td>\n",
       "      <td>/tmp/preds/FLAIR_1/train/D074_2020/Z1_NN/img/I...</td>\n",
       "      <td>0.906497</td>\n",
       "      <td>tensor(0.9382)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/data/FLAIR_1/train/D058_2020/Z11_FA/img/IMG_0...</td>\n",
       "      <td>/tmp/preds/FLAIR_1/train/D058_2020/Z11_FA/img/...</td>\n",
       "      <td>0.953491</td>\n",
       "      <td>tensor(0.9882)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>/data/FLAIR_1/train/D063_2019/Z17_AF/img/IMG_0...</td>\n",
       "      <td>/tmp/preds/FLAIR_1/train/D063_2019/Z17_AF/img/...</td>\n",
       "      <td>0.787233</td>\n",
       "      <td>tensor(0.8437)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>/data/FLAIR_1/train/D080_2021/Z1_UA/img/IMG_05...</td>\n",
       "      <td>/tmp/preds/FLAIR_1/train/D080_2021/Z1_UA/img/I...</td>\n",
       "      <td>0.707491</td>\n",
       "      <td>tensor(0.5589)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>/data/FLAIR_1/train/D058_2020/Z14_AA/img/IMG_0...</td>\n",
       "      <td>/tmp/preds/FLAIR_1/train/D058_2020/Z14_AA/img/...</td>\n",
       "      <td>0.872598</td>\n",
       "      <td>tensor(0.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>/data/FLAIR_1/train/D049_2020/Z5_UF/img/IMG_03...</td>\n",
       "      <td>/tmp/preds/FLAIR_1/train/D049_2020/Z5_UF/img/I...</td>\n",
       "      <td>0.775064</td>\n",
       "      <td>tensor(0.2953)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>/data/FLAIR_1/train/D070_2020/Z12_UF/img/IMG_0...</td>\n",
       "      <td>/tmp/preds/FLAIR_1/train/D070_2020/Z12_UF/img/...</td>\n",
       "      <td>0.703882</td>\n",
       "      <td>tensor(0.8642)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              img_path  \\\n",
       "0    /data/FLAIR_1/train/D058_2020/Z3_UU/img/IMG_03...   \n",
       "1    /data/FLAIR_1/train/D017_2018/Z17_AA/img/IMG_0...   \n",
       "2    /data/FLAIR_1/train/D072_2019/Z6_UU/img/IMG_04...   \n",
       "3    /data/FLAIR_1/train/D074_2020/Z1_NN/img/IMG_05...   \n",
       "4    /data/FLAIR_1/train/D058_2020/Z11_FA/img/IMG_0...   \n",
       "..                                                 ...   \n",
       "395  /data/FLAIR_1/train/D063_2019/Z17_AF/img/IMG_0...   \n",
       "396  /data/FLAIR_1/train/D080_2021/Z1_UA/img/IMG_05...   \n",
       "397  /data/FLAIR_1/train/D058_2020/Z14_AA/img/IMG_0...   \n",
       "398  /data/FLAIR_1/train/D049_2020/Z5_UF/img/IMG_03...   \n",
       "399  /data/FLAIR_1/train/D070_2020/Z12_UF/img/IMG_0...   \n",
       "\n",
       "                                             pred_path      conf  \\\n",
       "0    /tmp/preds/FLAIR_1/train/D058_2020/Z3_UU/img/I...  0.877644   \n",
       "1    /tmp/preds/FLAIR_1/train/D017_2018/Z17_AA/img/...  0.761988   \n",
       "2    /tmp/preds/FLAIR_1/train/D072_2019/Z6_UU/img/I...  0.826615   \n",
       "3    /tmp/preds/FLAIR_1/train/D074_2020/Z1_NN/img/I...  0.906497   \n",
       "4    /tmp/preds/FLAIR_1/train/D058_2020/Z11_FA/img/...  0.953491   \n",
       "..                                                 ...       ...   \n",
       "395  /tmp/preds/FLAIR_1/train/D063_2019/Z17_AF/img/...  0.787233   \n",
       "396  /tmp/preds/FLAIR_1/train/D080_2021/Z1_UA/img/I...  0.707491   \n",
       "397  /tmp/preds/FLAIR_1/train/D058_2020/Z14_AA/img/...  0.872598   \n",
       "398  /tmp/preds/FLAIR_1/train/D049_2020/Z5_UF/img/I...  0.775064   \n",
       "399  /tmp/preds/FLAIR_1/train/D070_2020/Z12_UF/img/...  0.703882   \n",
       "\n",
       "                acc  \n",
       "0    tensor(0.8700)  \n",
       "1    tensor(0.8269)  \n",
       "2    tensor(0.7786)  \n",
       "3    tensor(0.9382)  \n",
       "4    tensor(0.9882)  \n",
       "..              ...  \n",
       "395  tensor(0.8437)  \n",
       "396  tensor(0.5589)  \n",
       "397      tensor(0.)  \n",
       "398  tensor(0.2953)  \n",
       "399  tensor(0.8642)  \n",
       "\n",
       "[400 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from dl_toolbox.callbacks import ProgressBar, FeatureFt, Lora, TiffPredsWriter\n",
    "from dl_toolbox.modules import Segmenter\n",
    "from functools import partial\n",
    "from dl_toolbox.losses import CrossEntropy\n",
    "from dl_toolbox.transforms import Sliding\n",
    "\n",
    "module = Segmenter(\n",
    "    num_classes=13,\n",
    "    backbone='vit_small_patch14_dinov2',\n",
    "    optimizer=partial(torch.optim.Adam, lr=0.001),\n",
    "    scheduler=partial(torch.optim.lr_scheduler.ConstantLR, factor=1),\n",
    "    loss=CrossEntropy(),\n",
    "    batch_tf=None,\n",
    "    metric_ignore_index=None,\n",
    "    tta=None,\n",
    "    sliding=Sliding(\n",
    "        nols=512,\n",
    "        nrows=512,\n",
    "        width=504,\n",
    "        height=504,\n",
    "        step_w=500,\n",
    "        step_h=500\n",
    "    )\n",
    ")\n",
    "\n",
    "lora = Lora('encoder', 4)\n",
    "\n",
    "writer = TiffPredsWriter(\n",
    "    out_path='/tmp/preds',\n",
    "    base='/data'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=1,\n",
    "    limit_predict_batches=100,\n",
    "    callbacks=[lora, writer]\n",
    ")\n",
    "\n",
    "trainer.predict(\n",
    "    module,\n",
    "    datamodule=dm,\n",
    "    ckpt_path='/data/outputs/flair_segmenter/remote_ckpt/2024-05-16_115135/0/checkpoints/last.ckpt',\n",
    "    return_predictions=False\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(writer.out_path / 'stats.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27514187-f823-43e3-8c2d-66e8f999df97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
