{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63ac72-7420-4a2a-be3b-e59d6a96b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.v2 as v2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image2 = torch.clone(image)\n",
    "        if len(image2.shape) == 4:\n",
    "            # batched\n",
    "            image2 = image2.permute(1, 0, 2, 3)\n",
    "        for t, m, s in zip(image2, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return image2.permute(1, 0, 2, 3)\n",
    "    \n",
    "norm = v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "unnorm = UnNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "class TorchPCA(object):\n",
    "\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean_ = X.mean(dim=0)\n",
    "        unbiased = X - self.mean_.unsqueeze(0)\n",
    "        U, S, V = torch.pca_lowrank(unbiased, q=self.n_components, center=False, niter=4)\n",
    "        self.components_ = V.T\n",
    "        self.singular_values_ = S\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        t0 = X - self.mean_.unsqueeze(0)\n",
    "        projected = t0 @ self.components_.T\n",
    "        return projected\n",
    "\n",
    "\n",
    "def pca(image_feats_list, dim=3, fit_pca=None, max_samples=None):\n",
    "    device = image_feats_list[0].device\n",
    "\n",
    "    def flatten(tensor, target_size=None):\n",
    "        if target_size is not None and fit_pca is None:\n",
    "            tensor = F.interpolate(tensor, (target_size, target_size), mode=\"bilinear\")\n",
    "        B, C, H, W = tensor.shape\n",
    "        return tensor.permute(1, 0, 2, 3).reshape(C, B * H * W).permute(1, 0).detach().cpu()\n",
    "\n",
    "    if len(image_feats_list) > 1 and fit_pca is None:\n",
    "        target_size = image_feats_list[0].shape[2]\n",
    "    else:\n",
    "        target_size = None\n",
    "\n",
    "    flattened_feats = []\n",
    "    for feats in image_feats_list:\n",
    "        flattened_feats.append(flatten(feats, target_size))\n",
    "    x = torch.cat(flattened_feats, dim=0)\n",
    "\n",
    "    # Subsample the data if max_samples is set and the number of samples exceeds max_samples\n",
    "    if max_samples is not None and x.shape[0] > max_samples:\n",
    "        indices = torch.randperm(x.shape[0])[:max_samples]\n",
    "        x = x[indices]\n",
    "\n",
    "    if fit_pca is None:\n",
    "        fit_pca = TorchPCA(n_components=dim).fit(x)\n",
    "\n",
    "    reduced_feats = []\n",
    "    for feats in image_feats_list:\n",
    "        x_red = fit_pca.transform(flatten(feats))\n",
    "        if isinstance(x_red, np.ndarray):\n",
    "            x_red = torch.from_numpy(x_red)\n",
    "        x_red -= x_red.min(dim=0, keepdim=True).values\n",
    "        x_red /= x_red.max(dim=0, keepdim=True).values\n",
    "        B, C, H, W = feats.shape\n",
    "        reduced_feats.append(x_red.reshape(B, H, W, dim).permute(0, 3, 1, 2).to(device))\n",
    "\n",
    "    return reduced_feats, fit_pca\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "def _remove_axes(ax):\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "\n",
    "def remove_axes(axes):\n",
    "    if len(axes.shape) == 2:\n",
    "        for ax1 in axes:\n",
    "            for ax in ax1:\n",
    "                _remove_axes(ax)\n",
    "    else:\n",
    "        for ax in axes:\n",
    "            _remove_axes(ax)\n",
    "            \n",
    "#from dl_toolbox.datasets import Rellis3d\n",
    "#from torchvision import tv_tensors\n",
    "\n",
    "#tf = v2.Compose([\n",
    "#    v2.RandomCrop(size=(672, 672)),\n",
    "#    v2.ToDtype(\n",
    "#        dtype={tv_tensors.Image: torch.float32, tv_tensors.Mask: torch.int64, \"others\":None}, \n",
    "#        scale=True\n",
    "#    ),\n",
    "#    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#])\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "#rellis = '/data/Rellis-3D'\n",
    "#imgs = [rellis+'/00000/pylon_camera_node/frame000000-1581624652_750.jpg']\n",
    "#msks = [rellis+'/00000/pylon_camera_node_label_id/frame000000-1581624652_750.png']\n",
    "#dataset = Rellis3d(\n",
    "#    imgs=imgs,\n",
    "#    msks=msks,\n",
    "#    merge='all19',\n",
    "#    transforms=tf\n",
    "#)\n",
    "#elem = dataset[0]\n",
    "#image, mask = elem['image'].to(device).unsqueeze(0), elem['label']\n",
    "#h = 672 // 14\n",
    "#w = 672 // 14\n",
    "#encoder.to(device)\n",
    "#lr_feats = encoder.forward_features(image)\n",
    "#lr_feats = lr_feats[:,encoder.num_prefix_tokens:,...]\n",
    "#lr_feats = lr_feats.reshape(-1, h, w, 384).permute(0,3,1,2).detach().cpu()\n",
    "#hr_feats_bili = v2.functional.resize(lr_feats, (672, 672), Image.BILINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ce96cb-6b84-4e38-ad3d-0dae195e9f84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b61bc75-88a0-4b5b-8f06-03fcc1d8df9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "class DecoderLinear(nn.Module):\n",
    "    def __init__(self, n_cls, patch_size, d_encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_encoder = d_encoder\n",
    "        self.patch_size = patch_size\n",
    "        self.n_cls = n_cls\n",
    "\n",
    "        self.head = nn.Linear(self.d_encoder, n_cls)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return set()\n",
    "\n",
    "    def forward(self, x, im_size):\n",
    "        H, W = im_size\n",
    "        GS = H // self.patch_size\n",
    "        x = self.head(x)\n",
    "        x = rearrange(x, \"b (h w) c -> b c h w\", h=GS)\n",
    "\n",
    "        return x\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Segmenter(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        n_cls,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_cls = n_cls\n",
    "        self.patch_size = encoder.patch_size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        def append_prefix_no_weight_decay(prefix, module):\n",
    "            return set(map(lambda x: prefix + x, module.no_weight_decay()))\n",
    "\n",
    "        nwd_params = append_prefix_no_weight_decay(\"encoder.\", self.encoder).union(\n",
    "            append_prefix_no_weight_decay(\"decoder.\", self.decoder)\n",
    "        )\n",
    "        return nwd_params\n",
    "\n",
    "    def forward(self, im):\n",
    "        H_ori, W_ori = im.size(2), im.size(3)\n",
    "        #im = padding(im, self.patch_size)\n",
    "        H, W = im.size(2), im.size(3)\n",
    "\n",
    "        #x = self.encoder(im, return_features=True)\n",
    "        x = self.encoder.forward_features(im)\n",
    "        \n",
    "\n",
    "        # remove CLS/DIST tokens for decoding\n",
    "        #num_extra_tokens = 1 + self.encoder.distilled\n",
    "        #x = x[:, num_extra_tokens:]\n",
    "        x = x[:,self.encoder.num_prefix_tokens:,...]\n",
    "\n",
    "        masks = self.decoder(x, (H, W))\n",
    "\n",
    "        masks = F.interpolate(masks, size=(H, W), mode=\"bilinear\")\n",
    "        #masks = unpadding(masks, (H_ori, W_ori))\n",
    "\n",
    "        return masks\n",
    "\n",
    "#    def get_attention_map_enc(self, im, layer_id):\n",
    "#        return self.encoder.get_attention_map(im, layer_id)\n",
    "#\n",
    "#    def get_attention_map_dec(self, im, layer_id):\n",
    "#        x = self.encoder(im, return_features=True)\n",
    "#\n",
    "#        # remove CLS/DIST tokens for decoding\n",
    "#        num_extra_tokens = 1 + self.encoder.distilled\n",
    "#        x = x[:, num_extra_tokens:]\n",
    "#\n",
    "#        return self.decoder.get_attention_map(x, layer_id)\n",
    "    \n",
    "encoder = timm.create_model('vit_base_patch8_224_dino', pretrained=True)\n",
    "encoder.patch_size = 8\n",
    "decoder = DecoderLinear(n_cls=10, d_encoder=encoder.embed_dim, patch_size=8)\n",
    "model = Segmenter(encoder,decoder,n_cls=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce71af2a-8867-4e79-ac51-23d7bb195d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 224, 224])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,3,224,224)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891805d5-bbdc-4d48-91d1-c816059d474c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
