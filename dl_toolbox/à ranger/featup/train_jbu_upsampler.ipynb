{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a1a2def-d5af-4c96-ad9b-ad1c761830fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import hydra\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from os.path import join\n",
    "\n",
    "from featup.datasets.JitteredImage import apply_jitter, sample_transform\n",
    "from featup.datasets.util import get_dataset, SingleImageDataset\n",
    "from featup.downsamplers import SimpleDownsampler, AttentionDownsampler\n",
    "from featup.featurizers.util import get_featurizer\n",
    "from featup.layers import ChannelNorm\n",
    "from featup.losses import TVLoss, SampledCRFLoss, entropy\n",
    "from featup.upsamplers import get_upsampler\n",
    "from featup.util import pca, RollingAvg, unnorm, norm, prep_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a730f9-fc3f-4958-b657-f380597c02a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class JBUFeatUp(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 model_type,\n",
    "                 activation_type,\n",
    "                 n_jitters,\n",
    "                 max_pad,\n",
    "                 max_zoom,\n",
    "                 kernel_size,\n",
    "                 final_size,\n",
    "                 lr,\n",
    "                 random_projection,\n",
    "                 upsampler,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.activation_type = activation_type\n",
    "        self.n_jitters = n_jitters\n",
    "        self.max_pad = max_pad\n",
    "        self.max_zoom = max_zoom\n",
    "        self.kernel_size = kernel_size\n",
    "        self.final_size = final_size\n",
    "        self.lr = lr\n",
    "        self.random_projection = random_projection\n",
    "        self.model, self.patch_size, self.dim = get_featurizer(model_type, activation_type, num_classes=1000)\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.model = torch.nn.Sequential(self.model, ChannelNorm(self.dim))\n",
    "        self.upsampler = get_upsampler(upsampler, self.dim)\n",
    "        self.downsampler = SimpleDownsampler(self.kernel_size, self.final_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.upsampler(self.model(x))\n",
    "\n",
    "    def project(self, feats, proj):\n",
    "        if proj is None:\n",
    "            return feats\n",
    "        else:\n",
    "            return torch.einsum(\"bchw,bcd->bdhw\", feats, proj)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #opt = self.optimizers()\n",
    "        #opt.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if type(batch) == dict:\n",
    "                img = batch['image']\n",
    "            else:\n",
    "                img, _ = batch\n",
    "            lr_feats = self.model(img)\n",
    "\n",
    "        full_rec_loss = 0.0\n",
    "        for i in range(self.n_jitters):\n",
    "            hr_feats = self.upsampler(lr_feats, img)\n",
    "\n",
    "            if hr_feats.shape[2] != img.shape[2]:\n",
    "                hr_feats = torch.nn.functional.interpolate(hr_feats, img.shape[2:], mode=\"bilinear\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                transform_params = sample_transform(\n",
    "                    True, self.max_pad, self.max_zoom, img.shape[2], img.shape[3])\n",
    "                jit_img = apply_jitter(img, self.max_pad, transform_params)\n",
    "                lr_jit_feats = self.model(jit_img)\n",
    "\n",
    "            if self.random_projection is not None:\n",
    "                proj = torch.randn(lr_feats.shape[0],\n",
    "                                   lr_feats.shape[1],\n",
    "                                   self.random_projection, device=lr_feats.device)\n",
    "                proj /= proj.square().sum(1, keepdim=True).sqrt()\n",
    "            else:\n",
    "                proj = None\n",
    "\n",
    "            hr_jit_feats = apply_jitter(hr_feats, self.max_pad, transform_params)\n",
    "            proj_hr_feats = self.project(hr_jit_feats, proj)\n",
    "\n",
    "            down_jit_feats = self.project(self.downsampler(hr_jit_feats, jit_img), proj)\n",
    "\n",
    "            rec_loss = (self.project(lr_jit_feats, proj) - down_jit_feats).square().mean() / self.n_jitters\n",
    "\n",
    "            full_rec_loss += rec_loss\n",
    "\n",
    "            #self.manual_backward(loss)\n",
    "\n",
    "        #if self.global_step < 10:\n",
    "        #    self.clip_gradients(opt, gradient_clip_val=.0001, gradient_clip_algorithm=\"norm\")\n",
    "\n",
    "        #opt.step()\n",
    "\n",
    "        return full_rec_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            if self.trainer.is_global_zero and batch_idx == 0:\n",
    "\n",
    "                if type(batch) == dict:\n",
    "                    img = batch['image']\n",
    "                else:\n",
    "                    img, _ = batch\n",
    "                lr_feats = self.model(img)\n",
    "\n",
    "                hr_feats = self.upsampler(lr_feats, img)\n",
    "\n",
    "                if hr_feats.shape[2] != img.shape[2]:\n",
    "                    hr_feats = torch.nn.functional.interpolate(hr_feats, img.shape[2:], mode=\"bilinear\")\n",
    "\n",
    "                transform_params = sample_transform(\n",
    "                    True, self.max_pad, self.max_zoom, img.shape[2], img.shape[3])\n",
    "                jit_img = apply_jitter(img, self.max_pad, transform_params)\n",
    "                lr_jit_feats = self.model(jit_img)\n",
    "\n",
    "                if self.random_projection is not None:\n",
    "                    proj = torch.randn(lr_feats.shape[0],\n",
    "                                       lr_feats.shape[1],\n",
    "                                       self.random_projection, device=lr_feats.device)\n",
    "                    proj /= proj.square().sum(1, keepdim=True).sqrt()\n",
    "                else:\n",
    "                    proj = None\n",
    "                    \n",
    "                #scales = self.scale_net(lr_jit_feats)\n",
    "\n",
    "                writer = self.logger.experiment\n",
    "\n",
    "                hr_jit_feats = apply_jitter(hr_feats, self.max_pad, transform_params)\n",
    "                down_jit_feats = self.downsampler(hr_jit_feats, jit_img)\n",
    "                down_jit_feats_proj = self.project(down_jit_feats, proj)\n",
    "                \n",
    "                rec_loss = (self.project(lr_jit_feats, proj) - down_jit_feats_proj).square().mean()\n",
    "                print(rec_loss.item())\n",
    "\n",
    "\n",
    "                [red_lr_feats], fit_pca = pca([lr_feats[0].unsqueeze(0)])\n",
    "                [red_hr_feats], _ = pca([hr_feats[0].unsqueeze(0)], fit_pca=fit_pca)\n",
    "                [red_lr_jit_feats], _ = pca([lr_jit_feats[0].unsqueeze(0)], fit_pca=fit_pca)\n",
    "                [red_hr_jit_feats], _ = pca([hr_jit_feats[0].unsqueeze(0)], fit_pca=fit_pca)\n",
    "                [red_down_jit_feats], _ = pca([down_jit_feats[0].unsqueeze(0)], fit_pca=fit_pca)\n",
    "\n",
    "                writer.add_image(\"viz/image\", unnorm(img[0].unsqueeze(0))[0], self.global_step)\n",
    "                writer.add_image(\"viz/lr_feats\", red_lr_feats[0], self.global_step)\n",
    "                writer.add_image(\"viz/hr_feats\", red_hr_feats[0], self.global_step)\n",
    "                writer.add_image(\"jit_viz/jit_image\", unnorm(jit_img[0].unsqueeze(0))[0], self.global_step)\n",
    "                writer.add_image(\"jit_viz/lr_jit_feats\", red_lr_jit_feats[0], self.global_step)\n",
    "                writer.add_image(\"jit_viz/hr_jit_feats\", red_hr_jit_feats[0], self.global_step)\n",
    "                writer.add_image(\"jit_viz/down_jit_feats\", red_down_jit_feats[0], self.global_step)\n",
    "#\n",
    "                #norm_scales = scales[0]\n",
    "                #norm_scales /= scales.max()\n",
    "                #writer.add_image(\"scales\", norm_scales, self.global_step)\n",
    "                #writer.add_histogram(\"scales hist\", scales, self.global_step)\n",
    "#\n",
    "                writer.add_image(\n",
    "                    \"down/filter\",\n",
    "                    prep_image(self.downsampler.get_kernel().squeeze(), subtract_min=False),\n",
    "                    self.global_step)\n",
    "#\n",
    "                writer.flush()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        all_params = []\n",
    "        all_params.extend(list(self.downsampler.parameters()))\n",
    "        all_params.extend(list(self.upsampler.parameters()))\n",
    "        return torch.optim.NAdam(all_params, lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cca3fdc-588f-4a6a-83ce-bbdcfd4e6acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "import dl_toolbox.datasets as datasets\n",
    "from torch.utils.data import Subset, RandomSampler\n",
    "import torch\n",
    "from dl_toolbox.utils import CustomCollate\n",
    "\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.Resize(size=(224, 224), antialias=True),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "NB_IMG = 45*700\n",
    "dataset = datasets.Resisc('/data/NWPU-RESISC45', transform, 'all45')\n",
    "trainset = Subset(dataset, indices=[i for i in range(NB_IMG) if 100<=i%700])\n",
    "valset = Subset(dataset, indices=[i for i in range(NB_IMG) if 100>i%700])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    collate_fn=CustomCollate(),\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    sampler=RandomSampler(\n",
    "        trainset,\n",
    "        replacement=True,\n",
    "        num_samples=100\n",
    "    ),\n",
    "    drop_last=True,\n",
    "    batch_size=1,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    collate_fn=CustomCollate(),\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f413491-e78f-4a64-9c25-6edae954b8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "Missing logger folder: /data/outputs/jbu/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type              | Params\n",
      "--------------------------------------------------\n",
      "0 | model       | Sequential        | 21.7 M\n",
      "1 | upsampler   | JBUStack          | 172 K \n",
      "2 | downsampler | SimpleDownsampler | 256   \n",
      "--------------------------------------------------\n",
      "173 K     Trainable params\n",
      "21.7 M    Non-trainable params\n",
      "21.8 M    Total params\n",
      "87.358    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                                                                       | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5709522366523743\n",
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:27<00:00,  3.63it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0.642144501209259\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:27<00:00,  3.64it/s, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0.5918291211128235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 53, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 30, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 495, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "[rank: 0] Received SIGTERM: 15\n"
     ]
    }
   ],
   "source": [
    "from dl_toolbox.callbacks import ProgressBar\n",
    "\n",
    "log_dir = '/data/outputs/jbu'\n",
    "chkpt_dir = '/data/outputs/jbu/test.ckpt'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_logger = TensorBoardLogger(log_dir, default_hp_metric=False)\n",
    "callbacks = [ModelCheckpoint(chkpt_dir[:-5], every_n_epochs=1)]\n",
    "\n",
    "module = JBUFeatUp(\n",
    "     model_type=\"vit\",\n",
    "     activation_type=\"token\",\n",
    "     n_jitters=2,\n",
    "     max_pad=20,\n",
    "     max_zoom=2,\n",
    "     kernel_size=16,\n",
    "     final_size=14,\n",
    "     lr=1e-3,\n",
    "     random_projection=30,\n",
    "     upsampler=\"jbu_stack\",\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "                 \n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    logger=tb_logger,\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    limit_train_batches=1.,\n",
    "    limit_val_batches=1.,\n",
    "    callbacks=callbacks+[ProgressBar()],\n",
    "    val_check_interval=100,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    module,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fcdafe-d102-4847-b8bb-cb41e8194b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
