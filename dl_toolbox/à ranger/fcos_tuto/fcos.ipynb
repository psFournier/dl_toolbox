{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "930ed3f8-b02e-4558-8174-1e2df1d33f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class PascalVOCDataset:\n",
    "\n",
    "    def __init__(self):\n",
    "        root_dir = \"/data/PASCALVOC/VOCdevkit\"\n",
    "        root_dir = \"/scratchf/PASCALVOC/VOCdevkit\"\n",
    "        self.img_dir = os.path.join(root_dir, \"VOC2012\", \"JPEGImages\")\n",
    "\n",
    "    def load_data(self, dataset_path, labels):\n",
    "        instances = []\n",
    "        data_df = pd.read_pickle(dataset_path)\n",
    "        for _, row in data_df.iterrows():\n",
    "            img_path = row[\"filename\"]\n",
    "            labels_ = row[\"labels\"]\n",
    "\n",
    "            image_path = f\"{self.img_dir}/{img_path}\"\n",
    "\n",
    "            labels_ = [[labels.index(l)] for l in labels_]\n",
    "\n",
    "            targets_ = np.concatenate([row[\"bboxes\"], labels_],\n",
    "                                      axis=-1).tolist()\n",
    "\n",
    "            instances.append({\"image_path\": image_path, \"target\": targets_})\n",
    "        return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d776ac-9666-40d0-bb05-0cd876146c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voc = PascalVOCDataset()\n",
    "labels = [\n",
    "    '__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
    "    'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n",
    "    'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'tvmonitor'\n",
    "]\n",
    "data = voc.load_data(\"voc_combined.csv\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a815843-439b-49a0-a271-48335ef32180",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "def letterbox_image(img, inp_dim):\n",
    "    '''resize image with unchanged aspect ratio using padding\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    img : numpy.ndarray\n",
    "        Image \n",
    "    \n",
    "    inp_dim: tuple(int)\n",
    "        shape of the reszied image\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    numpy.ndarray:\n",
    "        Resized image\n",
    "    \n",
    "    '''\n",
    "\n",
    "    inp_dim = (inp_dim, inp_dim)\n",
    "    img_w, img_h = img.shape[1], img.shape[0]\n",
    "    w, h = inp_dim\n",
    "    new_w = int(img_w * min(w / img_w, h / img_h))\n",
    "    new_h = int(img_h * min(w / img_w, h / img_h))\n",
    "    resized_image = cv2.resize(img, (new_w, new_h))\n",
    "\n",
    "    canvas = np.full((inp_dim[1], inp_dim[0], 3), 0)\n",
    "\n",
    "    canvas[(h - new_h) // 2:(h - new_h) // 2 + new_h,\n",
    "           (w - new_w) // 2:(w - new_w) // 2 + new_w, :] = resized_image\n",
    "\n",
    "    return canvas\n",
    "\n",
    "class Resize(object):\n",
    "    \"\"\"Resize the image in accordance to `image_letter_box` function in darknet \n",
    "    \n",
    "    The aspect ratio is maintained. The longer side is resized to the input \n",
    "    size of the network, while the remaining space on the shorter side is filled \n",
    "    with black color. **This should be the last transform**\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inp_dim):\n",
    "        self.inp_dim = inp_dim\n",
    "\n",
    "    def __call__(self, img, bboxes=None):\n",
    "        w, h = img.shape[1], img.shape[0]\n",
    "        img = letterbox_image(img, self.inp_dim)\n",
    "\n",
    "        scale = min(self.inp_dim / h, self.inp_dim / w)\n",
    "        if bboxes is not None:\n",
    "            bboxes[:, :4] *= (scale)\n",
    "\n",
    "        new_w = scale * w\n",
    "        new_h = scale * h\n",
    "        inp_dim = self.inp_dim\n",
    "\n",
    "        del_h = (inp_dim - new_h) / 2\n",
    "        del_w = (inp_dim - new_w) / 2\n",
    "\n",
    "        add_matrix = np.array([[del_w, del_h, del_w, del_h]]).astype(int)\n",
    "\n",
    "        if bboxes is not None:\n",
    "            bboxes[:, :4] += add_matrix\n",
    "\n",
    "        img = img.astype(np.uint8)\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "class ObjDetDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, image_size, transforms=None):\n",
    "        image_paths = []\n",
    "        targets = []\n",
    "        for instance in data:\n",
    "            image_paths.append(instance['image_path'])\n",
    "            targets.append(instance[\"target\"])\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "        self.image_size = image_size\n",
    "        self.resize = Resize(image_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        targets = self.targets[idx]\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        image = np.array(image, dtype=np.float64)\n",
    "        targets = np.array(targets, dtype=np.float64)\n",
    "\n",
    "        if self.transforms:\n",
    "            image, targets = self.transforms(image.copy(), targets.copy())\n",
    "\n",
    "        image, targets = self.resize(image.copy(), targets.copy())\n",
    "\n",
    "        image = F.to_tensor(image.copy())\n",
    "\n",
    "        return image, torch.tensor(targets.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5cd1df7-c14a-44b3-a5b8-a7d5cda1089d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def _collate_targets(batch):\n",
    "    transposed_batch = list(zip(*batch))\n",
    "    images = transposed_batch[0]\n",
    "    targets = transposed_batch[1]\n",
    "    return torch.stack(images), targets # why not stack targets ?\n",
    "\n",
    "split = int(0.95*len(data))\n",
    "train_data = data[:split]\n",
    "eval_data = data[split:]\n",
    "print(len(eval_data))\n",
    "train_dataset = ObjDetDataset(train_data, 416, transforms=None)\n",
    "eval_dataset = ObjDetDataset(eval_data, 416, transforms=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de74c671-fd27-4f01-87d6-9b91076e66b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 416, 416])\n",
      "torch.Size([5, 5])\n",
      "tensor([[ 87.3600, 250.8480, 231.2960, 341.5360,   9.0000],\n",
      "        [ 23.2960, 277.4720,  89.0240, 359.0080,   9.0000],\n",
      "        [297.8560, 236.7040, 404.3520, 339.8720,   9.0000],\n",
      "        [ 59.0720, 175.9680, 104.8320, 241.6960,  15.0000],\n",
      "        [319.4880, 187.6160, 389.3760, 288.2880,  15.0000]])\n"
     ]
    }
   ],
   "source": [
    "image, targets = train_dataset[785]\n",
    "print(image.shape)\n",
    "print(targets.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2339eb2c-f5dc-4dc1-aa8e-5c89d705d407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=_collate_targets,\n",
    "    num_workers=6\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=_collate_targets,\n",
    "    num_workers=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f60387c5-95ce-47a8-bb5d-1f4908298c76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 416, 416])\n",
      "tensor([[ 64.0640, 134.6720, 376.0640, 360.1440,   2.0000],\n",
      "        [ 39.1040,  86.4160, 198.0160, 292.7520,  15.0000]])\n"
     ]
    }
   ],
   "source": [
    "for step, (images, targets) in enumerate(train_dataloader):\n",
    "    print(images.shape)\n",
    "    print(targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2690cfa8-1bd5-41d2-b9e0-a8074c696e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dl_toolbox.networks import FCOS\n",
    "model = FCOS(num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2bef7a0-4dc1-4dc6-977c-5547ad4af1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "INF = 100000000\n",
    "MAXIMUM_DISTANCES_PER_LEVEL = [-1, 64, 128, 256, 512, INF]\n",
    "\n",
    "\n",
    "def _match_reg_distances_shape(MAXIMUM_DISTANCES_PER_LEVEL, num_locs_per_level):\n",
    "    level_reg_distances = []\n",
    "    for m in range(1, len(MAXIMUM_DISTANCES_PER_LEVEL)):\n",
    "        level_distances = torch.tensor([\n",
    "            MAXIMUM_DISTANCES_PER_LEVEL[m - 1], MAXIMUM_DISTANCES_PER_LEVEL[m]\n",
    "        ],\n",
    "                                       dtype=torch.float32)\n",
    "        locs_per_level = num_locs_per_level[m - 1]\n",
    "        level_distances = level_distances.repeat(locs_per_level).view(\n",
    "            locs_per_level, 2)\n",
    "        level_reg_distances.append(level_distances)\n",
    "    # return tensor of size sum of locs_per_level x 2\n",
    "    return torch.cat(level_reg_distances, dim=0)\n",
    "\n",
    "\n",
    "def _calc_bbox_area(bbox):\n",
    "    return (bbox[:, 2] - bbox[:, 0] + 1.0) * (bbox[:, 3] - bbox[:, 1] + 1.0)\n",
    "\n",
    "\n",
    "def _compute_centerness_targets(reg_targets):\n",
    "    if len(reg_targets) == 0:\n",
    "        return reg_targets.new_zeros(len(reg_targets))\n",
    "    left_right = reg_targets[:, [0, 2]]\n",
    "    top_bottom = reg_targets[:, [1, 3]]\n",
    "    centerness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\\n",
    "                (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0])\n",
    "    return torch.sqrt(centerness)\n",
    "\n",
    "\n",
    "def _calculate_reg_targets(xs, ys, bbox_targets):\n",
    "    l = xs[:, None] - bbox_targets[:, 0][None] # Lx1 - 1xT -> LxT\n",
    "    t = ys[:, None] - bbox_targets[:, 1][None]\n",
    "    r = bbox_targets[:, 2][None] - xs[:, None]\n",
    "    b = bbox_targets[:, 3][None] - ys[:, None]\n",
    "    return torch.stack([l, t, r, b], dim=2) # LxTx4\n",
    "\n",
    "\n",
    "def _apply_distance_constraints(reg_targets, level_distances):\n",
    "    max_reg_targets, _ = reg_targets.max(dim=2)\n",
    "    return torch.logical_and(max_reg_targets >= level_distances[:, None, 0], \\\n",
    "                             max_reg_targets <= level_distances[:, None, 1])\n",
    "\n",
    "\n",
    "def _prepare_labels(locations, targets_batch):\n",
    "    device = targets_batch[0].device\n",
    "    # nb of locs for bbox in original image size\n",
    "    num_locs_per_level = [len(l) for l in locations]\n",
    "    # L = sum locs per level x 2 : for each loc in all_locs, the max size of bb authorized\n",
    "    level_distances = _match_reg_distances_shape(MAXIMUM_DISTANCES_PER_LEVEL,\n",
    "                                                 num_locs_per_level).to(device)\n",
    "    all_locations = torch.cat(locations, dim=0).to(device) # Lx2\n",
    "    xs, ys = all_locations[:, 0], all_locations[:, 1] # L & L\n",
    "\n",
    "    all_reg_targets = []\n",
    "    all_cls_targets = []\n",
    "    for targets in targets_batch:\n",
    "        bbox_targets = targets[:, :4] # Tx4\n",
    "        cls_targets = targets[:, 4] # T\n",
    "        \n",
    "        # for each loc in L and each target in T, the reg target\n",
    "        reg_targets = _calculate_reg_targets(xs, ys, bbox_targets) # LxTx4\n",
    "\n",
    "        is_in_boxes = reg_targets.min(dim=2)[0] > 0 # min returns values and indices -> LxT\n",
    "\n",
    "        fits_to_feature_level = _apply_distance_constraints(\n",
    "            reg_targets, level_distances).to(device) # LxT\n",
    "\n",
    "        bbox_areas = _calc_bbox_area(bbox_targets) # T\n",
    "        \n",
    "        # area of each target bbox repeated for each loc with inf where the the loc is not \n",
    "        # in the target bbox or if the loc is not at the right level for this bbox size\n",
    "        locations_to_gt_area = bbox_areas[None].repeat(len(all_locations), 1) # LxT\n",
    "        locations_to_gt_area[is_in_boxes == 0] = INF\n",
    "        locations_to_gt_area[fits_to_feature_level == 0] = INF\n",
    "        \n",
    "        # for each loc, area and target idx of the target of min area at that loc\n",
    "        loc_min_area, loc_mind_idxs = locations_to_gt_area.min(dim=1) # val&idx, size L, idx in [0,T-1]\n",
    "\n",
    "        reg_targets = reg_targets[range(len(all_locations)), loc_mind_idxs] # Lx4\n",
    "\n",
    "        cls_targets = cls_targets[loc_mind_idxs] # L\n",
    "        cls_targets[loc_min_area == INF] = 0\n",
    "        \n",
    "        all_cls_targets.append(\n",
    "            torch.split(cls_targets, num_locs_per_level, dim=0))\n",
    "        all_reg_targets.append(\n",
    "            torch.split(reg_targets, num_locs_per_level, dim=0))\n",
    "    # all_cls_targets contains B lists of num levels elem of loc_per_levelsx1\n",
    "    return _match_pred_format(all_cls_targets, all_reg_targets, locations)\n",
    "\n",
    "\n",
    "def _match_pred_format(cls_targets, reg_targets, locations):\n",
    "    cls_per_level = []\n",
    "    reg_per_level = []\n",
    "    for level in range(len(locations)):\n",
    "        cls_per_level.append(torch.cat([ct[level] for ct in cls_targets],\n",
    "                                       dim=0))\n",
    "\n",
    "        reg_per_level.append(torch.cat([rt[level] for rt in reg_targets],\n",
    "                                       dim=0))\n",
    "    # reg_per_level is a list of num_levels tensors of size Bxnum_loc_per_levelx4\n",
    "    return cls_per_level, reg_per_level\n",
    "\n",
    "\n",
    "def _get_positive_samples(cls_labels, reg_labels, box_cls_preds, box_reg_preds,\n",
    "                          centerness_preds, num_classes):\n",
    "    box_cls_flatten = []\n",
    "    box_regression_flatten = []\n",
    "    centerness_flatten = []\n",
    "    labels_flatten = []\n",
    "    reg_targets_flatten = []\n",
    "    for l in range(len(cls_labels)):\n",
    "        box_cls_flatten.append(box_cls_preds[l].permute(0, 2, 3, 1).reshape(\n",
    "            -1, num_classes))\n",
    "        box_regression_flatten.append(box_reg_preds[l].permute(0, 2, 3,\n",
    "                                                               1).reshape(\n",
    "                                                                   -1, 4))\n",
    "        labels_flatten.append(cls_labels[l].reshape(-1))\n",
    "        reg_targets_flatten.append(reg_labels[l].reshape(-1, 4))\n",
    "        centerness_flatten.append(centerness_preds[l].reshape(-1))\n",
    "\n",
    "    cls_preds = torch.cat(box_cls_flatten, dim=0)\n",
    "    cls_targets = torch.cat(labels_flatten, dim=0)\n",
    "    reg_preds = torch.cat(box_regression_flatten, dim=0)\n",
    "    reg_targets = torch.cat(reg_targets_flatten, dim=0)\n",
    "    centerness_preds = torch.cat(centerness_flatten, dim=0)\n",
    "\n",
    "    pos_inds = torch.nonzero(cls_targets > 0).squeeze(1)\n",
    "\n",
    "    reg_preds = reg_preds[pos_inds]\n",
    "    reg_targets = reg_targets[pos_inds]\n",
    "    centerness_preds = centerness_preds[pos_inds]\n",
    "\n",
    "    return reg_preds, reg_targets, cls_preds, cls_targets, centerness_preds, pos_inds\n",
    "\n",
    "def sigmoid_focal_loss(logits, targets, alpha=0.25, gamma=2):\n",
    "    num_classes = logits.shape[1]\n",
    "    dtype = targets.dtype\n",
    "    device = targets.device\n",
    "    class_range = torch.arange(1, num_classes + 1, dtype=dtype,\n",
    "                               device=device).unsqueeze(0)\n",
    "\n",
    "    t = targets.unsqueeze(1)\n",
    "    p = torch.sigmoid(logits)\n",
    "    term1 = (1 - p)**gamma * torch.log(p)\n",
    "    term2 = p**gamma * torch.log(1 - p)\n",
    "    return -(t == class_range).float() * term1 * alpha - (\n",
    "        (t != class_range) * (t >= 0)).float() * term2 * (1 - alpha)\n",
    "\n",
    "\n",
    "class IOULoss(nn.Module):\n",
    "\n",
    "    def __init__(self, loss_type=\"iou\"):\n",
    "        super(IOULoss, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def forward(self, pred, target, weight=None):\n",
    "        pred_left = pred[:, 0]\n",
    "        pred_top = pred[:, 1]\n",
    "        pred_right = pred[:, 2]\n",
    "        pred_bottom = pred[:, 3]\n",
    "\n",
    "        target_left = target[:, 0]\n",
    "        target_top = target[:, 1]\n",
    "        target_right = target[:, 2]\n",
    "        target_bottom = target[:, 3]\n",
    "\n",
    "        target_area = (target_left + target_right) * \\\n",
    "                      (target_top + target_bottom)\n",
    "        pred_area = (pred_left + pred_right) * \\\n",
    "                    (pred_top + pred_bottom)\n",
    "\n",
    "        w_intersect = torch.min(pred_left, target_left) + torch.min(\n",
    "            pred_right, target_right)\n",
    "        g_w_intersect = torch.max(pred_left, target_left) + torch.max(\n",
    "            pred_right, target_right)\n",
    "        h_intersect = torch.min(pred_bottom, target_bottom) + torch.min(\n",
    "            pred_top, target_top)\n",
    "        g_h_intersect = torch.max(pred_bottom, target_bottom) + torch.max(\n",
    "            pred_top, target_top)\n",
    "        ac_uion = g_w_intersect * g_h_intersect + 1e-7\n",
    "        area_intersect = w_intersect * h_intersect\n",
    "        area_union = target_area + pred_area - area_intersect\n",
    "        ious = (area_intersect + 1.0) / (area_union + 1.0)\n",
    "        gious = ious - (ac_uion - area_union) / ac_uion\n",
    "        if self.loss_type == 'iou':\n",
    "            losses = -torch.log(ious)\n",
    "        elif self.loss_type == 'linear_iou':\n",
    "            losses = 1 - ious\n",
    "        elif self.loss_type == 'giou':\n",
    "            losses = 1 - gious\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if weight is not None and weight.sum() > 0:\n",
    "            return (losses * weight).sum()\n",
    "        else:\n",
    "            assert losses.numel() != 0\n",
    "            return losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e24601f-642b-4245-a582-42a6dc91a23e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LossEvaluator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reg_loss_func = IOULoss(\"giou\")\n",
    "        self.centerness_loss_func = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "        self.cls_loss_func = sigmoid_focal_loss\n",
    "\n",
    "    def _get_cls_loss(self, cls_preds, cls_targets, total_num_pos):\n",
    "        cls_loss = self.cls_loss_func(cls_preds, cls_targets.int())\n",
    "        return cls_loss.sum() / total_num_pos\n",
    "\n",
    "    def _get_reg_loss(self, reg_preds, reg_targets, centerness_targets):\n",
    "        sum_centerness_targets = centerness_targets.sum()\n",
    "        reg_preds = reg_preds.reshape(-1, 4)\n",
    "        reg_targets = reg_targets.reshape(-1, 4)\n",
    "        reg_loss = self.reg_loss_func(reg_preds, reg_targets,\n",
    "                                      centerness_targets)\n",
    "        return reg_loss / sum_centerness_targets # Why dividing by that here ?\n",
    "\n",
    "    def _get_centerness_loss(self, centerness_preds, centerness_targets,\n",
    "                             total_num_pos):\n",
    "        centerness_loss = self.centerness_loss_func(centerness_preds,\n",
    "                                                    centerness_targets)\n",
    "        return centerness_loss / total_num_pos\n",
    "\n",
    "    def _evaluate_losses(self, reg_preds, cls_preds, centerness_preds,\n",
    "                         reg_targets, cls_targets, centerness_targets,\n",
    "                         pos_inds):\n",
    "        total_num_pos = max(pos_inds.new_tensor([pos_inds.numel()]), 1.0)\n",
    "\n",
    "        cls_loss = self._get_cls_loss(cls_preds, cls_targets, total_num_pos)\n",
    "\n",
    "        if pos_inds.numel() > 0:\n",
    "            reg_loss = self._get_reg_loss(reg_preds, reg_targets,\n",
    "                                          centerness_targets)\n",
    "            centerness_loss = self._get_centerness_loss(centerness_preds,\n",
    "                                                        centerness_targets,\n",
    "                                                        total_num_pos)\n",
    "        else:\n",
    "            reg_loss = reg_preds.sum() # 0 ??\n",
    "            centerness_loss = centerness_preds.sum() # 0 ??\n",
    "\n",
    "        return reg_loss, cls_loss, centerness_loss\n",
    "\n",
    "    def __call__(self, locations, preds, targets_batch, num_classes):\n",
    "        # reg_targets is a list of num_levels tensors of size Bxnum_loc_per_levelx4\n",
    "        cls_targets, reg_targets = _prepare_labels(locations, targets_batch)\n",
    "\n",
    "        cls_preds, reg_preds, centerness_preds = preds\n",
    "\n",
    "        reg_preds, reg_targets, cls_preds, cls_targets, centerness_preds, pos_inds = _get_positive_samples(\n",
    "            cls_targets, reg_targets, cls_preds, reg_preds, centerness_preds,\n",
    "            num_classes)\n",
    "\n",
    "        centerness_targets = _compute_centerness_targets(reg_targets)\n",
    "\n",
    "        reg_loss, cls_loss, centerness_loss = self._evaluate_losses(\n",
    "            reg_preds, cls_preds, centerness_preds, reg_targets, cls_targets,\n",
    "            centerness_targets, pos_inds)\n",
    "\n",
    "        return cls_loss, reg_loss, centerness_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c493e0c-7329-4a7f-892f-47c0fd8e588d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nms(bounding_boxes,\n",
    "        confidence_scores,\n",
    "        classes,\n",
    "        threshold,\n",
    "        class_agnostic=True):\n",
    "    device = bounding_boxes.device\n",
    "    if len(bounding_boxes) == 0:\n",
    "        return torch.tensor([]).to(device), torch.tensor(\n",
    "            []).to(device), torch.tensor([]).to(device)\n",
    "\n",
    "    bounding_boxes = bounding_boxes.detach().cpu().numpy()\n",
    "    confidence_scores = confidence_scores.detach().cpu().numpy()\n",
    "    classes = classes.detach().cpu().numpy()\n",
    "\n",
    "    start_x = bounding_boxes[:, 0]\n",
    "    start_y = bounding_boxes[:, 1]\n",
    "    end_x = bounding_boxes[:, 2]\n",
    "    end_y = bounding_boxes[:, 3]\n",
    "\n",
    "    picked_boxes = []\n",
    "    picked_scores = []\n",
    "    picked_classes = []\n",
    "\n",
    "    areas = (end_x - start_x + 1) * (end_y - start_y + 1)\n",
    "\n",
    "    order = np.argsort(confidence_scores)\n",
    "\n",
    "    while order.size > 0:\n",
    "        index = order[-1]\n",
    "\n",
    "        picked_boxes.append(bounding_boxes[index])\n",
    "        picked_scores.append(confidence_scores[index])\n",
    "        picked_classes.append(classes[index])\n",
    "\n",
    "        order = order[:-1]\n",
    "        if len(order) == 0:\n",
    "            break\n",
    "\n",
    "        x1 = np.maximum(start_x[index], start_x[order])\n",
    "        x2 = np.minimum(end_x[index], end_x[order])\n",
    "        y1 = np.maximum(start_y[index], start_y[order])\n",
    "        y2 = np.minimum(end_y[index], end_y[order])\n",
    "\n",
    "        w = np.maximum(0.0, x2 - x1 + 1)\n",
    "        h = np.maximum(0.0, y2 - y1 + 1)\n",
    "        intersection = w * h\n",
    "        ratio = intersection / (areas[index] + areas[order] - intersection)\n",
    "\n",
    "        if not class_agnostic:\n",
    "            other_classes = classes[order] != classes[index]\n",
    "            ratio[other_classes] = 0.0\n",
    "\n",
    "        left = np.where(ratio < threshold)\n",
    "        order = order[left]\n",
    "\n",
    "    outputs = [\n",
    "        torch.tensor(np.array(picked_boxes)).to(device),\n",
    "        torch.tensor(np.array(picked_scores)).to(device),\n",
    "        torch.tensor(np.array(picked_classes)).to(device)\n",
    "    ]\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def _convert_bbox_xywh(bbox):\n",
    "    xmin, ymin, xmax, ymax = _split_into_xyxy(bbox)\n",
    "    bbox = torch.cat((xmin, ymin, xmax - xmin + 1, ymax - ymin + 1), dim=-1)\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def _split_into_xyxy(bbox):\n",
    "    xmin, ymin, w, h = bbox.split(1, dim=-1)\n",
    "    return (\n",
    "        xmin,\n",
    "        ymin,\n",
    "        xmin + (w - 1).clamp(min=0),\n",
    "        ymin + (h - 1).clamp(min=0),\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_small_boxes(boxlist, min_size):\n",
    "    xywh_boxes = _convert_bbox_xywh(boxlist)\n",
    "    _, _, ws, hs = xywh_boxes.unbind(dim=1)\n",
    "    keep = ((ws >= min_size) & (hs >= min_size)).nonzero().squeeze(1)\n",
    "    return boxlist[keep]\n",
    "\n",
    "\n",
    "def _clip_to_image(bboxes, image_size):\n",
    "    h, w = image_size\n",
    "    bboxes[:, 0].clamp_(min=0, max=h - 1)\n",
    "    bboxes[:, 1].clamp_(min=0, max=w - 1)\n",
    "    bboxes[:, 2].clamp_(min=0, max=h - 1)\n",
    "    bboxes[:, 3].clamp_(min=0, max=w - 1)\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3aedb98-8a15-4894-8d00-402f910267cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FCOSPostProcessor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, pre_nms_thresh, pre_nms_top_n, nms_thresh,\n",
    "                 fpn_post_nms_top_n, min_size, num_classes):\n",
    "        super(FCOSPostProcessor, self).__init__()\n",
    "        self.pre_nms_thresh = pre_nms_thresh\n",
    "        self.pre_nms_top_n = pre_nms_top_n\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.fpn_post_nms_top_n = fpn_post_nms_top_n\n",
    "        self.min_size = min_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward_for_single_feature_map(self, locations, cls_preds, reg_preds,\n",
    "                                       cness_preds, image_size):\n",
    "        B, C, _, _ = cls_preds.shape\n",
    "\n",
    "        cls_preds = cls_preds.permute(0, 2, 3, 1).reshape(B, -1, C).sigmoid() # BxHWxC in [0,1]\n",
    "        reg_preds = reg_preds.permute(0, 2, 3, 1).reshape(B, -1, 4)\n",
    "        cness_preds = cness_preds.permute(0, 2, 3, 1).reshape(B, -1).sigmoid()\n",
    "\n",
    "        candidate_inds = cls_preds > self.pre_nms_thresh # BxHWxC\n",
    "        pre_nms_top_n = candidate_inds.reshape(B, -1).sum(1) # B\n",
    "        pre_nms_top_n = pre_nms_top_n.clamp(max=self.pre_nms_top_n)\n",
    "\n",
    "        cls_preds = cls_preds * cness_preds[:, :, None] # BxHWxC\n",
    "        \n",
    "        # Conversion en liste de bbox,scores,cls par image du batch\n",
    "        # POURQUOI le filtre cls_preds > nms_thresh arrive pas après la mul par cness_preds ?\n",
    "        bboxes = []\n",
    "        cls_labels = []\n",
    "        scores = []\n",
    "        for i in range(B):\n",
    "            per_cls_preds = cls_preds[i] # HWxC\n",
    "            per_candidate_inds = candidate_inds[i] # HWxC\n",
    "            # tenseur de taille L avec les elem de cls_preds*centerness tels que cls_preds > nms_thresh\n",
    "            per_cls_preds = per_cls_preds[per_candidate_inds] \n",
    "            \n",
    "            # tenseur de taille Lx2 avec les indices des elem de cls_preds où > nms_thresh\n",
    "            per_candidate_nonzeros = per_candidate_inds.nonzero() \n",
    "            # L : positions dans [0,HW] des elem dont cls_preds(c) > nms_thresh \n",
    "            per_box_loc = per_candidate_nonzeros[:, 0]\n",
    "            # L : classe dans [1, C] des elem dont cls_preds(h,w) > nms_thresh\n",
    "            per_class = per_candidate_nonzeros[:, 1] + 1\n",
    "\n",
    "            per_reg_preds = reg_preds[i] # HWx4\n",
    "            # liste des bb des elem dont cls_preds(c) > nms_thresh \n",
    "            per_reg_preds = per_reg_preds[per_box_loc] # Lx4\n",
    "            per_locations = locations[per_box_loc] # Lx2\n",
    "\n",
    "            per_pre_nms_top_n = pre_nms_top_n[i]\n",
    "            \n",
    "            # si y a plus de per_prenms_topn qui passe nms_thresh (si L est trop longue)\n",
    "            if per_candidate_inds.sum().item() > per_pre_nms_top_n.item():\n",
    "                per_cls_preds, top_k_indices = per_cls_preds.topk(\n",
    "                    per_pre_nms_top_n, sorted=False)\n",
    "                per_class = per_class[top_k_indices]\n",
    "                per_reg_preds = per_reg_preds[top_k_indices]\n",
    "                per_locations = per_locations[top_k_indices]\n",
    "\n",
    "            detections = torch.stack([\n",
    "                per_locations[:, 0] - per_reg_preds[:, 0],\n",
    "                per_locations[:, 1] - per_reg_preds[:, 1],\n",
    "                per_locations[:, 0] + per_reg_preds[:, 2],\n",
    "                per_locations[:, 1] + per_reg_preds[:, 3],\n",
    "            ],\n",
    "                                     dim=1)\n",
    "\n",
    "            detections = _clip_to_image(detections, (image_size, image_size))\n",
    "            detections = remove_small_boxes(detections, self.min_size)\n",
    "            bboxes.append(detections)\n",
    "            cls_labels.append(per_class)\n",
    "            scores.append(torch.sqrt(per_cls_preds))\n",
    "\n",
    "        return bboxes, scores, cls_labels\n",
    "\n",
    "    def forward(self, locations, cls_preds, reg_preds, cness_preds, image_size):\n",
    "        sampled_boxes = []\n",
    "        all_scores = []\n",
    "        all_classes = []\n",
    "        for l, o, b, c in list(zip(locations, cls_preds, reg_preds,\n",
    "                                   cness_preds)):\n",
    "            boxes, scores, cls_labels = self.forward_for_single_feature_map(\n",
    "                l, o, b, c, image_size)\n",
    "\n",
    "            sampled_boxes.append(boxes)\n",
    "            all_scores.append(scores)\n",
    "            all_classes.append(cls_labels)\n",
    "\n",
    "        all_bboxes = list(zip(*sampled_boxes))\n",
    "        all_scores = list(zip(*all_scores))\n",
    "        all_classes = list(zip(*all_classes))\n",
    "\n",
    "        all_bboxes = [torch.cat(bboxes, dim=0) for bboxes in all_bboxes]\n",
    "        all_scores = [torch.cat(scores, dim=0) for scores in all_scores]\n",
    "        all_classes = [torch.cat(classes, dim=0) for classes in all_classes]\n",
    "\n",
    "        boxes, scores, classes = self.select_over_all_levels(\n",
    "            all_bboxes, all_scores, all_classes)\n",
    "\n",
    "        return boxes, scores, classes\n",
    "\n",
    "    def select_over_all_levels(self, boxlists, scores, classes):\n",
    "        num_images = len(boxlists)\n",
    "        all_picked_boxes, all_confidence_scores, all_classes = [], [], []\n",
    "        for i in range(num_images):\n",
    "            picked_boxes, confidence_scores, picked_classes = nms(\n",
    "                boxlists[i], scores[i], classes[i], self.nms_thresh)\n",
    "\n",
    "            number_of_detections = len(picked_boxes)\n",
    "            if number_of_detections > self.fpn_post_nms_top_n > 0:\n",
    "                image_thresh, _ = torch.kthvalue(\n",
    "                    confidence_scores.cpu(),\n",
    "                    number_of_detections - self.fpn_post_nms_top_n + 1)\n",
    "                keep = confidence_scores >= image_thresh.item()\n",
    "\n",
    "                keep = torch.nonzero(keep).squeeze(1)\n",
    "                picked_boxes, confidence_scores, picked_classes = picked_boxes[\n",
    "                    keep], confidence_scores[keep], picked_classes[keep]\n",
    "\n",
    "            keep = confidence_scores >= self.pre_nms_thresh\n",
    "            picked_boxes, confidence_scores, picked_classes = picked_boxes[\n",
    "                keep], confidence_scores[keep], picked_classes[keep]\n",
    "\n",
    "            all_picked_boxes.append(picked_boxes)\n",
    "            all_confidence_scores.append(confidence_scores)\n",
    "            all_classes.append(picked_classes)\n",
    "        return all_picked_boxes, all_confidence_scores, all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28a21266-04c3-466f-8243-2cc6473e7d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _locations_per_level(h, w, s):\n",
    "    locs_x = [i for i in range(w)]\n",
    "    locs_y = [i for i in range(h)]\n",
    "\n",
    "    locs_x = [s / 2 + x * s for x in locs_x]\n",
    "    locs_y = [s / 2 + y * s for y in locs_y]\n",
    "    locs = [(y, x) for x in locs_x for y in locs_y]\n",
    "    return torch.tensor(locs)\n",
    "\n",
    "\n",
    "def _compute_locations(features, fpn_strides):\n",
    "    locations = []\n",
    "    for level, feature in enumerate(features):\n",
    "        h, w = feature.size()[-2:]\n",
    "        locs = _locations_per_level(h, w, fpn_strides[level]).to(feature.device)\n",
    "        locations.append(locs)\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cd41acc-4e80-43d8-9c45-34a3dd319154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "\n",
    "class FCOS(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        num_classes,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.le = LossEvaluator()\n",
    "        self.post_processor = FCOSPostProcessor(\n",
    "            pre_nms_thresh=0.3,\n",
    "            pre_nms_top_n=1000,\n",
    "            nms_thresh=0.45,\n",
    "            fpn_post_nms_top_n=50,\n",
    "            min_size=0,\n",
    "            num_classes=num_classes)\n",
    "        self.fpn_strides = [8, 16, 32, 64, 128]\n",
    "        self.num_classes = num_classes\n",
    "        self.network = network\n",
    "        self.map_metric = MeanAveragePrecision()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=1e-3,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decay=5e-2,\n",
    "            eps=1e-8,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=1e-3,\n",
    "            steps_per_epoch=len(train_dataloader),\n",
    "            epochs=20\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\"\n",
    "            },\n",
    "        }\n",
    "    \n",
    "    def forward(self, images, targets_batch=None):\n",
    "        features, box_cls, box_regression, centerness = self.network(images)\n",
    "        # locations is a list of num_feat_level elem, where each elem indicates the tensor of locations in the original image corresponding to each location in the feature map at this level\n",
    "        locations = _compute_locations(features, self.fpn_strides)\n",
    "        outputs = {}\n",
    "        if targets_batch != None:\n",
    "            cls_loss, reg_loss, centerness_loss = self.le(\n",
    "                locations, (box_cls, box_regression, centerness),\n",
    "                targets_batch,\n",
    "                num_classes=self.num_classes)\n",
    "            outputs[\"cls_loss\"] = cls_loss\n",
    "            outputs[\"reg_loss\"] = reg_loss\n",
    "            outputs[\"centerness_loss\"] = centerness_loss\n",
    "            outputs[\"combined_loss\"] = cls_loss + reg_loss + centerness_loss\n",
    "\n",
    "        image_size = images.shape[-1]\n",
    "        predicted_boxes, scores, all_classes = self.post_processor(\n",
    "            locations, box_cls, box_regression, centerness, image_size)\n",
    "\n",
    "        outputs[\"predicted_boxes\"] = predicted_boxes\n",
    "        outputs[\"scores\"] = scores\n",
    "        outputs[\"pred_classes\"] = all_classes\n",
    "        return outputs\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch #attention est-ce que y qui est une liste passe sur le bon device ?\n",
    "        results = self.forward(x, y)\n",
    "        loss = results[\"combined_loss\"]\n",
    "        self.log(f\"loss/train\", loss.detach().item())\n",
    "        self.log(f\"cls_loss/train\", results[\"cls_loss\"].detach().item())\n",
    "        self.log(f\"reg_loss/train\", results[\"reg_loss\"].detach().item())\n",
    "        self.log(f\"centerness_loss/train\", results[\"centerness_loss\"].detach().item())\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch #attention est-ce que y qui est une liste passe sur le bon device ?\n",
    "        results = self.forward(x, y)\n",
    "        loss = results[\"combined_loss\"]\n",
    "        preds = [{'boxes': bb, 'scores': s, 'labels': l} for bb,s,l in zip(\n",
    "            results[\"predicted_boxes\"], results[\"scores\"], results[\"pred_classes\"]\n",
    "        )]\n",
    "        target_bb = [t[:, :4] for t in y]\n",
    "        target_l = [t[:, 4] for t in y]\n",
    "        targets = [{'boxes': bb, 'labels': l} for bb,l in zip(target_bb, target_l)]\n",
    "        self.map_metric.update(preds, targets)\n",
    "        self.log(f\"loss/val\", loss.detach().item())\n",
    "        self.log(f\"cls_loss/val\", results[\"cls_loss\"].detach().item())\n",
    "        self.log(f\"reg_loss/val\", results[\"reg_loss\"].detach().item())\n",
    "        self.log(f\"centerness_loss/val\", results[\"centerness_loss\"].detach().item())\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        mapmetric = self.map_metric.compute()['map']\n",
    "        self.log(\"map/val\", mapmetric)\n",
    "        print(\"\\nMAP: \", mapmetric)\n",
    "        self.map_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d01e6b-5b98-4aae-a18b-a87aa7c0f8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
      "\n",
      "  | Name           | Type                 | Params\n",
      "--------------------------------------------------------\n",
      "0 | post_processor | FCOSPostProcessor    | 0     \n",
      "1 | network        | FCOS                 | 25.7 M\n",
      "2 | map_metric     | MeanAveragePrecision | 0     \n",
      "--------------------------------------------------------\n",
      "25.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.7 M    Total params\n",
      "102.942   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 12.50it/s]\n",
      "MAP:  tensor(0.)\n",
      "Epoch 0:  21%|█████████████████████▋                                                                                | 563/2647 [01:19<04:53,  7.09it/s, v_num=8]"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from dl_toolbox.callbacks import ProgressBar\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    limit_train_batches=1.,\n",
    "    limit_val_batches=1.,\n",
    "    callbacks=[ProgressBar()]\n",
    "    #enable_progress_bar=False\n",
    ")\n",
    "\n",
    "module = FCOS(\n",
    "    model,\n",
    "    num_classes=20\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    module,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=eval_dataloader\n",
    "    #dataloaders=eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de7bae-e32b-4162-b6b4-12141ed0bb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e56a4a-2951-439c-ba02-ed6265143c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
