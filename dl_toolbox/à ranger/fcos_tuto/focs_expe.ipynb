{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a815843-439b-49a0-a271-48335ef32180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "class ObjDetDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, transforms=None):\n",
    "        image_paths = []\n",
    "        targets = []\n",
    "        for instance in data:\n",
    "            image_paths.append(instance['image_path'])\n",
    "            targets.append(instance[\"target\"])\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        w, h = image.size\n",
    "        image = v2.functional.pil_to_tensor(image)\n",
    "        targets = self.targets[idx]\n",
    "        targets = torch.Tensor(targets)\n",
    "        bboxes = tv_tensors.BoundingBoxes(targets[:,:4], format=\"XYXY\", canvas_size=(h,w))\n",
    "        labels = targets[:, 4:]\n",
    "        if self.transforms:\n",
    "            image, bboxes = self.transforms(image, bboxes)\n",
    "        return image, bboxes, labels, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2339eb2c-f5dc-4dc1-aa8e-5c89d705d407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from pytorch_lightning.utilities import CombinedLoader\n",
    "from functools import partial\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "\n",
    "class PascalVOC(LightningDataModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        train_tf,\n",
    "        test_tf,\n",
    "        batch_size_s,\n",
    "        steps_per_epoch,\n",
    "        num_workers,\n",
    "        pin_memory,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_path = Path(data_path)\n",
    "        self.train_tf = train_tf\n",
    "        self.test_tf = test_tf\n",
    "        self.batch_size_s = batch_size_s\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        img_dir = self.data_path/\"PASCALVOC/VOCdevkit/VOC2012/JPEGImages\"\n",
    "        self.instances = []\n",
    "        labels = ['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'tvmonitor']\n",
    "        for _, row in pd.read_pickle(\"voc_combined.csv\").iterrows():\n",
    "            img_path = row[\"filename\"]\n",
    "            labels_ = row[\"labels\"]\n",
    "            image_path = f\"{img_dir}/{img_path}\"\n",
    "            labels_ = [[labels.index(l)] for l in labels_]\n",
    "            targets_ = np.concatenate([row[\"bboxes\"], labels_],\n",
    "                                      axis=-1).tolist()\n",
    "            self.instances.append({\"image_path\": image_path, \"target\": targets_})\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        split = int(0.9*len(self.instances))\n",
    "        train_data = self.instances[:split]\n",
    "        val_data = self.instances[split:]\n",
    "        self.train_s_set = ObjDetDataset(train_data, transforms=self.train_tf)\n",
    "        self.val_set = ObjDetDataset(val_data, transforms=self.test_tf)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _collate(batch):\n",
    "        images_b, bboxes_b, labels_b, image_paths_b = list(zip(*batch))\n",
    "        # don't stack bb because each batch elem may not have the same nb of bb\n",
    "        return torch.stack(images_b), bboxes_b, labels_b, image_paths_b \n",
    "                \n",
    "    def _dataloader(self, dataset):\n",
    "        return partial(\n",
    "            DataLoader,\n",
    "            dataset=dataset,\n",
    "            collate_fn=self._collate,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory\n",
    "        )\n",
    "                       \n",
    "    def train_dataloader(self):\n",
    "        return self._dataloader(self.train_s_set)(\n",
    "            shuffle=False,\n",
    "            #sampler=RandomSampler(\n",
    "            #    self.train_s_set,\n",
    "            #    replacement=True,\n",
    "            #    num_samples=self.steps_per_epoch*self.batch_size_s\n",
    "            #),\n",
    "            drop_last=True,\n",
    "            batch_size=self.batch_size_s\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self._dataloader(self.val_set)(\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            batch_size=self.batch_size_s\n",
    "        )\n",
    "    \n",
    "train_tf = v2.Compose([\n",
    "    v2.Resize(size=(480, 480), antialias=True),\n",
    "    #v2.CenterCrop(size=(224,224)),\n",
    "    #v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_tf = v2.Compose([\n",
    "    #v2.Resize(size=(232, 232), antialias=True),\n",
    "    v2.CenterCrop(size=(500,500)),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    #v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "dm = PascalVOC(\n",
    "    data_path='/data',\n",
    "    train_tf=train_tf,\n",
    "    test_tf=test_tf, # taken from https://pytorch.org/vision/0.17/models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights\n",
    "    batch_size_s=4,\n",
    "    steps_per_epoch=1000,\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba51df-e9c9-461c-b055-977f16969f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from dl_toolbox.networks import FCOS\n",
    "#model = FCOS(num_classes=20)\n",
    "\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelP6P7\n",
    "from dl_toolbox.networks.fcos import Head, FCOS\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#class FCOS(torch.nn.Module):\n",
    "#    \n",
    "#    def __init__(self, num_classes=19, out_channels=256):\n",
    "#        super(FCOS, self).__init__()\n",
    "#        #backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#        backbone = convnext_tiny(weights=None)\n",
    "#        return_nodes = {\n",
    "#            'features.3': 'layer2',\n",
    "#            'features.5': 'layer3',\n",
    "#            'features.7': 'layer4',\n",
    "#        }\n",
    "#        # Extract 4 main layers\n",
    "#        self.feature_extractor = create_feature_extractor(backbone, return_nodes)\n",
    "#        # Dry run to get number of channels for FPN\n",
    "#        inp = torch.randn(2, 3, 224, 224)\n",
    "#        with torch.no_grad():\n",
    "#            out = self.feature_extractor(inp)\n",
    "#        in_channels_list = [o.shape[1] for o in out.values()]\n",
    "#        # Build FPN\n",
    "#        fpn = FeaturePyramidNetwork(\n",
    "#            in_channels_list,\n",
    "#            out_channels=out_channels,\n",
    "#            extra_blocks=LastLevelP6P7(out_channels,out_channels)\n",
    "#        )\n",
    "#        self.fpn_features = nn.Sequential(self.feature_extractor, fpn)\n",
    "#        inp = torch.randn(2, 3, 224, 224)\n",
    "#        with torch.no_grad():\n",
    "#            out = self.fpn_features(inp)\n",
    "#        self.feat_sizes = [o.shape[2:] for o in out.values()]\n",
    "#        self.head = Head(out_channels, num_classes)\n",
    "#\n",
    "#    def forward(self, images):\n",
    "#        features = list(self.fpn_features(images).values())\n",
    "#        box_cls, box_regression, centerness = self.head(features)\n",
    "#        all_level_preds = (torch.cat([t.flatten(-2) for t in o], dim=-1) for o in [features, box_cls, box_regression, centerness])\n",
    "#        return (torch.permute(t, (0,2,1)) for t in all_level_preds)\n",
    "\n",
    "#dl = dm.val_dataloader()\n",
    "#for step, (images, bboxes, labels, image_paths) in enumerate(dl):\n",
    "#    print(images.shape)\n",
    "#    print(bboxes[0])\n",
    "#    print(labels[0])\n",
    "#    print(images[0, :, 200,200])\n",
    "#    break\n",
    "#    \n",
    "#torch.manual_seed(0)\n",
    "#torch.cuda.manual_seed(0)\n",
    "#    \n",
    "#network = FCOS(num_classes=19)\n",
    "#features, box_cls, box_regression, centerness = network(images[:1, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bef7a0-4dc1-4dc6-977c-5547ad4af1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "def _compute_centerness_targets(reg_targets):\n",
    "    if len(reg_targets) == 0:\n",
    "        return reg_targets.new_zeros(len(reg_targets))\n",
    "    left_right = reg_targets[:, [0, 2]]\n",
    "    top_bottom = reg_targets[:, [1, 3]]\n",
    "    centerness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\\n",
    "                (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0])\n",
    "    return torch.sqrt(centerness)\n",
    "\n",
    "\n",
    "def _calculate_reg_targets(xs, ys, bbox_targets):\n",
    "    l = xs[:, None] - bbox_targets[:, 0][None] # Lx1 - 1xT -> LxT\n",
    "    t = ys[:, None] - bbox_targets[:, 1][None]\n",
    "    r = bbox_targets[:, 2][None] - xs[:, None]\n",
    "    b = bbox_targets[:, 3][None] - ys[:, None]\n",
    "    return torch.stack([l, t, r, b], dim=2) # LxTx4\n",
    "\n",
    "\n",
    "def _apply_distance_constraints(reg_targets, level_distances):\n",
    "    max_reg_targets, _ = reg_targets.max(dim=2)\n",
    "    return torch.logical_and(max_reg_targets >= level_distances[:, None, 0], \\\n",
    "                             max_reg_targets <= level_distances[:, None, 1])\n",
    "\n",
    "\n",
    "def _prepare_labels(locations, targets_batch, reg_dists):\n",
    "    device = targets_batch[0].device\n",
    "    # L = sum locs per level x 2 : for each loc in all_locs, the max size of bb authorized\n",
    "    all_locations = locations.to(device) # Lx2\n",
    "    xs, ys = all_locations[:, 0], all_locations[:, 1] # L & L\n",
    "\n",
    "    all_reg_targets = []\n",
    "    all_cls_targets = []\n",
    "    for targets in targets_batch:\n",
    "        bbox_targets = targets[:, :4] # Tx4\n",
    "        cls_targets = targets[:, 4] # T\n",
    "        \n",
    "        # for each loc in L and each target in T, the reg target\n",
    "        reg_targets = _calculate_reg_targets(xs, ys, bbox_targets) # LxTx4\n",
    "\n",
    "        is_in_boxes = reg_targets.min(dim=2)[0] > 0 # min returns values and indices -> LxT\n",
    "\n",
    "        fits_to_feature_level = _apply_distance_constraints(\n",
    "            reg_targets, reg_dists).to(device) # LxT\n",
    "\n",
    "        bbox_areas = torchvision.ops.box_area(bbox_targets) # compared to above, does not deal with 0dim bb\n",
    "        \n",
    "        # area of each target bbox repeated for each loc with inf where the the loc is not \n",
    "        # in the target bbox or if the loc is not at the right level for this bbox size\n",
    "        locations_to_gt_area = bbox_areas[None].repeat(len(all_locations), 1) # LxT\n",
    "        locations_to_gt_area[is_in_boxes == 0] = INF\n",
    "        locations_to_gt_area[fits_to_feature_level == 0] = INF\n",
    "        \n",
    "        # for each loc, area and target idx of the target of min area at that loc\n",
    "        loc_min_area, loc_mind_idxs = locations_to_gt_area.min(dim=1) # val&idx, size L, idx in [0,T-1]\n",
    "\n",
    "        reg_targets = reg_targets[range(len(all_locations)), loc_mind_idxs] # Lx4\n",
    "        cls_targets = cls_targets[loc_mind_idxs] # L\n",
    "        cls_targets[loc_min_area == INF] = 0\n",
    "        \n",
    "        all_cls_targets.append(cls_targets)\n",
    "        all_reg_targets.append(reg_targets)\n",
    "    \n",
    "    return torch.stack(all_cls_targets), torch.stack(all_reg_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cd41acc-4e80-43d8-9c45-34a3dd319154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.utilities import rank_zero_info\n",
    "\n",
    "def nms(bounding_boxes,\n",
    "        confidence_scores,\n",
    "        classes,\n",
    "        threshold,\n",
    "        class_agnostic=True):\n",
    "    device = bounding_boxes.device\n",
    "    if len(bounding_boxes) == 0:\n",
    "        return torch.tensor([]).to(device), torch.tensor(\n",
    "            []).to(device), torch.tensor([]).to(device)\n",
    "\n",
    "    bounding_boxes = bounding_boxes.detach().cpu().numpy()\n",
    "    confidence_scores = confidence_scores.detach().cpu().numpy()\n",
    "    classes = classes.detach().cpu().numpy()\n",
    "\n",
    "    start_x = bounding_boxes[:, 0]\n",
    "    start_y = bounding_boxes[:, 1]\n",
    "    end_x = bounding_boxes[:, 2]\n",
    "    end_y = bounding_boxes[:, 3]\n",
    "\n",
    "    picked_boxes = []\n",
    "    picked_scores = []\n",
    "    picked_classes = []\n",
    "\n",
    "    areas = (end_x - start_x + 1) * (end_y - start_y + 1)\n",
    "\n",
    "    order = np.argsort(confidence_scores)\n",
    "    \n",
    "    while order.size > 0:\n",
    "        index = order[-1]\n",
    "        picked_boxes.append(bounding_boxes[index])\n",
    "        picked_scores.append(confidence_scores[index])\n",
    "        picked_classes.append(classes[index])\n",
    "\n",
    "        order = order[:-1]\n",
    "        if len(order) == 0:\n",
    "            break\n",
    "\n",
    "        x1 = np.maximum(start_x[index], start_x[order])\n",
    "        x2 = np.minimum(end_x[index], end_x[order])\n",
    "        y1 = np.maximum(start_y[index], start_y[order])\n",
    "        y2 = np.minimum(end_y[index], end_y[order])\n",
    "\n",
    "        w = np.maximum(0.0, x2 - x1 + 1)\n",
    "        h = np.maximum(0.0, y2 - y1 + 1)\n",
    "        intersection = w * h\n",
    "        ratio = intersection / (areas[index] + areas[order] - intersection)\n",
    "\n",
    "        if not class_agnostic:\n",
    "            other_classes = classes[order] != classes[index]\n",
    "            ratio[other_classes] = 0.0\n",
    "\n",
    "        left = np.where(ratio < threshold)\n",
    "        order = order[left]\n",
    "\n",
    "    outputs = [\n",
    "        torch.tensor(np.array(picked_boxes)).to(device),\n",
    "        torch.tensor(np.array(picked_scores)).to(device),\n",
    "        torch.tensor(np.array(picked_classes)).to(device)\n",
    "    ]\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def giou(pred, target, weight=None):\n",
    "\n",
    "    pred_left = pred[:, 0]\n",
    "    pred_top = pred[:, 1]\n",
    "    pred_right = pred[:, 2]\n",
    "    pred_bottom = pred[:, 3]\n",
    "\n",
    "    target_left = target[:, 0]\n",
    "    target_top = target[:, 1]\n",
    "    target_right = target[:, 2]\n",
    "    target_bottom = target[:, 3]\n",
    "\n",
    "    target_area = (target_left + target_right) * \\\n",
    "                  (target_top + target_bottom)\n",
    "    pred_area = (pred_left + pred_right) * \\\n",
    "                (pred_top + pred_bottom)\n",
    "\n",
    "    w_intersect = torch.min(pred_left, target_left) + torch.min(\n",
    "        pred_right, target_right)\n",
    "    g_w_intersect = torch.max(pred_left, target_left) + torch.max(\n",
    "        pred_right, target_right)\n",
    "    h_intersect = torch.min(pred_bottom, target_bottom) + torch.min(\n",
    "        pred_top, target_top)\n",
    "    g_h_intersect = torch.max(pred_bottom, target_bottom) + torch.max(\n",
    "        pred_top, target_top)\n",
    "    ac_uion = g_w_intersect * g_h_intersect + 1e-7\n",
    "    area_intersect = w_intersect * h_intersect\n",
    "    area_union = target_area + pred_area - area_intersect\n",
    "    #ious = (area_intersect + 1.0) / (area_union + 1.0)\n",
    "    ious = (area_intersect) / (area_union + 1e-7)\n",
    "    gious = ious - (ac_uion - area_union) / ac_uion\n",
    "    losses = 1 - gious\n",
    "\n",
    "    if weight is not None and weight.sum() > 0:\n",
    "        print(losses[:20])\n",
    "        print(weight[:20])\n",
    "        return (losses * weight).sum()\n",
    "    else:\n",
    "        assert losses.numel() != 0\n",
    "        return losses.sum()\n",
    "\n",
    "class FCOSPostProcessor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, pre_nms_thresh, pre_nms_top_n, nms_thresh,\n",
    "                 fpn_post_nms_top_n, min_size, num_classes):\n",
    "        super(FCOSPostProcessor, self).__init__()\n",
    "        self.pre_nms_thresh = pre_nms_thresh\n",
    "        self.pre_nms_top_n = pre_nms_top_n\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.fpn_post_nms_top_n = fpn_post_nms_top_n\n",
    "        self.min_size = min_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self,\n",
    "                locations, # sum num loc all levels (=L) x 2\n",
    "                cls_preds, # B x L x C \n",
    "                reg_preds, # B x L x 4\n",
    "                cness_preds, # B x L x 1\n",
    "                image_size):\n",
    "        \n",
    "        B, num_locs, C = cls_preds.shape\n",
    "        cls_preds = cls_preds.sigmoid() # BxLxC in [0,1]\n",
    "        cness_preds = cness_preds.sigmoid()\n",
    "        \n",
    "        candidate_inds = cls_preds > self.pre_nms_thresh # BxLxC\n",
    "        cls_preds = cls_preds * cness_preds # BxLxC\n",
    "        \n",
    "        pre_nms_top_n = candidate_inds.reshape(B, -1).sum(1) # B\n",
    "        pre_nms_top_n = pre_nms_top_n.clamp(max=self.pre_nms_top_n)\n",
    "        \n",
    "        #bboxes is a list of B tensors of size lx4 (filtered with pre_nms_threshold)\n",
    "        bboxes = []\n",
    "        cls_labels = []\n",
    "        scores = []\n",
    "        for i in range(B):\n",
    "            # Tensor with true where score for loc l and class c > pre_nms_thresh\n",
    "            per_candidate_inds = candidate_inds[i] # LxC\n",
    "            # tenseur de taille lx2 (l!=L) avec les indices des elem de cls_preds oÃ¹ > nms_thresh\n",
    "            per_candidate_nonzeros = per_candidate_inds.nonzero() \n",
    "            # dim l : positions dans [0,L] des elem dont cls_preds(c) > nms_thresh \n",
    "            per_box_loc = per_candidate_nonzeros[:, 0]\n",
    "            # dim l : classe dans [1, C] des elem dont cls_preds(h,w) > nms_thresh\n",
    "            per_class = per_candidate_nonzeros[:, 1] + 1\n",
    "\n",
    "            per_reg_preds = reg_preds[i] # Lx4\n",
    "            # liste des bb des elem dont cls_preds(c) > nms_thresh \n",
    "            per_reg_preds = per_reg_preds[per_box_loc] # lx4\n",
    "            per_locations = locations[per_box_loc] # lx2\n",
    "\n",
    "            per_pre_nms_top_n = pre_nms_top_n[i]\n",
    "            \n",
    "            per_cls_preds = cls_preds[i] # LxC\n",
    "            # tenseur de taille L avec les elem de cls_preds*centerness tels que cls_preds > nms_thresh\n",
    "            per_cls_preds = per_cls_preds[per_candidate_inds] \n",
    "            # si y a plus de per_pre_nms_topn qui passe nms_thresh (si l est trop longue)\n",
    "            if per_candidate_inds.sum().item() > per_pre_nms_top_n.item():\n",
    "                per_cls_preds, top_k_indices = per_cls_preds.topk(\n",
    "                    per_pre_nms_top_n, sorted=False)\n",
    "                per_class = per_class[top_k_indices]\n",
    "                per_reg_preds = per_reg_preds[top_k_indices]\n",
    "                per_locations = per_locations[top_k_indices]\n",
    "            \n",
    "            # Rewrites bbox (x0,y0,x1,y1) from reg targets (l,t,r,b) following eq (1) in paper\n",
    "            per_bboxes = torch.stack([\n",
    "                per_locations[:, 0] - per_reg_preds[:, 0],\n",
    "                per_locations[:, 1] - per_reg_preds[:, 1],\n",
    "                per_locations[:, 0] + per_reg_preds[:, 2],\n",
    "                per_locations[:, 1] + per_reg_preds[:, 3],\n",
    "            ], dim=1)\n",
    "            #per_bboxes = per_bboxes[:, [1,0,3,2]]\n",
    "            per_bboxes = torchvision.ops.clip_boxes_to_image(per_bboxes, (image_size, image_size))\n",
    "            per_bboxes = per_bboxes[torchvision.ops.remove_small_boxes(per_bboxes, self.min_size)]\n",
    "            per_scores = torch.sqrt(per_cls_preds)\n",
    "            \n",
    "            #picked_indices = torchvision.ops.nms(per_bboxes, per_scores, self.nms_thresh)\n",
    "            #print(picked_indices[:20])\n",
    "            #picked_boxes = per_bboxes[picked_indices]\n",
    "            #confidence_scores = per_scores[picked_indices]\n",
    "            #picked_classes = per_class[picked_indices]\n",
    "            picked_boxes, confidence_scores, picked_classes = nms(per_bboxes,\n",
    "                per_scores,\n",
    "                per_class,\n",
    "                self.nms_thresh,\n",
    "                class_agnostic=True)\n",
    "            \n",
    "            number_of_detections = len(picked_boxes)\n",
    "            if number_of_detections > self.fpn_post_nms_top_n > 0:\n",
    "                image_thresh, _ = torch.kthvalue(\n",
    "                    confidence_scores.cpu(),\n",
    "                    number_of_detections - self.fpn_post_nms_top_n + 1)\n",
    "                keep = confidence_scores >= image_thresh.item()\n",
    "\n",
    "                keep = torch.nonzero(keep).squeeze(1)\n",
    "                picked_boxes, confidence_scores, picked_classes = picked_boxes[\n",
    "                    keep], confidence_scores[keep], picked_classes[keep]\n",
    "\n",
    "            keep = confidence_scores >= self.pre_nms_thresh\n",
    "            picked_boxes, confidence_scores, picked_classes = picked_boxes[\n",
    "                keep], confidence_scores[keep], picked_classes[keep]\n",
    "            \n",
    "            bboxes.append(picked_boxes)\n",
    "            cls_labels.append(picked_classes)\n",
    "            scores.append(confidence_scores)\n",
    "        \n",
    "        return bboxes, scores, cls_labels\n",
    "\n",
    "\n",
    "INF = 100000000\n",
    "\n",
    "class FCOS(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        num_classes,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.post_processor = FCOSPostProcessor(\n",
    "            pre_nms_thresh=0.3,\n",
    "            pre_nms_top_n=100000,\n",
    "            nms_thresh=0.45,\n",
    "            fpn_post_nms_top_n=50,\n",
    "            min_size=0,\n",
    "            num_classes=num_classes)\n",
    "        self.fpn_strides = [8, 16, 32, 64, 128]\n",
    "        self.feat_sizes = network.feat_sizes\n",
    "        self.max_dist_per_level = [-1, 64, 128, 256, 512, INF]\n",
    "        self.num_classes = num_classes\n",
    "        self.network = network\n",
    "        self.map_metric = MeanAveragePrecision()\n",
    "        # locations is a list of num_feat_level elem, where each elem indicates the tensor of \n",
    "        # locations in the original image corresponding to each location in the feature map at this level\n",
    "        self.locations, self.reg_dists = self._compute_locations()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        parameters = list(self.parameters())\n",
    "        trainable_parameters = list(filter(lambda p: p.requires_grad, parameters))\n",
    "        rank_zero_info(\n",
    "            f\"The model will start training with only {sum([int(torch.numel(p)) for p in trainable_parameters])} \"\n",
    "            f\"trainable parameters out of {sum([int(torch.numel(p)) for p in parameters])}.\"\n",
    "        )\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=1e-3,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decay=5e-2,\n",
    "            eps=1e-8,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=1e-3,\n",
    "            steps_per_epoch=1000,\n",
    "            epochs=100\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\"\n",
    "            },\n",
    "        }\n",
    "    \n",
    "    def _compute_locations(self):\n",
    "        locations = []\n",
    "        reg_dists = []\n",
    "        \n",
    "        def _locations_per_level(h, w, s):\n",
    "            locs_x = [i for i in range(w)]\n",
    "            locs_y = [i for i in range(h)]\n",
    "            locs_x = [s / 2 + x * s for x in locs_x]\n",
    "            locs_y = [s / 2 + y * s for y in locs_y]\n",
    "            locs = [(x, y) for y in locs_y for x in locs_x]\n",
    "            return torch.tensor(locs)\n",
    "        \n",
    "        for level, (h,w) in enumerate(self.feat_sizes):\n",
    "            locs = _locations_per_level(h, w, self.fpn_strides[level])\n",
    "            locations.append(locs)\n",
    "            \n",
    "            level_distances = torch.tensor([\n",
    "                self.max_dist_per_level[level], self.max_dist_per_level[level+1]\n",
    "            ], dtype=torch.float32)\n",
    "            level_distances = level_distances.repeat(len(locs)).view(\n",
    "                len(locs), 2)\n",
    "            reg_dists.append(level_distances)\n",
    "            \n",
    "        all_locs = torch.cat(locations)\n",
    "        all_reg_dists = torch.cat(reg_dists)\n",
    "        return all_locs, all_reg_dists\n",
    "\n",
    "    def _get_cls_loss(self, cls_preds, cls_targets, total_num_pos):\n",
    "        nc = cls_preds.shape[-1]\n",
    "        onehot = F.one_hot(cls_targets.long(), nc+1)[...,1:].float()\n",
    "        cls_loss = torchvision.ops.sigmoid_focal_loss(cls_preds, onehot)\n",
    "        return cls_loss.sum() / total_num_pos\n",
    "\n",
    "    def _get_reg_loss(self, reg_preds, reg_targets, centerness_targets):\n",
    "        reg_preds = reg_preds.reshape(-1, 4)\n",
    "        reg_targets = reg_targets.reshape(-1, 4)\n",
    "        reg_loss = giou(reg_preds, reg_targets, weight=centerness_targets)\n",
    "        return reg_loss / centerness_targets.sum()\n",
    "\n",
    "    def _get_centerness_loss(self, centerness_preds, centerness_targets,\n",
    "                             total_num_pos):\n",
    "        centerness_loss = F.binary_cross_entropy_with_logits(\n",
    "            centerness_preds.squeeze(), centerness_targets, reduction='sum')\n",
    "        return centerness_loss / total_num_pos\n",
    "    \n",
    "    def forward(self, images, targets_batch=None):\n",
    "        features, cls_preds, reg_preds, cness_preds = self.network(images)\n",
    "        locations = self.locations.to(features.device)\n",
    "        predicted_boxes, scores, all_classes = self.post_processor(\n",
    "            locations, cls_preds, reg_preds, cness_preds, images.shape[-1])\n",
    "        \n",
    "        outputs = {}\n",
    "        if targets_batch != None:\n",
    "            reg_dists = self.reg_dists.to(features.device) # remove this by reorg code\n",
    "            cls_targets, reg_targets = _prepare_labels(locations, targets_batch, reg_dists)\n",
    "            pos_inds_b, pos_inds_loc = torch.nonzero(cls_targets > 0, as_tuple=True)\n",
    "            reg_preds = reg_preds[pos_inds_b, pos_inds_loc, :]\n",
    "            reg_targets = reg_targets[pos_inds_b, pos_inds_loc, :]\n",
    "            cness_preds = cness_preds[pos_inds_b, pos_inds_loc, :]\n",
    "            cness_targets = _compute_centerness_targets(reg_targets)\n",
    "            total_num_pos = max(pos_inds_b.new_tensor([pos_inds_b.numel()]), 1.0)\n",
    "            cls_loss = self._get_cls_loss(cls_preds, cls_targets, total_num_pos)\n",
    "            if pos_inds_b.numel() > 0:\n",
    "                reg_loss = self._get_reg_loss(reg_preds, reg_targets,\n",
    "                                              cness_targets)\n",
    "                centerness_loss = self._get_centerness_loss(cness_preds,\n",
    "                                                            cness_targets,\n",
    "                                                            total_num_pos)\n",
    "            else:\n",
    "                reg_loss = reg_preds.sum() # 0 ??\n",
    "                centerness_loss = cness_preds.sum() # 0 ??\n",
    "            \n",
    "            outputs[\"cls_loss\"] = cls_loss\n",
    "            outputs[\"reg_loss\"] = reg_loss\n",
    "            outputs[\"centerness_loss\"] = centerness_loss\n",
    "            outputs[\"combined_loss\"] = cls_loss + reg_loss + centerness_loss\n",
    "\n",
    "        outputs[\"predicted_boxes\"] = predicted_boxes\n",
    "        outputs[\"scores\"] = scores\n",
    "        outputs[\"pred_classes\"] = all_classes\n",
    "        return outputs\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, bboxes, labels, image_paths = batch\n",
    "        y = [torch.cat([bb, l], dim=1) for bb, l in zip(bboxes, labels)]\n",
    "        results = self.forward(x, y)\n",
    "        loss = results[\"combined_loss\"]\n",
    "        self.log(f\"loss/train\", loss.detach().item())\n",
    "        self.log(f\"cls_loss/train\", results[\"cls_loss\"].detach().item())\n",
    "        self.log(f\"reg_loss/train\", results[\"reg_loss\"].detach().item())\n",
    "        self.log(f\"centerness_loss/train\", results[\"centerness_loss\"].detach().item())\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, bboxes, labels, image_paths = batch\n",
    "        y = [torch.cat([bb, l], dim=1) for bb, l in zip(bboxes, labels)]\n",
    "        results = self.forward(x, y)\n",
    "        loss = results[\"combined_loss\"]\n",
    "        preds = [{'boxes': bb, 'scores': s, 'labels': l} for bb,s,l in zip(\n",
    "            results[\"predicted_boxes\"], results[\"scores\"], results[\"pred_classes\"]\n",
    "        )]\n",
    "        target_bb = [t[:, :4] for t in y]\n",
    "        target_l = [t[:, 4] for t in y]\n",
    "        targets = [{'boxes': bb, 'labels': l} for bb,l in zip(target_bb, target_l)]\n",
    "        self.map_metric.update(preds, targets)\n",
    "        self.log(f\"loss/val\", loss.detach().item())\n",
    "        self.log(f\"cls_loss/val\", results[\"cls_loss\"].detach().item())\n",
    "        self.log(f\"reg_loss/val\", results[\"reg_loss\"].detach().item())\n",
    "        self.log(f\"centerness_loss/val\", results[\"centerness_loss\"].detach().item())\n",
    "        if batch_idx==0:\n",
    "            self.trainer.logger.experiment.add_image_with_boxes(\n",
    "                f\"preds/val\",\n",
    "                x[0].detach().cpu(),\n",
    "                preds[0]['boxes'].detach().cpu(),\n",
    "                global_step=self.trainer.global_step,\n",
    "                dataformats='CHW', \n",
    "                labels=[f\"{l}: {s:.2f}\" for l,s in zip(preds[0]['labels'], preds[0]['scores'])]\n",
    "            )\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        mapmetric = self.map_metric.compute()['map']\n",
    "        self.log(\"map/val\", mapmetric)\n",
    "        print(\"\\nMAP: \", mapmetric)\n",
    "        self.map_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a167bec2-0832-44de-b285-8ab975255525",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 0.9999, 0.9999,\n",
      "        1.0000, 1.0000, 0.9999, 0.9998, 0.9996, 0.9999, 0.9997, 0.9996, 0.9999,\n",
      "        0.9990, 0.9995], grad_fn=<SliceBackward0>)\n",
      "tensor([0.0815, 0.3075, 0.1443, 0.5443, 0.2058, 0.7760, 0.2226, 0.8396, 0.1592,\n",
      "        0.6003, 0.0986, 0.3718, 0.0546, 0.2004, 0.1833, 0.1557, 0.3636, 0.1237,\n",
      "        0.3416, 0.1118])\n",
      "{'cls_loss': tensor([202.3216], grad_fn=<DivBackward0>), 'reg_loss': tensor(0.9999, grad_fn=<DivBackward0>), 'centerness_loss': tensor([0.6850], grad_fn=<DivBackward0>), 'combined_loss': tensor([204.0065], grad_fn=<AddBackward0>), 'predicted_boxes': [tensor([[483.8583,  11.4390, 484.1184,  12.3289],\n",
      "        [344.0000, 407.8074, 344.0000, 408.5421],\n",
      "        [248.0000, 407.8881, 248.0000, 408.6679],\n",
      "        [392.0000, 407.7247, 392.0000, 408.7883],\n",
      "        [327.9951, 408.0000, 328.0000, 408.6599],\n",
      "        [ 59.3715,  51.5601,  60.0000,  53.1626],\n",
      "        [ 26.8359, 451.5045,  28.0000, 452.6901],\n",
      "        [327.6914, 423.7030, 328.0000, 425.0627],\n",
      "        [475.9031,  35.3919, 476.0000,  36.9856],\n",
      "        [475.2083,  51.4929, 476.0000,  52.9627],\n",
      "        [471.6200,  23.8440, 472.0000,  24.0352],\n",
      "        [459.2961,  51.3780, 460.0000,  53.0924],\n",
      "        [456.0000, 343.9158, 456.0000, 344.3798],\n",
      "        [279.7180, 407.8240, 280.0000, 408.8322],\n",
      "        [200.0000, 407.8746, 200.0000, 408.7543],\n",
      "        [456.0000, 407.9310, 456.0000, 408.4260],\n",
      "        [312.0000, 408.0000, 312.0370, 408.7890],\n",
      "        [ 19.5786,  27.6722,  20.0000,  29.5358],\n",
      "        [451.3615,  51.5095, 452.0000,  53.1619],\n",
      "        [184.0000, 407.8114, 184.0000, 408.7933],\n",
      "        [216.0000, 407.6812, 216.0000, 408.6921],\n",
      "        [391.8759, 327.4359, 392.0000, 328.5810],\n",
      "        [104.0000, 407.8337, 104.0000, 408.5807],\n",
      "        [ 91.2964,  51.5944,  92.0000,  53.0121],\n",
      "        [287.7463, 287.8302, 288.0000, 288.6423],\n",
      "        [ 67.4209,  51.4277,  68.0000,  53.1644],\n",
      "        [475.6366,  43.6479, 476.0000,  45.1451],\n",
      "        [459.2321, 443.4016, 460.0000, 445.0424],\n",
      "        [359.8869, 247.5642, 360.0000, 248.5595],\n",
      "        [168.0000, 407.8763, 168.0000, 408.6618],\n",
      "        [450.9608, 451.5971, 452.0536, 452.8399],\n",
      "        [ 27.3834, 467.3990,  28.0015, 468.5819],\n",
      "        [275.0533, 451.6509, 276.0000, 453.1044],\n",
      "        [439.7856, 391.9286, 440.0000, 392.8196],\n",
      "        [459.4569, 467.5424, 460.0000, 468.9608],\n",
      "        [ 83.4158,  51.4336,  84.0000,  53.1699],\n",
      "        [ 11.8371,  27.1778,  12.0000,  29.4632],\n",
      "        [107.8109,  35.6586, 108.0000,  37.0885],\n",
      "        [152.0000, 407.8864, 152.0000, 408.5995],\n",
      "        [391.5317, 423.6035, 392.0000, 425.0886],\n",
      "        [216.0000, 391.8413, 216.0000, 392.6097],\n",
      "        [107.5659,  43.7913, 108.0000,  44.9125],\n",
      "        [360.0000, 327.9551, 360.0000, 328.6340],\n",
      "        [264.0000, 407.7097, 264.0000, 408.8098],\n",
      "        [200.0000, 391.8620, 200.0000, 392.7041],\n",
      "        [235.3243, 443.4197, 236.0000, 445.0966],\n",
      "        [328.0000, 391.8856, 328.0000, 392.4791],\n",
      "        [ 19.0847, 459.6816,  20.0000, 460.9506],\n",
      "        [467.6987,  35.5621, 468.0000,  36.7640],\n",
      "        [387.3100, 443.4775, 388.0000, 445.0464]]), tensor([[ 27.4686,  75.1286,  28.0000,  77.0511],\n",
      "        [483.8814,  11.5102, 484.2427,  12.2570],\n",
      "        [ 19.6262,  75.2938,  20.0000,  77.1094],\n",
      "        [ 99.5460,  75.1555, 100.0000,  77.2252],\n",
      "        [352.0000, 351.9442, 352.0000, 352.6018],\n",
      "        [ 35.6760,  74.9721,  36.0000,  77.1003],\n",
      "        [223.7022, 351.6255, 224.0000, 352.2883],\n",
      "        [459.5715,  75.1610, 460.0000,  77.1825],\n",
      "        [ 59.6657,  75.0765,  60.0000,  77.1725],\n",
      "        [471.6001,  24.0000, 472.0700,  24.0680],\n",
      "        [295.9954, 391.7147, 296.0000, 392.8261],\n",
      "        [107.6689,  75.0637, 108.0000,  77.1346],\n",
      "        [455.6492,  24.0000, 456.1630,  24.2949],\n",
      "        [ 51.6086,  75.1533,  52.0000,  77.2129],\n",
      "        [123.6783,  75.2298, 124.0000,  77.1040],\n",
      "        [ 43.7214,  75.1299,  44.0000,  77.2763],\n",
      "        [ 67.7460,  75.0879,  68.0000,  77.1380],\n",
      "        [451.5716,  75.1092, 452.0000,  77.2034],\n",
      "        [443.6463,  75.0973, 444.0000,  77.1744],\n",
      "        [435.6437,  75.0980, 436.0000,  77.1745],\n",
      "        [427.6431,  75.0993, 428.0000,  77.1727],\n",
      "        [296.0000, 375.7674, 296.0000, 376.6626],\n",
      "        [419.6427,  75.0980, 420.0000,  77.1704],\n",
      "        [411.6396,  75.0988, 412.0000,  77.1681],\n",
      "        [403.6376,  75.0997, 404.0000,  77.1689],\n",
      "        [395.6372,  75.1005, 396.0000,  77.1668],\n",
      "        [387.6342,  75.1005, 388.0000,  77.1659],\n",
      "        [131.5427,  75.0552, 132.0000,  77.2195],\n",
      "        [379.6323,  75.1011, 380.0000,  77.1652],\n",
      "        [371.6324,  75.1007, 372.0000,  77.1651],\n",
      "        [363.6322,  75.1034, 364.0000,  77.1639],\n",
      "        [355.6317,  75.1045, 356.0000,  77.1628],\n",
      "        [171.6392,  75.0967, 172.0000,  77.1504],\n",
      "        [347.6298,  75.1050, 348.0000,  77.1622],\n",
      "        [339.6302,  75.1061, 340.0000,  77.1630],\n",
      "        [203.6266,  75.1076, 204.0000,  77.1574],\n",
      "        [195.6293,  75.1097, 196.0000,  77.1597],\n",
      "        [211.6252,  75.1078, 212.0000,  77.1576],\n",
      "        [331.6293,  75.1065, 332.0000,  77.1632],\n",
      "        [ 11.9262,  75.3328,  12.0000,  76.9232],\n",
      "        [307.6260,  75.1077, 308.0000,  77.1597],\n",
      "        [259.6248,  75.1098, 260.0000,  77.1596],\n",
      "        [179.6303,  75.0997, 180.0000,  77.1509],\n",
      "        [243.6258,  75.1084, 244.0000,  77.1592],\n",
      "        [323.6281,  75.1077, 324.0000,  77.1613],\n",
      "        [315.6283,  75.1073, 316.0000,  77.1584],\n",
      "        [219.6250,  75.1080, 220.0000,  77.1585],\n",
      "        [291.6265,  75.1097, 292.0000,  77.1584],\n",
      "        [267.6255,  75.1084, 268.0000,  77.1598],\n",
      "        [235.6248,  75.1079, 236.0000,  77.1585]]), tensor([[483.8781,  11.4947, 484.2176,  12.2747],\n",
      "        [407.9560, 391.6502, 408.0000, 392.7603],\n",
      "        [312.0000, 391.7157, 312.0000, 392.6947],\n",
      "        [423.9671, 391.7008, 424.0000, 392.8762],\n",
      "        [120.0000, 407.8427, 120.1684, 408.8608],\n",
      "        [216.0000, 391.7262, 216.0000, 392.7281],\n",
      "        [136.0000, 407.7759, 136.1343, 408.8312],\n",
      "        [ 40.0000, 375.6269,  40.0000, 376.7587],\n",
      "        [392.0000, 407.7656, 392.2403, 408.6675],\n",
      "        [360.0000, 391.7831, 360.0000, 392.6448],\n",
      "        [439.8051, 391.7670, 440.0000, 392.7865],\n",
      "        [344.0000, 391.7163, 344.0000, 392.6670],\n",
      "        [152.0000, 391.8542, 152.0000, 392.6515],\n",
      "        [328.0000, 391.7201, 328.0000, 392.7157],\n",
      "        [392.0000, 391.7899, 392.0000, 392.6940],\n",
      "        [408.0000, 423.7202, 408.0335, 424.5623],\n",
      "        [471.6455,  24.0000, 472.0909,  24.0152],\n",
      "        [407.9188, 407.6887, 408.1941, 408.7029],\n",
      "        [232.0000, 391.8190, 232.0000, 392.6769],\n",
      "        [327.9014, 407.8120, 328.1574, 408.7714],\n",
      "        [376.0000, 391.7580, 376.0000, 392.5884],\n",
      "        [232.0000, 407.7653, 232.1658, 408.8929],\n",
      "        [455.6689,  24.0000, 456.1788,  24.2936],\n",
      "        [120.0000, 391.8397, 120.0000, 392.8634],\n",
      "        [168.0000, 391.9069, 168.0000, 392.6821],\n",
      "        [136.0000, 391.9000, 136.0000, 392.7148],\n",
      "        [ 72.0000, 423.5937,  72.0109, 424.6680],\n",
      "        [183.9827, 407.8867, 184.0860, 408.8455],\n",
      "        [184.0000, 391.7203, 184.0000, 392.8694],\n",
      "        [168.0000, 407.8593, 168.1293, 408.8762],\n",
      "        [279.9528, 391.6841, 280.0000, 392.8781],\n",
      "        [312.0000, 423.6915, 312.2514, 424.5449],\n",
      "        [375.9439, 407.9030, 376.0503, 408.7293],\n",
      "        [311.8483, 407.8004, 312.1512, 408.7419],\n",
      "        [ 23.9487, 407.9809,  24.0142, 408.7138],\n",
      "        [200.0000, 407.8651, 200.1085, 408.9518],\n",
      "        [ 87.8985, 391.7173,  88.0000, 392.7333],\n",
      "        [263.8019, 407.8058, 264.0757, 408.7318],\n",
      "        [359.9973, 407.7373, 360.0665, 408.7761],\n",
      "        [ 71.9806, 391.7667,  72.0000, 392.7802],\n",
      "        [248.0000, 391.7481, 248.0000, 392.7002],\n",
      "        [103.9243, 407.9333, 104.1418, 408.8749],\n",
      "        [ 19.8860,  67.4514,  20.0000,  68.6768],\n",
      "        [439.7532, 407.9158, 440.0000, 408.7253],\n",
      "        [104.0000, 247.3691, 104.0000, 248.3590],\n",
      "        [ 39.8556, 407.6689,  40.0000, 408.9459],\n",
      "        [136.0000, 423.5986, 136.1662, 424.6429],\n",
      "        [295.9252, 391.7123, 296.0000, 392.8008],\n",
      "        [ 24.0000, 391.9011,  24.0000, 392.5730],\n",
      "        [216.0000, 407.7344, 216.1150, 408.8543]]), tensor([[483.8620,  11.4030, 484.1271,  12.3587],\n",
      "        [123.2829, 283.5525, 124.0000, 284.6146],\n",
      "        [123.5343, 235.8930, 124.0000, 236.5522],\n",
      "        [168.0000, 407.8517, 168.1413, 408.5473],\n",
      "        [123.4240, 323.9702, 124.0000, 324.4784],\n",
      "        [123.4498, 347.8290, 124.0000, 348.4904],\n",
      "        [131.2439, 243.8614, 132.0000, 244.4946],\n",
      "        [123.5490, 227.7457, 124.0000, 228.5134],\n",
      "        [107.3784, 355.7847, 108.0000, 356.4668],\n",
      "        [184.0000, 408.0000, 184.1051, 408.7133],\n",
      "        [471.6148,  23.8182, 472.0000,  24.0384],\n",
      "        [107.4715, 339.7249, 108.0000, 340.4570],\n",
      "        [123.3169, 275.6542, 124.0000, 276.6074],\n",
      "        [ 99.3180, 307.7869, 100.0000, 308.3978],\n",
      "        [107.3124, 251.7379, 108.0000, 252.4404],\n",
      "        [115.1750, 355.8796, 116.0000, 356.4956],\n",
      "        [123.3939, 243.8002, 124.0000, 244.5265],\n",
      "        [123.2434, 355.9078, 124.0000, 356.4747],\n",
      "        [147.2540, 187.9858, 148.0000, 188.7607],\n",
      "        [107.4589, 315.8584, 108.0000, 316.4494],\n",
      "        [107.3582, 243.7334, 108.0000, 244.3755],\n",
      "        [480.0000, 287.8648, 480.0000, 288.5590],\n",
      "        [107.3680, 227.8613, 108.0000, 228.5993],\n",
      "        [131.1409, 443.4373, 132.0000, 445.0570],\n",
      "        [200.0000, 407.9607, 200.0000, 408.6379],\n",
      "        [123.6233, 219.9538, 124.0000, 220.5628],\n",
      "        [115.3418, 235.9268, 116.0000, 236.5208],\n",
      "        [115.2762, 347.9323, 116.0000, 348.4594],\n",
      "        [107.3990, 299.8293, 108.0000, 300.4301],\n",
      "        [ 26.8776, 451.4693,  28.0000, 452.6125],\n",
      "        [107.5428, 363.8041, 108.0000, 364.4368],\n",
      "        [131.5253, 259.9167, 132.0000, 260.5940],\n",
      "        [115.4395, 339.7602, 116.0000, 340.5233],\n",
      "        [115.3361, 243.8051, 116.0000, 244.5466],\n",
      "        [107.3886, 283.7324, 108.0000, 284.3812],\n",
      "        [475.9042,  35.3921, 476.0000,  36.9909],\n",
      "        [107.3924, 259.8123, 108.0000, 260.4285],\n",
      "        [131.3979, 219.8253, 132.0000, 220.5105],\n",
      "        [131.4343, 235.7514, 132.0000, 236.4763],\n",
      "        [115.5240, 227.9514, 116.0000, 228.5900],\n",
      "        [107.3645, 347.8556, 108.0000, 348.5967],\n",
      "        [123.3513, 267.9260, 124.0000, 268.5064],\n",
      "        [115.3214, 283.6465, 116.0000, 284.4140],\n",
      "        [475.2445,  51.5010, 476.0000,  52.9600],\n",
      "        [328.0000, 407.7687, 328.0000, 408.5462],\n",
      "        [155.4935, 195.8035, 156.0000, 196.7537],\n",
      "        [107.4199, 331.8416, 108.0000, 332.3878],\n",
      "        [ 99.3051, 315.7737, 100.0000, 316.5709],\n",
      "        [ 99.4589,  51.6964, 100.0000,  53.1192],\n",
      "        [115.3841, 315.9174, 116.0000, 316.4713]])], 'scores': [tensor([0.6776, 0.6710, 0.6638, 0.6620, 0.6619, 0.6600, 0.6590, 0.6589, 0.6587,\n",
      "        0.6586, 0.6581, 0.6574, 0.6554, 0.6545, 0.6532, 0.6504, 0.6501, 0.6488,\n",
      "        0.6475, 0.6474, 0.6468, 0.6442, 0.6441, 0.6439, 0.6439, 0.6437, 0.6437,\n",
      "        0.6424, 0.6424, 0.6423, 0.6421, 0.6415, 0.6412, 0.6408, 0.6403, 0.6402,\n",
      "        0.6401, 0.6398, 0.6391, 0.6390, 0.6390, 0.6388, 0.6384, 0.6383, 0.6380,\n",
      "        0.6379, 0.6374, 0.6367, 0.6361, 0.6353]), tensor([0.6954, 0.6790, 0.6733, 0.6718, 0.6659, 0.6656, 0.6616, 0.6595, 0.6589,\n",
      "        0.6579, 0.6561, 0.6558, 0.6554, 0.6529, 0.6519, 0.6506, 0.6496, 0.6484,\n",
      "        0.6484, 0.6480, 0.6475, 0.6473, 0.6472, 0.6469, 0.6466, 0.6464, 0.6460,\n",
      "        0.6457, 0.6455, 0.6455, 0.6452, 0.6448, 0.6448, 0.6444, 0.6442, 0.6441,\n",
      "        0.6441, 0.6439, 0.6439, 0.6439, 0.6437, 0.6437, 0.6437, 0.6437, 0.6437,\n",
      "        0.6437, 0.6436, 0.6436, 0.6436, 0.6436]), tensor([0.6868, 0.6794, 0.6772, 0.6763, 0.6745, 0.6720, 0.6677, 0.6661, 0.6649,\n",
      "        0.6649, 0.6648, 0.6646, 0.6628, 0.6620, 0.6618, 0.6616, 0.6616, 0.6610,\n",
      "        0.6608, 0.6601, 0.6596, 0.6568, 0.6565, 0.6560, 0.6541, 0.6533, 0.6533,\n",
      "        0.6524, 0.6521, 0.6519, 0.6513, 0.6512, 0.6503, 0.6502, 0.6497, 0.6494,\n",
      "        0.6494, 0.6489, 0.6479, 0.6477, 0.6470, 0.6466, 0.6464, 0.6461, 0.6458,\n",
      "        0.6455, 0.6442, 0.6440, 0.6430, 0.6428]), tensor([0.6811, 0.6776, 0.6741, 0.6733, 0.6731, 0.6731, 0.6699, 0.6698, 0.6698,\n",
      "        0.6683, 0.6678, 0.6667, 0.6666, 0.6655, 0.6644, 0.6637, 0.6637, 0.6632,\n",
      "        0.6632, 0.6630, 0.6629, 0.6619, 0.6616, 0.6600, 0.6592, 0.6591, 0.6589,\n",
      "        0.6585, 0.6581, 0.6580, 0.6569, 0.6566, 0.6562, 0.6559, 0.6555, 0.6551,\n",
      "        0.6551, 0.6549, 0.6543, 0.6532, 0.6530, 0.6527, 0.6527, 0.6516, 0.6512,\n",
      "        0.6509, 0.6505, 0.6504, 0.6503, 0.6501])], 'pred_classes': [tensor([ 4, 11, 11, 11, 11, 11,  4, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 14, 11, 11, 16, 11, 11,  9, 11, 11, 11,  4,  4, 11,  4, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11,  9, 11, 11, 11,  9]), tensor([11,  4, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11,  4, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]), tensor([ 4,  4,  4,  4,  4,  4,  4,  2,  4,  4,  4,  4,  4,  4,  4, 11, 11, 11,\n",
      "        11,  4,  4, 11, 11,  4,  4,  4, 11,  4,  4,  4,  4, 11, 11, 11,  4,  4,\n",
      "        11, 11, 11,  4, 11,  4, 11,  4,  4, 11, 11,  4,  4, 11]), tensor([ 4, 17, 17, 11, 11, 17, 11, 17, 17, 11, 11, 17, 17, 17, 17, 17, 17, 17,\n",
      "        11, 17, 17, 14, 17,  9, 11, 17, 17, 17, 17,  4, 17, 11, 17, 17, 17, 11,\n",
      "        17, 11, 17, 17, 17, 17, 17, 11, 11, 11, 17, 17, 11, 11])]}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "from dl_toolbox.networks.fcos import FCOS as net\n",
    "    \n",
    "network = net(num_classes=19)\n",
    "  \n",
    "module = FCOS(\n",
    "    network,\n",
    "    num_classes=19\n",
    ")\n",
    "dl = dm.val_dataloader()\n",
    "for step, (images, bboxes, labels, image_paths) in enumerate(dl):\n",
    "    y = [torch.cat([bb, l], dim=1) for bb, l in zip(bboxes, labels)]\n",
    "    results = module.forward(images, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d01e6b-5b98-4aae-a18b-a87aa7c0f8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from dl_toolbox.callbacks import ProgressBar, FeatureFt\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "tensorboard = pl.loggers.TensorBoardLogger(\n",
    "    \"lightning_logs\", \"\", \"\", default_hp_metric=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=200,\n",
    "    limit_train_batches=1.,\n",
    "    limit_val_batches=1.,\n",
    "    callbacks=[ProgressBar(), FeatureFt(do_finetune=True)],\n",
    "    logger=tensorboard\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer.fit(\n",
    "    module,\n",
    "    datamodule=dm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427863c6-95db-4b92-9845-c4c031dab227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
