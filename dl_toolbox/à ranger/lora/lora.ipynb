{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f96ad40-9784-4845-b82e-9c241f69a30b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CustomCollate' from 'dl_toolbox.utils' (/d/pfournie/dl_toolbox/dl_toolbox/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Subset, RandomSampler\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdl_toolbox\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CustomCollate\n\u001b[1;32m      8\u001b[0m transform \u001b[38;5;241m=\u001b[39m v2\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      9\u001b[0m     v2\u001b[38;5;241m.\u001b[39mResize(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m), antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     10\u001b[0m     v2\u001b[38;5;241m.\u001b[39mToDtype(torch\u001b[38;5;241m.\u001b[39mfloat32, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     11\u001b[0m     v2\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),\n\u001b[1;32m     12\u001b[0m ])\n\u001b[1;32m     14\u001b[0m NB_IMG \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m45\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m700\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CustomCollate' from 'dl_toolbox.utils' (/d/pfournie/dl_toolbox/dl_toolbox/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "import dl_toolbox.datasets as datasets\n",
    "from torch.utils.data import Subset, RandomSampler\n",
    "import torch\n",
    "from dl_toolbox.utils import CustomCollate\n",
    "\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.Resize(size=(224, 224), antialias=True),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "NB_IMG = 45*700\n",
    "dataset = datasets.Resisc('/data/NWPU-RESISC45', transform, 'all45')\n",
    "trainset = Subset(dataset, indices=[i for i in range(NB_IMG) if 100<=i%700])\n",
    "valset = Subset(dataset, indices=[i for i in range(NB_IMG) if 100>i%700])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    collate_fn=CustomCollate(),\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    sampler=RandomSampler(\n",
    "        trainset,\n",
    "        replacement=True,\n",
    "        num_samples=5000\n",
    "    ),\n",
    "    drop_last=True,\n",
    "    batch_size=4,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valset,\n",
    "    collate_fn=CustomCollate(),\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb213b9-b5bc-4865-9be9-2c417e7b51b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "def train(model, criterion, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    #optimizer.train()\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        data, target = batch['image'], batch['label']\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c22745-20a6-48e5-ab6e-d04c0814af52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, criterion, optimizer, device, test_loader):\n",
    "    model.eval()\n",
    "    #optimizer.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            data, target = batch['image'], batch['label']\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86e6f5-7813-4f57-bca4-3f3235dc1d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "import minlora \n",
    "\n",
    "def get_lora_config(rank):\n",
    "    return {  # specify which layers to add lora to, by default only add to linear layers\n",
    "        nn.Linear: {\n",
    "            \"weight\": partial(minlora.LoRAParametrization.from_linear, rank=rank),\n",
    "        },\n",
    "    }\n",
    "\n",
    "class vit_ft(nn.Module):\n",
    "    def __init__(self, freeze, lora, rank):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model('vit_base_patch16_224', pretrained=True, global_pool='token')\n",
    "        if freeze:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if lora:\n",
    "            cfg = get_lora_config(rank)\n",
    "            minlora.add_lora(self.encoder, lora_config=cfg)\n",
    "        self.head = nn.Linear(self.encoder.num_features, 45)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.encoder.forward_features(x)\n",
    "        x = x[:, self.encoder.num_prefix_tokens:].mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b419ec3-df53-4324-acd9-fc79790b12bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def name_is_lora(name):\n",
    "    return (\n",
    "        len(name.split(\".\")) >= 4\n",
    "        and (name.split(\".\")[-4]) == \"parametrizations\"\n",
    "        and name.split(\".\")[-1] in [\"lora_A\", \"lora_B\"]\n",
    "    )\n",
    "\n",
    "def name_is_head(name):\n",
    "    return (name.split(\".\")[0]) == \"head\"\n",
    "\n",
    "def lora_or_head(name):\n",
    "    return name_is_lora(name) or name_is_head(name)\n",
    "\n",
    "def get_params_by_name(model, print_shapes=False, name_filter=None):\n",
    "    for n, p in model.named_parameters():\n",
    "        if name_filter is None or name_filter(n):\n",
    "            if print_shapes:\n",
    "                print(n, p.shape)\n",
    "            yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a7936-7788-4be4-b632-7d01f8f8ebd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "model = vit_ft(freeze=True, lora=False, rank=4)\n",
    "print([p[0] for p in list(model.named_parameters())][-10:])\n",
    "parameters = list(model.parameters())\n",
    "trainable_parameters = list(get_params_by_name(model, name_filter=lora_or_head))\n",
    "print(\n",
    "    f\"The model will start training with only {sum([int(torch.numel(p)) for p in trainable_parameters])} \"\n",
    "    f\"trainable parameters out of {sum([int(torch.numel(p)) for p in parameters])}.\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    trainable_parameters,\n",
    "    lr=1e-3,\n",
    ")\n",
    "\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "for epoch in range(1, 10):\n",
    "    train(model, criterion, device, train_loader, optimizer, epoch)\n",
    "    test(model, criterion, optimizer, device, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3299f9-8886-439e-9ab6-c1478dde56d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
