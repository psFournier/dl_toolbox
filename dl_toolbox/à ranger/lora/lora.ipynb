{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f96ad40-9784-4845-b82e-9c241f69a30b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "import dl_toolbox.datasets as datasets\n",
    "from torch.utils.data import Subset, RandomSampler\n",
    "import torch\n",
    "from dl_toolbox.utils import CustomCollate\n",
    "\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.Resize(size=(224, 224), antialias=True),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "NB_IMG = 45*700\n",
    "dataset = datasets.Resisc('/data/NWPU-RESISC45', transform, 'all45')\n",
    "trainset = Subset(dataset, indices=[i for i in range(NB_IMG) if 100<=i%700])\n",
    "valset = Subset(dataset, indices=[i for i in range(NB_IMG) if 100>i%700])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    collate_fn=CustomCollate(),\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    sampler=RandomSampler(\n",
    "        trainset,\n",
    "        replacement=True,\n",
    "        num_samples=5000\n",
    "    ),\n",
    "    drop_last=True,\n",
    "    batch_size=4,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valset,\n",
    "    collate_fn=CustomCollate(),\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eb213b9-b5bc-4865-9be9-2c417e7b51b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "def train(model, criterion, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    #optimizer.train()\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        data, target = batch['image'], batch['label']\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c22745-20a6-48e5-ab6e-d04c0814af52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, criterion, optimizer, device, test_loader):\n",
    "    model.eval()\n",
    "    #optimizer.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            data, target = batch['image'], batch['label']\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d86e6f5-7813-4f57-bca4-3f3235dc1d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "import minlora \n",
    "\n",
    "def get_lora_config(rank):\n",
    "    return {  # specify which layers to add lora to, by default only add to linear layers\n",
    "        nn.Linear: {\n",
    "            \"weight\": partial(LoRAParametrization.from_linear, rank=rank),\n",
    "        },\n",
    "    }\n",
    "\n",
    "class vit_ft(nn.Module):\n",
    "    def __init__(self, freeze, lora, rank):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model('vit_base_patch16_224', pretrained=True, global_pool='token')\n",
    "        if freeze:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if lora:\n",
    "            cfg = get_lora_config(rank)\n",
    "            minlora.add_lora(self.encoder, lora_config=cfg)\n",
    "        self.head = nn.Linear(self.encoder.num_features, 45)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.encoder.forward_features(x)\n",
    "        x = x[:, self.encoder.num_prefix_tokens:].mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b419ec3-df53-4324-acd9-fc79790b12bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def name_is_lora(name):\n",
    "    return (\n",
    "        len(name.split(\".\")) >= 4\n",
    "        and (name.split(\".\")[-4]) == \"parametrizations\"\n",
    "        and name.split(\".\")[-1] in [\"lora_A\", \"lora_B\"]\n",
    "    )\n",
    "\n",
    "def name_is_head(name):\n",
    "    return (name.split(\".\")[0]) == \"head\"\n",
    "\n",
    "def lora_or_head(name):\n",
    "    return name_is_lora(name) or name_is_head(name)\n",
    "\n",
    "def get_params_by_name(model, print_shapes=False, name_filter=None):\n",
    "    for n, p in model.named_parameters():\n",
    "        if name_filter is None or name_filter(n):\n",
    "            if print_shapes:\n",
    "                print(n, p.shape)\n",
    "            yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182a7936-7788-4be4-b632-7d01f8f8ebd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['encoder.blocks.11.mlp.fc1.weight', 'encoder.blocks.11.mlp.fc1.bias', 'encoder.blocks.11.mlp.fc2.weight', 'encoder.blocks.11.mlp.fc2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'encoder.head.weight', 'encoder.head.bias', 'head.weight', 'head.bias']\n",
      "The model will start training with only 34605 trainable parameters out of 86602261.\n",
      "Train Epoch: 1 [0/27000 (0%)]\tLoss: 4.663311\n",
      "Train Epoch: 1 [400/27000 (8%)]\tLoss: 2.891991\n",
      "Train Epoch: 1 [800/27000 (16%)]\tLoss: 1.728450\n",
      "Train Epoch: 1 [1200/27000 (24%)]\tLoss: 1.110016\n",
      "Train Epoch: 1 [1600/27000 (32%)]\tLoss: 0.848358\n",
      "Train Epoch: 1 [2000/27000 (40%)]\tLoss: 0.590583\n",
      "Train Epoch: 1 [2400/27000 (48%)]\tLoss: 1.534051\n",
      "Train Epoch: 1 [2800/27000 (56%)]\tLoss: 0.518393\n",
      "Train Epoch: 1 [3200/27000 (64%)]\tLoss: 2.076843\n",
      "Train Epoch: 1 [3600/27000 (72%)]\tLoss: 3.224518\n",
      "Train Epoch: 1 [4000/27000 (80%)]\tLoss: 0.570041\n",
      "Train Epoch: 1 [4400/27000 (88%)]\tLoss: 0.719603\n",
      "Train Epoch: 1 [4800/27000 (96%)]\tLoss: 0.510974\n",
      "\n",
      "Test set: Average loss: 0.1071, Accuracy: 3400/4500 (75.56%)\n",
      "\n",
      "Train Epoch: 2 [0/27000 (0%)]\tLoss: 1.140894\n",
      "Train Epoch: 2 [400/27000 (8%)]\tLoss: 3.084741\n",
      "Train Epoch: 2 [800/27000 (16%)]\tLoss: 0.732569\n",
      "Train Epoch: 2 [1200/27000 (24%)]\tLoss: 0.355663\n",
      "Train Epoch: 2 [1600/27000 (32%)]\tLoss: 0.387237\n",
      "Train Epoch: 2 [2000/27000 (40%)]\tLoss: 0.618884\n",
      "Train Epoch: 2 [2400/27000 (48%)]\tLoss: 1.183207\n",
      "Train Epoch: 2 [2800/27000 (56%)]\tLoss: 0.154381\n",
      "Train Epoch: 2 [3200/27000 (64%)]\tLoss: 0.204315\n",
      "Train Epoch: 2 [3600/27000 (72%)]\tLoss: 0.113852\n",
      "Train Epoch: 2 [4000/27000 (80%)]\tLoss: 0.846475\n",
      "Train Epoch: 2 [4400/27000 (88%)]\tLoss: 0.121989\n",
      "Train Epoch: 2 [4800/27000 (96%)]\tLoss: 0.809360\n",
      "\n",
      "Test set: Average loss: 0.1146, Accuracy: 3450/4500 (76.67%)\n",
      "\n",
      "Train Epoch: 3 [0/27000 (0%)]\tLoss: 1.494410\n",
      "Train Epoch: 3 [400/27000 (8%)]\tLoss: 0.004110\n",
      "Train Epoch: 3 [800/27000 (16%)]\tLoss: 2.125027\n",
      "Train Epoch: 3 [1200/27000 (24%)]\tLoss: 1.011705\n",
      "Train Epoch: 3 [1600/27000 (32%)]\tLoss: 0.399038\n",
      "Train Epoch: 3 [2000/27000 (40%)]\tLoss: 2.086751\n",
      "Train Epoch: 3 [2400/27000 (48%)]\tLoss: 0.016087\n",
      "Train Epoch: 3 [2800/27000 (56%)]\tLoss: 0.000015\n",
      "Train Epoch: 3 [3200/27000 (64%)]\tLoss: 0.625825\n",
      "Train Epoch: 3 [3600/27000 (72%)]\tLoss: 1.957103\n",
      "Train Epoch: 3 [4000/27000 (80%)]\tLoss: 0.204167\n",
      "Train Epoch: 3 [4400/27000 (88%)]\tLoss: 0.261603\n",
      "Train Epoch: 3 [4800/27000 (96%)]\tLoss: 0.027530\n",
      "\n",
      "Test set: Average loss: 0.1217, Accuracy: 3587/4500 (79.71%)\n",
      "\n",
      "Train Epoch: 4 [0/27000 (0%)]\tLoss: 0.879114\n",
      "Train Epoch: 4 [400/27000 (8%)]\tLoss: 0.001893\n",
      "Train Epoch: 4 [800/27000 (16%)]\tLoss: 0.106460\n",
      "Train Epoch: 4 [1200/27000 (24%)]\tLoss: 0.011636\n",
      "Train Epoch: 4 [1600/27000 (32%)]\tLoss: 1.520973\n",
      "Train Epoch: 4 [2000/27000 (40%)]\tLoss: 0.665729\n",
      "Train Epoch: 4 [2400/27000 (48%)]\tLoss: 1.344911\n",
      "Train Epoch: 4 [2800/27000 (56%)]\tLoss: 1.243865\n",
      "Train Epoch: 4 [3200/27000 (64%)]\tLoss: 0.020265\n",
      "Train Epoch: 4 [3600/27000 (72%)]\tLoss: 0.000725\n",
      "Train Epoch: 4 [4000/27000 (80%)]\tLoss: 1.450592\n",
      "Train Epoch: 4 [4400/27000 (88%)]\tLoss: 0.000006\n",
      "Train Epoch: 4 [4800/27000 (96%)]\tLoss: 2.439279\n",
      "\n",
      "Test set: Average loss: 0.0767, Accuracy: 3837/4500 (85.27%)\n",
      "\n",
      "Train Epoch: 5 [0/27000 (0%)]\tLoss: 0.242553\n",
      "Train Epoch: 5 [400/27000 (8%)]\tLoss: 0.014640\n",
      "Train Epoch: 5 [800/27000 (16%)]\tLoss: 0.576028\n",
      "Train Epoch: 5 [1200/27000 (24%)]\tLoss: 0.007775\n",
      "Train Epoch: 5 [1600/27000 (32%)]\tLoss: 1.674707\n",
      "Train Epoch: 5 [2000/27000 (40%)]\tLoss: 0.012038\n",
      "Train Epoch: 5 [2400/27000 (48%)]\tLoss: 0.037395\n",
      "Train Epoch: 5 [2800/27000 (56%)]\tLoss: 5.155097\n",
      "Train Epoch: 5 [3200/27000 (64%)]\tLoss: 0.000005\n",
      "Train Epoch: 5 [3600/27000 (72%)]\tLoss: 0.036639\n",
      "Train Epoch: 5 [4000/27000 (80%)]\tLoss: 0.007926\n",
      "Train Epoch: 5 [4400/27000 (88%)]\tLoss: 0.248884\n",
      "Train Epoch: 5 [4800/27000 (96%)]\tLoss: 0.972824\n",
      "\n",
      "Test set: Average loss: 0.1067, Accuracy: 3665/4500 (81.44%)\n",
      "\n",
      "Train Epoch: 6 [0/27000 (0%)]\tLoss: 0.504128\n",
      "Train Epoch: 6 [400/27000 (8%)]\tLoss: 0.026947\n",
      "Train Epoch: 6 [800/27000 (16%)]\tLoss: 3.220994\n",
      "Train Epoch: 6 [1200/27000 (24%)]\tLoss: 1.105717\n",
      "Train Epoch: 6 [1600/27000 (32%)]\tLoss: 0.047199\n",
      "Train Epoch: 6 [2000/27000 (40%)]\tLoss: 0.428890\n",
      "Train Epoch: 6 [2400/27000 (48%)]\tLoss: 0.381290\n",
      "Train Epoch: 6 [2800/27000 (56%)]\tLoss: 1.310045\n",
      "Train Epoch: 6 [3200/27000 (64%)]\tLoss: 1.862848\n",
      "Train Epoch: 6 [3600/27000 (72%)]\tLoss: 0.125545\n",
      "Train Epoch: 6 [4000/27000 (80%)]\tLoss: 0.008036\n",
      "Train Epoch: 6 [4400/27000 (88%)]\tLoss: 0.910262\n",
      "Train Epoch: 6 [4800/27000 (96%)]\tLoss: 0.096578\n",
      "\n",
      "Test set: Average loss: 0.0770, Accuracy: 3843/4500 (85.40%)\n",
      "\n",
      "Train Epoch: 7 [0/27000 (0%)]\tLoss: 0.015468\n",
      "Train Epoch: 7 [400/27000 (8%)]\tLoss: 0.101597\n",
      "Train Epoch: 7 [800/27000 (16%)]\tLoss: 0.139078\n",
      "Train Epoch: 7 [1200/27000 (24%)]\tLoss: 0.192461\n",
      "Train Epoch: 7 [1600/27000 (32%)]\tLoss: 0.033469\n",
      "Train Epoch: 7 [2000/27000 (40%)]\tLoss: 0.000067\n",
      "Train Epoch: 7 [2400/27000 (48%)]\tLoss: 4.055867\n",
      "Train Epoch: 7 [2800/27000 (56%)]\tLoss: 1.469273\n",
      "Train Epoch: 7 [3200/27000 (64%)]\tLoss: 0.437048\n",
      "Train Epoch: 7 [3600/27000 (72%)]\tLoss: 0.000022\n",
      "Train Epoch: 7 [4000/27000 (80%)]\tLoss: 0.000408\n",
      "Train Epoch: 7 [4400/27000 (88%)]\tLoss: 0.068584\n",
      "Train Epoch: 7 [4800/27000 (96%)]\tLoss: 0.408766\n",
      "\n",
      "Test set: Average loss: 0.0810, Accuracy: 3843/4500 (85.40%)\n",
      "\n",
      "Train Epoch: 8 [0/27000 (0%)]\tLoss: 0.000377\n",
      "Train Epoch: 8 [400/27000 (8%)]\tLoss: 0.210175\n",
      "Train Epoch: 8 [800/27000 (16%)]\tLoss: 0.378528\n",
      "Train Epoch: 8 [1200/27000 (24%)]\tLoss: 0.027319\n",
      "Train Epoch: 8 [1600/27000 (32%)]\tLoss: 0.278610\n",
      "Train Epoch: 8 [2000/27000 (40%)]\tLoss: 1.102844\n",
      "Train Epoch: 8 [2400/27000 (48%)]\tLoss: 0.000326\n",
      "Train Epoch: 8 [2800/27000 (56%)]\tLoss: 1.760651\n",
      "Train Epoch: 8 [3200/27000 (64%)]\tLoss: 0.014589\n",
      "Train Epoch: 8 [3600/27000 (72%)]\tLoss: 0.887333\n",
      "Train Epoch: 8 [4000/27000 (80%)]\tLoss: 0.000044\n",
      "Train Epoch: 8 [4400/27000 (88%)]\tLoss: 0.088921\n",
      "Train Epoch: 8 [4800/27000 (96%)]\tLoss: 0.002291\n",
      "\n",
      "Test set: Average loss: 0.1001, Accuracy: 3779/4500 (83.98%)\n",
      "\n",
      "Train Epoch: 9 [0/27000 (0%)]\tLoss: 1.177853\n",
      "Train Epoch: 9 [400/27000 (8%)]\tLoss: 0.000023\n",
      "Train Epoch: 9 [800/27000 (16%)]\tLoss: 0.956221\n",
      "Train Epoch: 9 [1200/27000 (24%)]\tLoss: 0.001423\n",
      "Train Epoch: 9 [1600/27000 (32%)]\tLoss: 0.020262\n",
      "Train Epoch: 9 [2000/27000 (40%)]\tLoss: 0.863282\n",
      "Train Epoch: 9 [2400/27000 (48%)]\tLoss: 0.014485\n",
      "Train Epoch: 9 [2800/27000 (56%)]\tLoss: 0.008785\n",
      "Train Epoch: 9 [3200/27000 (64%)]\tLoss: 0.388409\n",
      "Train Epoch: 9 [3600/27000 (72%)]\tLoss: 0.337745\n",
      "Train Epoch: 9 [4000/27000 (80%)]\tLoss: 2.350664\n",
      "Train Epoch: 9 [4400/27000 (88%)]\tLoss: 0.438390\n",
      "Train Epoch: 9 [4800/27000 (96%)]\tLoss: 0.006595\n",
      "\n",
      "Test set: Average loss: 0.0804, Accuracy: 3875/4500 (86.11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "model = vit_ft(freeze=True, lora=False, rank=4)\n",
    "print([p[0] for p in list(model.named_parameters())][-10:])\n",
    "parameters = list(model.parameters())\n",
    "trainable_parameters = list(get_params_by_name(model, name_filter=lora_or_head))\n",
    "print(\n",
    "    f\"The model will start training with only {sum([int(torch.numel(p)) for p in trainable_parameters])} \"\n",
    "    f\"trainable parameters out of {sum([int(torch.numel(p)) for p in parameters])}.\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    trainable_parameters,\n",
    "    lr=1e-3,\n",
    ")\n",
    "\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "for epoch in range(1, 10):\n",
    "    train(model, criterion, device, train_loader, optimizer, epoch)\n",
    "    test(model, criterion, optimizer, device, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3299f9-8886-439e-9ab6-c1478dde56d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
