{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebba51df-e9c9-461c-b055-977f16969f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "INF = 100000000\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from dl_toolbox.callbacks import ProgressBar\n",
    "import gc \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from dl_toolbox.callbacks import ProgressBar\n",
    "from dl_toolbox import datamodules\n",
    "from dl_toolbox import modules\n",
    "import torchvision.transforms.v2 as v2\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from dl_toolbox.callbacks import ProgressBar, Finetuning, Lora, TiffPredsWriter, CalibrationLogger\n",
    "from functools import partial\n",
    "import gc\n",
    "\n",
    "\n",
    "train_tf = v2.Compose(\n",
    "    [\n",
    "        v2.Resize(size=480, max_size=640),\n",
    "        v2.RandomCrop(size=(640,640), pad_if_needed=True, fill=0),\n",
    "        v2.SanitizeBoundingBoxes(),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_tf = v2.Compose(\n",
    "    [\n",
    "        v2.Resize(size=480, max_size=640),\n",
    "        v2.RandomCrop(size=(640,640), pad_if_needed=True, fill=0),\n",
    "        v2.SanitizeBoundingBoxes(),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "dm = datamodules.Coco(\n",
    "    data_path='/data',\n",
    "    train_tf=train_tf,\n",
    "    test_tf=test_tf,\n",
    "    batch_tf=None,\n",
    "    batch_size=2,\n",
    "    num_workers=1,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "#dm = datamodules.xView(\n",
    "#    data_path='/data',\n",
    "#    merge='building',\n",
    "#    train_tf=train_tf,\n",
    "#    test_tf=test_tf,\n",
    "#    batch_tf=None,\n",
    "#    batch_size=2,\n",
    "#    num_workers=1,\n",
    "#    pin_memory=False\n",
    "#)\n",
    "\n",
    "lora = Lora('backbone', 4, True)\n",
    "\n",
    "num_classes = dm.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d849dbcd-3e08-4707-8e6d-ec92f02b0bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_fm_anchors(h, w, s):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        h, w: height, width of the feat map\n",
    "        s: stride of the featmap = size reduction factor relative to image\n",
    "    Returns:\n",
    "        Tensor NumAnchorsInFeatMap x 2, ordered by column (TODO: check why)\n",
    "    \"\"\"\n",
    "    locs_x = [s / 2 + x * s for x in range(w)]\n",
    "    locs_y = [s / 2 + y * s for y in range(h)]\n",
    "    locs = [(y, x) for x in locs_x for y in locs_y] # order !\n",
    "    return torch.tensor(locs)\n",
    "\n",
    "# test\n",
    "anchors = get_fm_anchors(14, 16, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7988be38-42ca-4dca-bfbd-908d679e75a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_anchors_bb_sizes(fm_sizes, fm_strides, bb_sizes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fm_sizes: seq of feature_maps sizes\n",
    "        fm_strides: seq of corresponding strides\n",
    "        bb_sizes: seq of bbox sizes feature maps are associated with, len = len(fm) + 1\n",
    "    Returns:\n",
    "        anchors: list of num_featmaps elem, where each elem indicates the tensor of anchors of size Nx2 in the original image corresponding to each location in the feature map at this level\n",
    "        anchors_bb_sizes: sizes of the bbox each anchor is authorized/supposed to detect\n",
    "    \"\"\"\n",
    "    anchors, anchors_bb_sizes = [], []\n",
    "    for l, ((h,w), s) in enumerate(zip(fm_sizes, fm_strides)):\n",
    "        fm_anchors = get_fm_anchors(h, w, s)\n",
    "        sizes = torch.tensor([bb_sizes[l], bb_sizes[l+1]], dtype=torch.float32)\n",
    "        sizes = sizes.repeat(len(fm_anchors)).view(len(fm_anchors), 2)\n",
    "        anchors.append(fm_anchors)\n",
    "        anchors_bb_sizes.append(sizes)\n",
    "    return torch.cat(anchors, 0), torch.cat(anchors_bb_sizes, 0)\n",
    "#test\n",
    "all_anchors = get_all_anchors_bb_sizes([(4,4),(2,2)], [8, 16], [-1, 64, 128])\n",
    "#all_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b92ff0f-d90b-4267-9022-996e39f2efca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _calculate_reg_targets(anchors, bbox):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        anchors: Lx2, anchors coordinates\n",
    "        bbox: tensor of bbox Tx4, format should be xywh\n",
    "    Returns:\n",
    "        reg_tgt: l,t,r,b values to regress for each pair (anchor, bbox)\n",
    "        anchor_in_box: whether anchor is in bbox for each pair (anchor, bbox)\n",
    "    \"\"\"\n",
    "    xs, ys = anchors[:, 0], anchors[:, 1] # L & L, x & y reversed ??\n",
    "    bbox[:, 2] += bbox[:, 0]\n",
    "    bbox[:, 3] += bbox[:, 1]\n",
    "    l = xs[:, None] - bbox[:, 0][None] # Lx1 - 1xT -> LxT\n",
    "    t = ys[:, None] - bbox[:, 1][None]\n",
    "    r = bbox[:, 2][None] - xs[:, None]\n",
    "    b = bbox[:, 3][None] - ys[:, None]\n",
    "    reg_tgt = torch.stack([l, t, r, b], dim=2) # LxTx4\n",
    "    anchor_in_box = reg_tgt.min(dim=2)[0] > 0 # LxT\n",
    "    return reg_tgt, anchor_in_box\n",
    "\n",
    "def _apply_distance_constraints(reg_targets, anchor_bb_sizes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        reg_targets: LxTx4\n",
    "        anchor_bb_sizes: Lx2\n",
    "    Returns:\n",
    "        A LxT tensor where value at (anchor, bbox) is true if the max value to regress at this anchor for this bbox is inside the bounds associated to this anchor\n",
    "        If other values to regress than the max are negatives, it is dealt with anchor_in_boxes.\n",
    "    \"\"\"\n",
    "    max_reg_targets, _ = reg_targets.max(dim=2)\n",
    "    return torch.logical_and(\n",
    "        max_reg_targets >= anchor_bb_sizes[:, None, 0],\n",
    "        max_reg_targets <= anchor_bb_sizes[:, None, 1]\n",
    "    )\n",
    "\n",
    "def anchor_bbox_area(bbox, anchors, is_in_boxes, fits_to_feature_level):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Returns: \n",
    "        Tensor LxT where value at (anchor, bbox) is the area of bbox if anchor is in bbox and anchor is associated with bbox of that size\n",
    "        Else INF.\n",
    "    \"\"\"\n",
    "    #bbox_areas = _calc_bbox_area(bbox_targets) # T\n",
    "    bbox_areas = torchvision.ops.box_area(bbox) # compared to above, does not deal with 0dim bb\n",
    "    # area of each target bbox repeated for each loc with inf where the the loc is not \n",
    "    # in the target bbox or if the loc is not at the right level for this bbox size\n",
    "    anchor_bbox_area = bbox_areas[None].repeat(len(anchors), 1) # LxT\n",
    "    anchor_bbox_area[is_in_boxes == 0] = INF\n",
    "    anchor_bbox_area[fits_to_feature_level == 0] = INF\n",
    "    return anchor_bbox_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08d3ed6-20a8-4ce8-a517-91dc0057811a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def associate_targets_to_anchors(targets_batch, anchors, anchors_bb_sizes):\n",
    "    \"\"\"\n",
    "    Associate one target cls/bbox to regress ONLY to each anchor: among the bboxes that contain the anchor and have the right size, pick that of min area.\n",
    "    If no tgt exists for an anchor, the tgt class is 0.\n",
    "    inputs:\n",
    "        targets_batch: list of dict of tv_tensors {'labels':, 'boxes':}; boxes should be in XYWH format\n",
    "        anchors: \n",
    "        anchor_bb_sizes:\n",
    "    outputs:\n",
    "        all class targets: BxNumAnchors\n",
    "        all bbox targets: BxNumAnchorsx4\n",
    "    \"\"\"\n",
    "    all_reg_targets, all_cls_targets = [], []\n",
    "    for targets in targets_batch:\n",
    "        bbox_targets = targets['boxes'] # Tx4, format XYWH\n",
    "        cls_targets = targets['labels'] # T\n",
    "        reg_targets, anchor_in_box = _calculate_reg_targets(\n",
    "            anchors, bbox_targets) # LxTx4, LxT\n",
    "        fits_to_feature_level = _apply_distance_constraints(\n",
    "            reg_targets, anchors_bb_sizes) # LxT\n",
    "        locations_to_gt_area = anchor_bbox_area(\n",
    "            bbox_targets, anchors, anchor_in_box, fits_to_feature_level)\n",
    "        # Core of the anchor/target association\n",
    "        if cls_targets.shape[0]>0:\n",
    "            loc_min_area, loc_min_idxs = locations_to_gt_area.min(dim=1) #L,idx in [0,T-1],T must be>0\n",
    "            reg_targets = reg_targets[range(len(anchors)), loc_min_idxs] # Lx4\n",
    "            cls_targets = cls_targets[loc_min_idxs] # L\n",
    "            cls_targets[loc_min_area == INF] = 0 # 0 is no-obj category\n",
    "        else:\n",
    "            cls_targets = cls_targets.new_zeros((len(anchors),))\n",
    "            reg_targets = reg_targets.new_zeros((len(anchors),4))\n",
    "        all_cls_targets.append(cls_targets)\n",
    "        all_reg_targets.append(reg_targets)\n",
    "    # BxL & BxLx4\n",
    "    return torch.stack(all_cls_targets), torch.stack(all_reg_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab3ffc0a-4079-4873-8194-08f7f3edbee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _compute_centerness_targets(reg_tgts):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        reg_tgts: l, t, r, b values to regress, shape BxNumAx4\n",
    "    Returns:\n",
    "        A tensor of shape BxNumA giving how centered each anchor is for the bbox it must regress\n",
    "    \"\"\"\n",
    "    left_right = reg_tgts[..., [0, 2]]\n",
    "    top_bottom = reg_tgts[..., [1, 3]]\n",
    "    centerness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\\n",
    "                (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0])\n",
    "    return torch.sqrt(centerness)\n",
    "\n",
    "class LossEvaluator(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(LossEvaluator, self).__init__()\n",
    "        self.centerness_loss_func = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def __call__(self, cls_logits, reg_preds, cness_preds, cls_tgts, reg_tgts):\n",
    "        pos_inds_b, pos_inds_loc = torch.nonzero(cls_tgts > 0, as_tuple=True)\n",
    "        num_pos = len(pos_inds_b)\n",
    "        reg_preds = reg_preds[pos_inds_b, pos_inds_loc, :]\n",
    "        reg_tgts = reg_tgts[pos_inds_b, pos_inds_loc, :]\n",
    "        cness_preds = cness_preds[pos_inds_b, pos_inds_loc, :].squeeze(-1)\n",
    "        cness_tgts = _compute_centerness_targets(reg_tgts)\n",
    "        cls_loss = self._get_cls_loss(cls_logits, cls_tgts, max(num_pos, 1.))\n",
    "        reg_loss, centerness_loss = 0,0\n",
    "        if num_pos > 0:\n",
    "            reg_loss = self._get_reg_loss(\n",
    "                reg_preds, reg_tgts, cness_tgts)\n",
    "            centerness_loss = self._get_centerness_loss(\n",
    "                cness_preds, cness_tgts, num_pos)\n",
    "        losses = {}\n",
    "        losses[\"cls_loss\"] = cls_loss\n",
    "        losses[\"reg_loss\"] = reg_loss\n",
    "        losses[\"centerness_loss\"] = centerness_loss\n",
    "        losses[\"combined_loss\"] = cls_loss + reg_loss + centerness_loss\n",
    "        return losses\n",
    "\n",
    "    def _get_cls_loss(self, cls_preds, cls_targets, num_pos_samples):\n",
    "        \"\"\"\n",
    "        cls_targets takes values in 0...C, 0 only when there is no obj to be detected for the anchor\n",
    "        \"\"\"\n",
    "        onehot = F.one_hot(cls_targets.long(), self.num_classes+1)[...,1:].float()\n",
    "        cls_loss = torchvision.ops.sigmoid_focal_loss(cls_preds, onehot)\n",
    "        return cls_loss.sum() / num_pos_samples\n",
    "\n",
    "    def _get_reg_loss(self, reg_preds, reg_targets, centerness_targets):\n",
    "        reg_preds = reg_preds.reshape(-1, 4)\n",
    "        reg_targets = reg_targets.reshape(-1, 4)\n",
    "        reg_losses = torchvision.ops.distance_box_iou_loss(reg_preds, reg_targets, reduction='none')\n",
    "        sum_centerness_targets = centerness_targets.sum()\n",
    "        reg_loss = (reg_losses * centerness_targets).sum() / sum_centerness_targets\n",
    "        return reg_loss\n",
    "\n",
    "    def _get_centerness_loss(self, centerness_preds, centerness_targets,\n",
    "                             num_pos_samples):\n",
    "        centerness_loss = self.centerness_loss_func(centerness_preds,\n",
    "                                                    centerness_targets)\n",
    "        return centerness_loss / num_pos_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d83a784-19cd-417b-b26c-d81bdf817fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from post_processor import *\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelP6P7\n",
    "from resnet_fcos import Head\n",
    "\n",
    "class FCOS(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        out_channels,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        tta=None,\n",
    "        sliding=None,\n",
    "        pre_nms_thresh=0.3,\n",
    "        pre_nms_top_n=1000,\n",
    "        nms_thresh=0.45,\n",
    "        fpn_post_nms_top_n=50,\n",
    "        min_size=0,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.map_metric = MeanAveragePrecision(\n",
    "            box_format='xywh', # make sure your dataset outputs target in xywh format\n",
    "            backend='faster_coco_eval'\n",
    "        )\n",
    "        self.sliding = sliding\n",
    "        \n",
    "        self.backbone = create_feature_extractor(\n",
    "            resnet50(weights=ResNet50_Weights.IMAGENET1K_V2), \n",
    "            {\n",
    "                'layer2.3.relu_2': 'layer2', # 1/8th feat map\n",
    "                'layer3.5.relu_2': 'layer3', # 1/16\n",
    "                'layer4.2.relu_2': 'layer4', # 1/32\n",
    "            }\n",
    "        )\n",
    "        inp = torch.randn(2, 3, 224, 224)\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(inp)\n",
    "        in_channels_list = [o.shape[1] for o in out.values()]\n",
    "        fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list,\n",
    "            out_channels=out_channels,\n",
    "            extra_blocks=LastLevelP6P7(out_channels,out_channels)\n",
    "        )\n",
    "        self.features = nn.Sequential(self.backbone, fpn)\n",
    "        inp = torch.randn(2, 3, 640, 640)\n",
    "        with torch.no_grad():\n",
    "            out = self.features(inp)\n",
    "        fm_sizes = [o.shape[2:] for o in out.values()]\n",
    "        self.head = Head(out_channels, num_classes)\n",
    "        \n",
    "        fm_strides = [8, 16, 32, 64, 128] \n",
    "        bb_sizes = [-1, 64, 128, 256, 512, INF] \n",
    "        anchors, anchor_sizes = get_all_anchors_bb_sizes(\n",
    "            fm_sizes, fm_strides, bb_sizes)\n",
    "        self.register_buffer('anchors', anchors) # Lx2\n",
    "        self.register_buffer('anchor_sizes', anchor_sizes) # Lx2\n",
    "        self.loss = LossEvaluator(num_classes)\n",
    "        self.pre_nms_thresh = pre_nms_thresh\n",
    "        self.pre_nms_top_n = pre_nms_top_n\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.fpn_post_nms_top_n = fpn_post_nms_top_n\n",
    "        self.min_size = min_size\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        train_params = list(filter(lambda p: p[1].requires_grad, self.named_parameters()))\n",
    "        nb_train = sum([int(torch.numel(p[1])) for p in train_params])\n",
    "        nb_tot = sum([int(torch.numel(p)) for p in self.parameters()])\n",
    "        print(f\"Training {nb_train} params out of {nb_tot}\")\n",
    "        optimizer = self.optimizer(params=[p[1] for p in train_params])\n",
    "        scheduler = self.scheduler(optimizer)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\"\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = list(self.features(x).values()) # feature maps from FPN\n",
    "        box_cls, box_regression, centerness = self.head(features)\n",
    "        return box_cls, box_regression, centerness\n",
    "        #return self.network(x)\n",
    "    \n",
    "    def _post_process(self, logits, ltrb, cness, image_size):\n",
    "        probas = logits.sigmoid() # LxC\n",
    "        high_probas = probas > self.pre_nms_thresh # LxC\n",
    "        # Indices on L and C axis of high prob pairs anchor/class\n",
    "        high_prob_anchors_idx, high_prob_cls = high_probas.nonzero(as_tuple=True) # dim l <= L*C\n",
    "        high_prob_cls += 1 # 0 is for no object\n",
    "        high_prob_ltrb = ltrb[high_prob_anchors_idx] # lx4\n",
    "        high_prob_anchors = self.anchors[high_prob_anchors_idx] # lx2\n",
    "        # Tensor shape l with values from logits*cness such that logits > pre_nms_thresh \n",
    "        cness_modulated_probas = probas * cness.sigmoid() # LxC\n",
    "        high_prob_scores = cness_modulated_probas[high_probas] # l\n",
    "        # si l est trop longue\n",
    "        if high_probas.sum().item() > self.pre_nms_top_n:\n",
    "            # Filter the pre_nms_top_n most probable pairs \n",
    "            high_prob_scores, top_k_indices = high_prob_scores.topk(\n",
    "                self.pre_nms_top_n, sorted=False) \n",
    "            high_prob_cls = high_prob_cls[top_k_indices]\n",
    "            high_prob_ltrb = high_prob_ltrb[top_k_indices]\n",
    "            high_prob_anchors = high_prob_anchors[top_k_indices]\n",
    "            \n",
    "        # Rewrites bbox (x0,y0,x1,y1) from reg targets (l,t,r,b) following eq (1) in paper\n",
    "        boxes = torch.stack([\n",
    "            high_prob_anchors[:, 0] - high_prob_ltrb[:, 0],\n",
    "            high_prob_anchors[:, 1] - high_prob_ltrb[:, 1],\n",
    "            high_prob_anchors[:, 0] + high_prob_ltrb[:, 2],\n",
    "            high_prob_anchors[:, 1] + high_prob_ltrb[:, 3],\n",
    "        ], dim=1)\n",
    "        \n",
    "        boxes = torchvision.ops.clip_boxes_to_image(boxes, (image_size, image_size))\n",
    "        big_enough_box_idxs = torchvision.ops.remove_small_boxes(boxes, self.min_size)\n",
    "        # Why not do that on scores and classes too ? DONE\n",
    "        boxes = boxes[big_enough_box_idxs]\n",
    "        scores = high_prob_scores[big_enough_box_idxs]\n",
    "        classes = high_prob_cls[big_enough_box_idxs]\n",
    "        #scores = torch.sqrt(high_prob_scores) # WHY SQRT ? REmOVED\n",
    "        # NMS expects boxes to be in xyxy format\n",
    "        nms_idxs = torchvision.ops.nms(boxes, scores, self.nms_thresh)\n",
    "        # Then back to xywh boxes for preds and metric computation\n",
    "        boxes[:, 2] += boxes[:, 0]\n",
    "        boxes[:, 3] += boxes[:, 1]\n",
    "        boxes = boxes[nms_idxs]\n",
    "        scores = scores[nms_idxs]\n",
    "        classes = classes[nms_idxs]\n",
    "\n",
    "        if len(nms_idxs) > self.fpn_post_nms_top_n:\n",
    "            image_thresh, _ = torch.kthvalue(\n",
    "                scores.cpu(),\n",
    "                len(nms_idxs) - self.fpn_post_nms_top_n + 1)\n",
    "            keep = scores >= image_thresh.item()\n",
    "            #keep = torch.nonzero(keep).squeeze(1)\n",
    "            boxes, scores, classes = boxes[keep], scores[keep], classes[keep]\n",
    "\n",
    "        keep = scores >= self.pre_nms_thresh\n",
    "        boxes, scores, classes = boxes[keep], scores[keep], classes[keep]\n",
    "        return boxes, scores, classes \n",
    "    \n",
    "    def post_process(\n",
    "        self,\n",
    "        cls_preds, # B x L x C \n",
    "        reg_preds, # B x L x 4\n",
    "        cness_preds, # B x L x 1\n",
    "        image_size\n",
    "    ): \n",
    "        B, L, C = cls_preds.shape\n",
    "        all_boxes = []\n",
    "        all_classes = []\n",
    "        all_scores = []\n",
    "        for i in range(B):\n",
    "            logits = cls_preds[i]\n",
    "            ltrb = reg_preds[i]\n",
    "            cness = cness_preds[i]\n",
    "            boxes, scores, classes = self._post_process(logits, ltrb, cness, image_size)\n",
    "            all_boxes.append(boxes)\n",
    "            all_classes.append(classes)\n",
    "            all_scores.append(scores)\n",
    "        predictions = [{'boxes': bb, 'scores': s, 'labels': l} for bb,s,l in \n",
    "                       zip(all_boxes, all_scores, all_classes)]\n",
    "        return predictions\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, targets, paths = batch[\"sup\"] #targets is a list of dict\n",
    "        cls_logits, bbox_reg, centerness = self.forward(x) # BxNumAnchorsxC, BxNumAnchorsx4, BxNumx1\n",
    "        cls_tgts, reg_tgts = associate_targets_to_anchors(\n",
    "            targets, self.anchors, self.anchor_sizes) # BxNumAnchors, BxNumAnchorsx4\n",
    "        losses = self.loss(cls_logits, bbox_reg, centerness, cls_tgts, reg_tgts)\n",
    "        train_loss = losses[\"combined_loss\"]\n",
    "        self.log(f\"loss/train\", train_loss.detach().item())\n",
    "        self.train_losses.append(train_loss.detach().item())\n",
    "        #preds = self.post_process(cls_logits, bbox_reg, centerness, x.shape[-1])\n",
    "        #self.map_metric.update(preds, targets)\n",
    "        return train_loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, targets, paths = batch\n",
    "        cls_logits, bbox_reg, centerness = self.forward(x) # BxNumAnchorsxC, BxNumAnchorsx4, BxNumx1\n",
    "        cls_tgts, reg_tgts = associate_targets_to_anchors(\n",
    "            targets, self.anchors, self.anchor_sizes) # BxNumAnchors, BxNumAnchorsx4\n",
    "        losses = self.loss(cls_logits, bbox_reg, centerness, cls_tgts, reg_tgts)\n",
    "        val_loss = losses[\"combined_loss\"]\n",
    "        self.log(f\"Total loss/val\", val_loss.detach().item())\n",
    "        preds = self.post_process(cls_logits, bbox_reg, centerness, x.shape[-1])\n",
    "        self.map_metric.update(preds, targets)\n",
    "        self.val_losses.append(val_loss.detach().item())\n",
    "        \n",
    "    def on_train_epoch_end(self):\n",
    "        train_loss = sum(self.train_losses)/len(self.train_losses)\n",
    "        print(f'\\n{train_loss=}')\n",
    "        self.train_losses.clear()\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        mapmetric = self.map_metric.compute()['map']\n",
    "        self.log(\"map/val\", mapmetric)\n",
    "        #print(\"\\nMAP: \", mapmetric)\n",
    "        self.map_metric.reset()\n",
    "        val_loss = sum(self.val_losses)/len(self.val_losses)\n",
    "        print(f'\\n{val_loss=}')\n",
    "        self.val_losses.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "862a98aa-ba57-41d5-a435-26f4b0bac896",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_steps=100,\n",
    "    limit_train_batches=5,\n",
    "    limit_val_batches=5,\n",
    "    callbacks=[ProgressBar(), lora]\n",
    ")\n",
    "\n",
    "module = FCOS(\n",
    "    num_classes=num_classes,\n",
    "    out_channels=256,\n",
    "    optimizer=partial(torch.optim.SGD, lr=0.01, momentum=0.9, weight_decay=0.0001),\n",
    "    scheduler=partial(torch.optim.lr_scheduler.ConstantLR, factor=1),\n",
    "    pre_nms_thresh=0.05,\n",
    "    pre_nms_top_n=1000,\n",
    "    nms_thresh=0.6,\n",
    "    fpn_post_nms_top_n=100,\n",
    "    min_size=0,\n",
    ")\n",
    " \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0d01e6b-5b98-4aae-a18b-a87aa7c0f8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.11s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.33s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type                 | Params\n",
      "----------------------------------------------------\n",
      "0 | map_metric | MeanAveragePrecision | 0     \n",
      "1 | backbone   | ResNet               | 23.5 M\n",
      "2 | features   | Sequential           | 27.4 M\n",
      "3 | head       | Head                 | 4.9 M \n",
      "4 | loss       | LossEvaluator        | 0     \n",
      "----------------------------------------------------\n",
      "8.9 M     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "32.3 M    Total params\n",
      "129.291   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n",
      "Training 8867813 params out of 32322725\n",
      "Sanity Checking: |                                                                                                                                                                      | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.82it/s, v_num=45]\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.97it/s, v_num=45]\u001b[A\n",
      "train_loss=3.759005403518677\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.63it/s, v_num=45]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.85it/s, v_num=45]\u001b[A\n",
      "train_loss=3.7556307315826416\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.83it/s, v_num=45]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.97it/s, v_num=45]\u001b[A\n",
      "train_loss=8.235127449035645\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.64it/s, v_num=45]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.94it/s, v_num=45]\u001b[A\n",
      "train_loss=11.08840446472168\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.67it/s, v_num=45]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.96it/s, v_num=45]\u001b[A\n",
      "train_loss=4.665365219116211\n",
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.78it/s, v_num=45]\n",
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.02it/s, v_num=45]\u001b[A\n",
      "train_loss=5.161081600189209\n",
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.74it/s, v_num=45]\n",
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.00it/s, v_num=45]\u001b[A\n",
      "train_loss=18.187313079833984\n",
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.81it/s, v_num=45]\n",
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.02it/s, v_num=45]\u001b[A\n",
      "train_loss=5.10589485168457\n",
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.53it/s, v_num=45]\n",
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.96it/s, v_num=45]\u001b[A\n",
      "train_loss=3.7315598011016844\n",
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.86it/s, v_num=45]\n",
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.89it/s, v_num=45]\u001b[A\n",
      "train_loss=5.545734548568726\n",
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.78it/s, v_num=45]\n",
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.03it/s, v_num=45]\u001b[A\n",
      "train_loss=3.1784475326538084\n",
      "Epoch 11: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.81it/s, v_num=45]\n",
      "Epoch 11: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.01it/s, v_num=45]\u001b[A\n",
      "train_loss=3.4187571525573732\n",
      "Epoch 12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.84it/s, v_num=45]\n",
      "Epoch 12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.04it/s, v_num=45]\u001b[A\n",
      "train_loss=3.3466522693634033\n",
      "Epoch 13: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.72it/s, v_num=45]\n",
      "Epoch 13: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.00it/s, v_num=45]\u001b[A\n",
      "train_loss=3.401997518539429\n",
      "Epoch 14: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.80it/s, v_num=45]\n",
      "Epoch 14: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.98it/s, v_num=45]\u001b[A\n",
      "train_loss=3.1669009208679197\n",
      "Epoch 15: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.75it/s, v_num=45]\n",
      "Epoch 15: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.02it/s, v_num=45]\u001b[A\n",
      "train_loss=3.4567564487457276\n",
      "Epoch 16: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.79it/s, v_num=45]\n",
      "Epoch 16: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.06it/s, v_num=45]\u001b[A\n",
      "train_loss=3.5682540416717528\n",
      "Epoch 17: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.69it/s, v_num=45]\n",
      "Epoch 17: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.95it/s, v_num=45]\u001b[A\n",
      "train_loss=3.0802280426025392\n",
      "Epoch 18: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.83it/s, v_num=45]\n",
      "Epoch 18: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.02it/s, v_num=45]\u001b[A\n",
      "train_loss=3.2486042022705077\n",
      "Epoch 19: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.67it/s, v_num=45]\n",
      "Epoch 19: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.82it/s, v_num=45]\u001b[A\n",
      "train_loss=3.036016893386841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.07it/s, v_num=45]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    module,\n",
    "    datamodule=dm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582c933-1b60-4c8a-9778-a215a1266861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
