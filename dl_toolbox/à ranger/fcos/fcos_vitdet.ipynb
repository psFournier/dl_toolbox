{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebba51df-e9c9-461c-b055-977f16969f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelP6P7\n",
    "#from torchvision.models.detection.backbone_utils import LastLevelP6P7\n",
    "from dl_toolbox.networks.fcos import Head\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "INF = 100000000\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e24601f-642b-4245-a582-42a6dc91a23e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def _compute_centerness_targets(reg_targets):\n",
    "    if len(reg_targets) == 0:\n",
    "        return reg_targets.new_zeros(len(reg_targets))\n",
    "    left_right = reg_targets[:, [0, 2]]\n",
    "    top_bottom = reg_targets[:, [1, 3]]\n",
    "    centerness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\\n",
    "                (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0])\n",
    "    return torch.sqrt(centerness)\n",
    "\n",
    "\n",
    "def _calculate_reg_targets(xs, ys, bbox_targets):\n",
    "    l = xs[:, None] - bbox_targets[:, 0][None] # Lx1 - 1xT -> LxT\n",
    "    t = ys[:, None] - bbox_targets[:, 1][None]\n",
    "    r = bbox_targets[:, 2][None] - xs[:, None]\n",
    "    b = bbox_targets[:, 3][None] - ys[:, None]\n",
    "    return torch.stack([l, t, r, b], dim=2) # LxTx4\n",
    "\n",
    "\n",
    "def _apply_distance_constraints(reg_targets, level_distances):\n",
    "    max_reg_targets, _ = reg_targets.max(dim=2)\n",
    "    return torch.logical_and(max_reg_targets >= level_distances[:, None, 0], \\\n",
    "                             max_reg_targets <= level_distances[:, None, 1])\n",
    "\n",
    "def _match_pred_format(cls_targets, reg_targets, locations):\n",
    "    cls_per_level = []\n",
    "    reg_per_level = []\n",
    "    for level in range(len(locations)):\n",
    "        cls_per_level.append(torch.cat([ct[level] for ct in cls_targets],\n",
    "                                       dim=0))\n",
    "\n",
    "        reg_per_level.append(torch.cat([rt[level] for rt in reg_targets],\n",
    "                                       dim=0))\n",
    "    # reg_per_level is a list of num_levels tensors of size Bxnum_loc_per_levelx4\n",
    "    return cls_per_level, reg_per_level\n",
    "\n",
    "\n",
    "def _get_positive_samples(cls_labels, reg_labels, box_cls_preds, box_reg_preds,\n",
    "                          centerness_preds, num_classes):\n",
    "    box_cls_flatten = []\n",
    "    box_regression_flatten = []\n",
    "    centerness_flatten = []\n",
    "    labels_flatten = []\n",
    "    reg_targets_flatten = []\n",
    "    for l in range(len(cls_labels)):\n",
    "        box_cls_flatten.append(box_cls_preds[l].permute(0, 2, 3, 1).reshape(\n",
    "            -1, num_classes))\n",
    "        box_regression_flatten.append(box_reg_preds[l].permute(0, 2, 3,\n",
    "                                                               1).reshape(\n",
    "                                                                   -1, 4))\n",
    "        labels_flatten.append(cls_labels[l].reshape(-1))\n",
    "        reg_targets_flatten.append(reg_labels[l].reshape(-1, 4))\n",
    "        centerness_flatten.append(centerness_preds[l].reshape(-1))\n",
    "\n",
    "    cls_preds = torch.cat(box_cls_flatten, dim=0)\n",
    "    cls_targets = torch.cat(labels_flatten, dim=0)\n",
    "    reg_preds = torch.cat(box_regression_flatten, dim=0)\n",
    "    reg_targets = torch.cat(reg_targets_flatten, dim=0)\n",
    "    centerness_preds = torch.cat(centerness_flatten, dim=0)\n",
    "    pos_inds = torch.nonzero(cls_targets > 0).squeeze(1) # dim #loc in all batches where there is one cls to pred not background\n",
    "\n",
    "    reg_preds = reg_preds[pos_inds]\n",
    "    reg_targets = reg_targets[pos_inds]\n",
    "    centerness_preds = centerness_preds[pos_inds]\n",
    "\n",
    "    return reg_preds, reg_targets, cls_preds, cls_targets, centerness_preds, pos_inds\n",
    "\n",
    "class LossEvaluator(nn.Module):\n",
    "\n",
    "    def __init__(self, locs_info, num_classes):\n",
    "        super(LossEvaluator, self).__init__()\n",
    "        locs_per_level, bb_sizes_per_level, num_locs_per_level = locs_info\n",
    "        self.centerness_loss_func = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "        self.register_buffer('locations', torch.cat(locs_per_level, dim=0))\n",
    "        self.register_buffer('bb_sizes', torch.cat(bb_sizes_per_level, dim=0))\n",
    "        self.num_locs_per_level = num_locs_per_level\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def _get_cls_loss(self, cls_preds, cls_targets, total_num_pos):\n",
    "        nc = cls_preds.shape[1]\n",
    "        onehot = F.one_hot(cls_targets.long(), nc+1)[:,1:].float()\n",
    "        cls_loss = torchvision.ops.sigmoid_focal_loss(cls_preds, onehot)\n",
    "        return cls_loss.sum() / total_num_pos\n",
    "\n",
    "    def _get_reg_loss(self, reg_preds, reg_targets, centerness_targets):\n",
    "        reg_preds = reg_preds.reshape(-1, 4)\n",
    "        reg_targets = reg_targets.reshape(-1, 4)\n",
    "        reg_losses = torchvision.ops.distance_box_iou_loss(reg_preds, reg_targets, reduction='none')\n",
    "        sum_centerness_targets = centerness_targets.sum()\n",
    "        reg_loss = (reg_losses * centerness_targets).sum() / sum_centerness_targets\n",
    "        return reg_loss\n",
    "\n",
    "    def _get_centerness_loss(self, centerness_preds, centerness_targets,\n",
    "                             total_num_pos):\n",
    "        centerness_loss = self.centerness_loss_func(centerness_preds,\n",
    "                                                    centerness_targets)\n",
    "        return centerness_loss / total_num_pos\n",
    "\n",
    "    def _evaluate_losses(self, reg_preds, cls_preds, centerness_preds,\n",
    "                         reg_targets, cls_targets, centerness_targets,\n",
    "                         pos_inds):\n",
    "        total_num_pos = max(pos_inds.new_tensor([pos_inds.numel()]), 1.0)\n",
    "\n",
    "        cls_loss = self._get_cls_loss(cls_preds, cls_targets, total_num_pos)\n",
    "\n",
    "        if pos_inds.numel() > 0:\n",
    "            reg_loss = self._get_reg_loss(reg_preds, reg_targets,\n",
    "                                          centerness_targets)\n",
    "            centerness_loss = self._get_centerness_loss(centerness_preds,\n",
    "                                                        centerness_targets,\n",
    "                                                        total_num_pos)\n",
    "        else:\n",
    "            reg_loss = reg_preds.sum() # 0 ??\n",
    "            centerness_loss = centerness_preds.sum() # 0 ??\n",
    "\n",
    "        return reg_loss, cls_loss, centerness_loss\n",
    "    \n",
    "    def _prepare_labels(self, targets_batch):\n",
    "        # nb of locs for bbox in original image size\n",
    "        # L = sum locs per level x 2 : for each loc in all_locs, the max size of bb authorized\n",
    "        xs, ys = self.locations[:, 0], self.locations[:, 1] # L & L\n",
    "        num_locs = sum(self.num_locs_per_level)\n",
    "\n",
    "        all_reg_targets = []\n",
    "        all_cls_targets = []\n",
    "        for targets in targets_batch:\n",
    "            \n",
    "            bbox_targets = targets['boxes'] # Tx4\n",
    "            cls_targets = targets['labels'] # T\n",
    "            num_targets = cls_targets.shape[0]\n",
    "\n",
    "            # for each loc in L and each target in T, the reg target\n",
    "            reg_targets = _calculate_reg_targets(xs, ys, bbox_targets) # LxTx4\n",
    "\n",
    "            is_in_boxes = reg_targets.min(dim=2)[0] > 0 # min returns values and indices -> LxT\n",
    "\n",
    "            fits_to_feature_level = _apply_distance_constraints(\n",
    "                reg_targets, self.bb_sizes) # LxT\n",
    "\n",
    "            #bbox_areas = _calc_bbox_area(bbox_targets) # T\n",
    "            bbox_areas = torchvision.ops.box_area(bbox_targets) # compared to above, does not deal with 0dim bb\n",
    "\n",
    "            # area of each target bbox repeated for each loc with inf where the the loc is not \n",
    "            # in the target bbox or if the loc is not at the right level for this bbox size\n",
    "            locations_to_gt_area = bbox_areas[None].repeat(len(self.locations), 1) # LxT\n",
    "            locations_to_gt_area[is_in_boxes == 0] = INF\n",
    "            locations_to_gt_area[fits_to_feature_level == 0] = INF\n",
    "\n",
    "            # for each loc, area and target idx of the target of min area at that loc\n",
    "            if num_targets>0:\n",
    "                loc_min_area, loc_mind_idxs = locations_to_gt_area.min(dim=1) # val&idx, size L, idx in [0,T-1]\n",
    "                reg_targets = reg_targets[range(len(self.locations)), loc_mind_idxs] # Lx4\n",
    "                cls_targets = cls_targets[loc_mind_idxs] # L\n",
    "                cls_targets[loc_min_area == INF] = 0\n",
    "            else:\n",
    "                cls_targets = cls_targets.new_zeros((num_locs,))\n",
    "                reg_targets = reg_targets.new_zeros((num_locs,4))\n",
    "\n",
    "            all_cls_targets.append(\n",
    "                torch.split(cls_targets, self.num_locs_per_level, dim=0))\n",
    "            all_reg_targets.append(\n",
    "                torch.split(reg_targets, self.num_locs_per_level, dim=0))\n",
    "        # all_cls_targets contains B lists of num levels elem of loc_per_levelsx1\n",
    "        locations = torch.split(self.locations, self.num_locs_per_level, dim=0)\n",
    "        return _match_pred_format(all_cls_targets, all_reg_targets, locations)\n",
    "\n",
    "    def __call__(self, out, targets_batch):\n",
    "        # reg_targets is a list of num_levels tensors of size Bxnum_loc_per_levelx4\n",
    "        cls_targets, reg_targets = self._prepare_labels(targets_batch)\n",
    "        box_cls, box_regression, centerness = out\n",
    "        reg_p, reg_t, cls_p, cls_t, centerness_p, pos_inds = _get_positive_samples(\n",
    "            cls_targets,\n",
    "            reg_targets,\n",
    "            box_cls,\n",
    "            box_regression,\n",
    "            centerness,\n",
    "            self.num_classes\n",
    "        )\n",
    "        centerness_t = _compute_centerness_targets(reg_t)\n",
    "        losses = {}\n",
    "        reg_loss, cls_loss, centerness_loss = self._evaluate_losses(\n",
    "            reg_p, cls_p, centerness_p, reg_t, cls_t, centerness_t, pos_inds)\n",
    "        losses[\"cls_loss\"] = cls_loss\n",
    "        losses[\"reg_loss\"] = reg_loss\n",
    "        losses[\"centerness_loss\"] = centerness_loss\n",
    "        losses[\"combined_loss\"] = cls_loss + reg_loss + centerness_loss\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a46c251-4920-4f5c-b466-dbd8bddcd187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FCOSPostProcessor(nn.Module):\n",
    "\n",
    "    def __init__(self, locs_info, pre_nms_thresh, pre_nms_top_n, nms_thresh,\n",
    "                 fpn_post_nms_top_n, min_size, num_classes):\n",
    "        super(FCOSPostProcessor, self).__init__()\n",
    "        self.pre_nms_thresh = pre_nms_thresh\n",
    "        self.pre_nms_top_n = pre_nms_top_n\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.fpn_post_nms_top_n = fpn_post_nms_top_n\n",
    "        self.min_size = min_size\n",
    "        self.num_classes = num_classes\n",
    "        locs_per_level, bb_sizes_per_level, num_locs_per_level = locs_info\n",
    "        self.register_buffer('locations', torch.cat(locs_per_level, dim=0))\n",
    "        self.num_locs_per_level = num_locs_per_level\n",
    "\n",
    "    def forward_for_single_feature_map(self, locations, cls_preds, reg_preds,\n",
    "                                       cness_preds, image_size):\n",
    "        B, C, _, _ = cls_preds.shape\n",
    "        cls_preds = cls_preds.permute(0, 2, 3, 1).reshape(B, -1, C).sigmoid() # BxHWxC in [0,1]\n",
    "        reg_preds = reg_preds.permute(0, 2, 3, 1).reshape(B, -1, 4)\n",
    "        cness_preds = cness_preds.permute(0, 2, 3, 1).reshape(B, -1).sigmoid()\n",
    "\n",
    "        candidate_inds = cls_preds > self.pre_nms_thresh # BxHWxC\n",
    "        pre_nms_top_n = candidate_inds.reshape(B, -1).sum(1) # B\n",
    "        pre_nms_top_n = pre_nms_top_n.clamp(max=self.pre_nms_top_n)\n",
    "\n",
    "        cls_preds = cls_preds * cness_preds[:, :, None] # BxHWxC\n",
    "        \n",
    "        # Conversion en liste de bbox,scores,cls par image du batch\n",
    "        # POURQUOI le filtre cls_preds > nms_thresh arrive pas après la mul par cness_preds ?\n",
    "        bboxes = []\n",
    "        cls_labels = []\n",
    "        scores = []\n",
    "        for i in range(B):\n",
    "            # Tensor with true where score for loc l and class c > pre_nms_thresh\n",
    "            per_candidate_inds = candidate_inds[i] # HWxC\n",
    "            # tenseur de taille Lx2 avec les indices des elem de cls_preds où > nms_thresh\n",
    "            per_candidate_nonzeros = per_candidate_inds.nonzero() \n",
    "            # L : positions dans [0,HW] des elem dont cls_preds(c) > nms_thresh \n",
    "            per_box_loc = per_candidate_nonzeros[:, 0]\n",
    "            # L : classe dans [1, C] des elem dont cls_preds(h,w) > nms_thresh\n",
    "            per_class = per_candidate_nonzeros[:, 1] + 1\n",
    "\n",
    "            per_reg_preds = reg_preds[i] # HWx4\n",
    "            # liste des bb des elem dont cls_preds(c) > nms_thresh \n",
    "            per_reg_preds = per_reg_preds[per_box_loc] # Lx4\n",
    "            per_locations = locations[per_box_loc] # Lx2\n",
    "\n",
    "            per_pre_nms_top_n = pre_nms_top_n[i]\n",
    "            \n",
    "            per_cls_preds = cls_preds[i] # HWxC\n",
    "            # tenseur de taille L avec les elem de cls_preds*centerness tels que cls_preds > nms_thresh\n",
    "            per_cls_preds = per_cls_preds[per_candidate_inds] \n",
    "            \n",
    "            # si y a plus de per_prenms_topn qui passe nms_thresh (si L est trop longue)\n",
    "            if per_candidate_inds.sum().item() > per_pre_nms_top_n.item():\n",
    "                per_cls_preds, top_k_indices = per_cls_preds.topk(\n",
    "                    per_pre_nms_top_n, sorted=False)\n",
    "                per_class = per_class[top_k_indices]\n",
    "                per_reg_preds = per_reg_preds[top_k_indices]\n",
    "                per_locations = per_locations[top_k_indices]\n",
    "            \n",
    "            # Rewrites bbox (x0,y0,x1,y1) from reg targets (l,t,r,b) following eq (1) in paper\n",
    "            per_bboxes = torch.stack([\n",
    "                per_locations[:, 0] - per_reg_preds[:, 0],\n",
    "                per_locations[:, 1] - per_reg_preds[:, 1],\n",
    "                per_locations[:, 0] + per_reg_preds[:, 2],\n",
    "                per_locations[:, 1] + per_reg_preds[:, 3],\n",
    "            ],\n",
    "                                     dim=1)\n",
    "            per_bboxes = torchvision.ops.clip_boxes_to_image(per_bboxes, (image_size, image_size))\n",
    "            #detections = _clip_to_image(detections, (image_size, image_size))\n",
    "            per_bboxes = per_bboxes[torchvision.ops.remove_small_boxes(per_bboxes, self.min_size)]\n",
    "            #detections = remove_small_boxes(detections, self.min_size)\n",
    "            bboxes.append(per_bboxes)\n",
    "            cls_labels.append(per_class)\n",
    "            scores.append(torch.sqrt(per_cls_preds))\n",
    "            \n",
    "        #bboxes is a list of B tensors of size Lx4 (potentially filtered with pre_nms_threshold)\n",
    "        return bboxes, scores, cls_labels\n",
    "\n",
    "    def forward(self, cls_preds, reg_preds, cness_preds, image_size):\n",
    "        # loc: list of n_feat_level tensors of size HW(level)\n",
    "        # reg_preds: list of n_feat_level tensors BxHW(level)x4\n",
    "        \n",
    "        # list of n_feat_level lists of B tensors of size Lx4\n",
    "        sampled_boxes = []\n",
    "        all_scores = []\n",
    "        all_classes = []\n",
    "        locations = torch.split(self.locations, self.num_locs_per_level, dim=0)\n",
    "        for l, o, b, c in list(zip(locations, cls_preds, reg_preds,\n",
    "                                   cness_preds)):\n",
    "            boxes, scores, cls_labels = self.forward_for_single_feature_map(\n",
    "                l, o, b, c, image_size)\n",
    "            # boxes : list of B tensors Lx4\n",
    "            sampled_boxes.append(boxes)\n",
    "            all_scores.append(scores)\n",
    "            all_classes.append(cls_labels)\n",
    "        \n",
    "        # list of B lists of n_feat_level bbox preds\n",
    "        all_bboxes = list(zip(*sampled_boxes))\n",
    "        all_scores = list(zip(*all_scores))\n",
    "        all_classes = list(zip(*all_classes))\n",
    "    \n",
    "        # list of B tensors with all feature level bbox preds grouped\n",
    "        all_bboxes = [torch.cat(bboxes, dim=0) for bboxes in all_bboxes]\n",
    "        all_scores = [torch.cat(scores, dim=0) for scores in all_scores]\n",
    "        all_classes = [torch.cat(classes, dim=0) for classes in all_classes]\n",
    "        boxes, scores, classes = self.select_over_all_levels(\n",
    "            all_bboxes, all_scores, all_classes)\n",
    "\n",
    "        return boxes, scores, classes\n",
    "\n",
    "    def select_over_all_levels(self, boxlists, scores, classes):\n",
    "        num_images = len(boxlists)\n",
    "        all_picked_boxes, all_confidence_scores, all_classes = [], [], []\n",
    "        for i in range(num_images):\n",
    "            picked_indices = torchvision.ops.nms(boxlists[i], scores[i], self.nms_thresh)\n",
    "            picked_boxes = boxlists[i][picked_indices]\n",
    "            confidence_scores = scores[i][picked_indices]\n",
    "            picked_classes = classes[i][picked_indices]\n",
    "\n",
    "            number_of_detections = len(picked_indices)\n",
    "            if number_of_detections > self.fpn_post_nms_top_n > 0:\n",
    "                image_thresh, _ = torch.kthvalue(\n",
    "                    confidence_scores.cpu(),\n",
    "                    number_of_detections - self.fpn_post_nms_top_n + 1)\n",
    "                keep = confidence_scores >= image_thresh.item()\n",
    "\n",
    "                keep = torch.nonzero(keep).squeeze(1)\n",
    "                picked_boxes, confidence_scores, picked_classes = picked_boxes[\n",
    "                    keep], confidence_scores[keep], picked_classes[keep]\n",
    "\n",
    "            keep = confidence_scores >= self.pre_nms_thresh\n",
    "            picked_boxes, confidence_scores, picked_classes = picked_boxes[\n",
    "                keep], confidence_scores[keep], picked_classes[keep]\n",
    "\n",
    "            all_picked_boxes.append(picked_boxes)\n",
    "            all_confidence_scores.append(confidence_scores)\n",
    "            all_classes.append(picked_classes)\n",
    "        \n",
    "        # all_picked_boxes : list of B tensors with all feature level bbox preds filtered by nms\n",
    "        return all_picked_boxes, all_confidence_scores, all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd41acc-4e80-43d8-9c45-34a3dd319154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "class FCOS(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        network,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        pred_thresh,\n",
    "        tta=None,\n",
    "        sliding=None,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.network = network\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.map_metric = MeanAveragePrecision(box_format='xywh', backend='faster_coco_eval')\n",
    "        self.sliding = sliding\n",
    "        self.pred_thresh = pred_thresh      \n",
    "\n",
    "        \n",
    "        fpn_strides = [8, 16, 32, 64, 128]\n",
    "        bb_sizes = [-1, 64, 128, 256, 512, INF]\n",
    "        # locations is a list of num_feat_level elem, where each elem indicates the tensor of \n",
    "        # locations in the original image corresponding to each location in the feature map at this level\n",
    "        anchors = self.get_anchors(network.feat_sizes, fpn_strides, bb_sizes)\n",
    "        self.loss = LossEvaluator(\n",
    "            anchors,\n",
    "            num_classes\n",
    "        )\n",
    "        self.post_processor = FCOSPostProcessor(\n",
    "            locs_info = anchors,\n",
    "            pre_nms_thresh=0.3,\n",
    "            pre_nms_top_n=1000,\n",
    "            nms_thresh=0.45,\n",
    "            fpn_post_nms_top_n=50,\n",
    "            min_size=0,\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        train_params = list(filter(lambda p: p[1].requires_grad, self.named_parameters()))\n",
    "        nb_train = sum([int(torch.numel(p[1])) for p in train_params])\n",
    "        nb_tot = sum([int(torch.numel(p)) for p in self.parameters()])\n",
    "        print(f\"Training {nb_train} params out of {nb_tot}\")\n",
    "        optimizer = self.optimizer(params=[p[1] for p in train_params])\n",
    "        scheduler = self.scheduler(optimizer)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_anchors(cls, feat_sizes, fpn_strides, bb_sizes):\n",
    "        anchors, anchor_sizes, num_anchors = [], [], []\n",
    "        def _locations_per_level(h, w, s):\n",
    "            locs_x = [i for i in range(w)]\n",
    "            locs_y = [i for i in range(h)]\n",
    "            locs_x = [s / 2 + x * s for x in locs_x]\n",
    "            locs_y = [s / 2 + y * s for y in locs_y]\n",
    "            locs = [(y, x) for x in locs_x for y in locs_y]\n",
    "            return torch.tensor(locs)\n",
    "        for l, (h,w) in enumerate(feat_sizes):\n",
    "            locs = _locations_per_level(h, w, fpn_strides[l])\n",
    "            sizes = torch.tensor([bb_sizes[l], bb_sizes[l+1]], dtype=torch.float32)\n",
    "            sizes = sizes.repeat(len(locs)).view(len(locs), 2)\n",
    "            anchors.append(locs)\n",
    "            anchor_sizes.append(sizes)\n",
    "            num_anchors.append(len(locs))\n",
    "        return anchors, anchor_sizes, num_anchors\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def post_process(self, out, images):\n",
    "        predicted_boxes, scores, all_classes = self.post_processor(*out, images.shape[-1])\n",
    "        preds = [{'boxes': bb, 'scores': s, 'labels': l} for bb,s,l in zip(\n",
    "            predicted_boxes, scores, all_classes\n",
    "        )]\n",
    "        return preds\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, targets, paths = batch[\"sup\"]\n",
    "        outputs = self.forward(x)\n",
    "        losses = self.loss(outputs, targets)\n",
    "        loss = losses[\"combined_loss\"]\n",
    "        self.log(f\"loss/train\", loss.detach().item())\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, targets, paths = batch\n",
    "        outputs = self.forward(x)\n",
    "        losses = self.loss(outputs, targets)\n",
    "        loss = losses[\"combined_loss\"]\n",
    "        self.log(f\"Total loss/val\", loss.detach().item())\n",
    "        preds = self.post_process(outputs, x)\n",
    "        self.map_metric.update(preds, targets)\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        mapmetric = self.map_metric.compute()['map']\n",
    "        self.log(\"map/val\", mapmetric)\n",
    "        print(\"\\nMAP: \", mapmetric)\n",
    "        self.map_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579a3cf1-98c3-4326-a186-40842ffd349d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from torchvision.ops.feature_pyramid_network import ExtraFPNBlock\n",
    "from torchvision.ops.misc import Conv2dNormActivation\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "from torch import nn, Tensor\n",
    "import timm\n",
    "from torchvision.ops.feature_pyramid_network import LastLevelP6P7\n",
    "from dl_toolbox.networks.fcos import Head\n",
    "import torch.nn as nn\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from einops import rearrange\n",
    "import torch\n",
    "\n",
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    \"\"\" LayerNorm for channels of '2D' spatial NCHW tensors \"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6, affine=True):\n",
    "        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.layer_norm(\n",
    "            x.permute(0, 2, 3, 1), self.normalized_shape, self.weight, self.bias, self.eps).permute(0, 3, 1, 2)\n",
    "\n",
    "class SimpleFeaturePyramidNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Module that adds a Simple FPN from on top of a set of feature maps. This is based on\n",
    "    `\"Exploring Plain Vision Transformer Backbones for Object Detection\" <https://arxiv.org/abs/2203.16527>`_.\n",
    "\n",
    "    Unlike regular FPN, Simple FPN expects a single feature map,\n",
    "    on which the Simple FPN will be added.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of channels for the input feature map that\n",
    "            is passed to the module\n",
    "        out_channels (int): number of channels of the Simple FPN representation\n",
    "        extra_blocks (ExtraFPNBlock or None): if provided, extra operations will\n",
    "            be performed. It is expected to take the fpn features, the original\n",
    "            features and the names of the original features as input, and returns\n",
    "            a new list of feature maps and their corresponding names\n",
    "        norm_layer (callable, optional): Module specifying the normalization layer to use. Default: LayerNorm\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = torchvision.ops.SimpleFeaturePyramidNetwork(10, 5)\n",
    "        >>> # get some dummy data\n",
    "        >>> x = torch.rand(1, 10, 64, 64)\n",
    "        >>> # compute the Simple FPN on top of x\n",
    "        >>> output = m(x)\n",
    "        >>> print([(k, v.shape) for k, v in output.items()])\n",
    "        >>> # returns\n",
    "        >>>   [('feat0', torch.Size([1, 5, 64, 64])),\n",
    "        >>>    ('feat2', torch.Size([1, 5, 16, 16])),\n",
    "        >>>    ('feat3', torch.Size([1, 5, 8, 8]))]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        extra_blocks: Optional[ExtraFPNBlock] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for block_index in range(0,4):\n",
    "            layers = []\n",
    "            current_in_channels = in_channels\n",
    "            if block_index == 0:\n",
    "                layers.extend([\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels,\n",
    "                        in_channels // 2,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                    norm_layer(in_channels // 2),\n",
    "                    nn.GELU(),\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels // 2,\n",
    "                        in_channels // 4,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                ])\n",
    "                current_in_channels = in_channels // 4\n",
    "            elif block_index == 1:\n",
    "                layers.append(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels,\n",
    "                        in_channels // 2,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                )\n",
    "                current_in_channels = in_channels // 2\n",
    "            elif block_index == 2:\n",
    "                # nothing to do for this scale\n",
    "                pass\n",
    "            elif block_index == 3:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "            layers.extend([\n",
    "                Conv2dNormActivation(\n",
    "                    current_in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=1,\n",
    "                    padding=0,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=None\n",
    "                ),\n",
    "                Conv2dNormActivation(\n",
    "                    out_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=None\n",
    "                )\n",
    "            ])\n",
    "            self.blocks.append(nn.Sequential(*layers))\n",
    "\n",
    "        if extra_blocks is not None:\n",
    "            if not isinstance(extra_blocks, ExtraFPNBlock):\n",
    "                raise TypeError(f\"extra_blocks should be of type ExtraFPNBlock not {type(extra_blocks)}\")\n",
    "        self.extra_blocks = extra_blocks\n",
    "\n",
    "    def forward(self, x: Tensor) -> Dict[str, Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the Simple FPN for a feature map.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): input feature map.\n",
    "\n",
    "        Returns:\n",
    "            results (list[Tensor]): feature maps after FPN layers.\n",
    "                They are ordered from highest resolution first.\n",
    "        \"\"\"\n",
    "        results = [block(x) for block in self.blocks]\n",
    "        names = [f\"{i}\" for i in range(len(self.blocks))]\n",
    "\n",
    "        if self.extra_blocks is not None:\n",
    "            results, names = self.extra_blocks(results, [x], names)\n",
    "\n",
    "        # make it back an OrderedDict\n",
    "        out = OrderedDict([(k, v) for k, v in zip(names, results)])\n",
    "\n",
    "        return out\n",
    "\n",
    "class ViTDet(nn.Module):\n",
    "    \n",
    "    def __init__(self, out_channels, num_classes):\n",
    "        super(ViTDet, self).__init__()\n",
    "        self.backbone = timm.create_model('samvit_base_patch16.sa1b', pretrained=True)\n",
    "        self.sfpn = SimpleFeaturePyramidNetwork(\n",
    "            in_channels=768,\n",
    "            out_channels=out_channels,\n",
    "            #extra_blocks=LastLevelP6P7(out_channels,out_channels),\n",
    "            norm_layer=LayerNorm2d\n",
    "        )\n",
    "        inp = torch.randn(2, 3, 224, 224)\n",
    "        with torch.no_grad():\n",
    "            out = self.forward_feat(inp)\n",
    "        self.feat_sizes = [o.shape[2:] for o in out.values()]\n",
    "        self.head = Head(out_channels, num_classes, n_feat_levels=6)\n",
    "        \n",
    "    def forward_feat(self, x):\n",
    "        intermediates = self.backbone.forward_intermediates(x, indices=1, norm=False, intermediates_only=True)\n",
    "        features = self.sfpn(intermediates[0])\n",
    "        return features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat_dict = self.forward_feat(x)\n",
    "        features = list(feat_dict.values())\n",
    "        box_cls, box_regression, centerness = self.head(features)\n",
    "        # box_reg: lists of n_feat_level tensors BxHW(level)x4\n",
    "        # why not tensor Bxsum_level(HW)x4 ?\n",
    "        return box_cls, box_regression, centerness\n",
    "    \n",
    "vitdet = ViTDet(256, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94c18a6-e9f7-4cbd-8179-4aff7900498e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat shapes: [f.shape for f in features.values()] = [torch.Size([2, 256, 56, 56]), torch.Size([2, 256, 28, 28]), torch.Size([2, 256, 14, 14]), torch.Size([2, 256, 7, 7])]\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 224, 224)\n",
    "features = vitdet.forward_feat(x)\n",
    "print(f'feat shapes: {[f.shape for f in features.values()] = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "634b547e-54e1-49ed-b6c6-6a9b04196f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box pred shapes: [f.shape for f in box_cls] = [torch.Size([2, 4, 56, 56]), torch.Size([2, 4, 28, 28]), torch.Size([2, 4, 14, 14]), torch.Size([2, 4, 7, 7])]\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 224, 224)\n",
    "box_cls, box_regression, centerness = vitdet.forward(x)\n",
    "print(f'box pred shapes: {[f.shape for f in box_cls] = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0d01e6b-5b98-4aae-a18b-a87aa7c0f8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.83s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=3.24s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type                 | Params\n",
      "--------------------------------------------------------\n",
      "0 | network        | ViTDet               | 97.7 M\n",
      "1 | map_metric     | MeanAveragePrecision | 0     \n",
      "2 | loss           | LossEvaluator        | 0     \n",
      "3 | post_processor | FCOSPostProcessor    | 0     \n",
      "--------------------------------------------------------\n",
      "97.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "97.7 M    Total params\n",
      "390.972   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 97743111 params out of 97743111\n",
      "Sanity Checking: |                                                                                                                        | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                                                                       | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:21<00:00,  0.09it/s]\n",
      "MAP:  tensor(0.)\n",
      "                                                                                                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:07<00:00,  0.65it/s, v_num=64]\n",
      "Validation: |                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "MAP:  tensor(0.)\n",
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  0.50it/s, v_num=64]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from dl_toolbox.callbacks import ProgressBar\n",
    "import gc \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from dl_toolbox.callbacks import ProgressBar\n",
    "from dl_toolbox import datamodules\n",
    "from dl_toolbox import modules\n",
    "import torchvision.transforms.v2 as v2\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from dl_toolbox.callbacks import ProgressBar, Finetuning, Lora, TiffPredsWriter, CalibrationLogger\n",
    "from functools import partial\n",
    "import gc\n",
    "\n",
    "\n",
    "train_tf = v2.Compose(\n",
    "    [\n",
    "        v2.RandomCrop(224),\n",
    "        v2.SanitizeBoundingBoxes(),\n",
    "        v2.Normalize([0.5]*3, [0.5]*3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_tf = v2.Compose(\n",
    "    [\n",
    "        v2.CenterCrop(1000),\n",
    "        v2.Resize(224),\n",
    "        v2.SanitizeBoundingBoxes(),\n",
    "        v2.Normalize([0.5]*3, [0.5]*3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "dm = datamodules.xView(\n",
    "    data_path='/data',\n",
    "    merge='all',\n",
    "    train_tf=train_tf,\n",
    "    test_tf=test_tf,\n",
    "    batch_tf=None,\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    limit_train_batches=5,\n",
    "    limit_val_batches=5,\n",
    "    callbacks=[ProgressBar()]\n",
    ")\n",
    "\n",
    "num_classes = dm.num_classes\n",
    "network = ViTDet(num_classes=num_classes, out_channels=256)\n",
    "\n",
    "module = FCOS(\n",
    "    num_classes=num_classes,\n",
    "    network=network,\n",
    "    optimizer=partial(torch.optim.Adam, lr=0.001),\n",
    "    scheduler=partial(torch.optim.lr_scheduler.ConstantLR, factor=1),\n",
    "    pred_thresh=0.1,\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "trainer.fit(\n",
    "    module,\n",
    "    datamodule=dm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3800cd88-5eab-497f-a960-c8c77f1f52e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
