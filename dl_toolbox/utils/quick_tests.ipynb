{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8085b4-c6db-44df-bf4a-481476952d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Naval Fate.\n",
    "\n",
    "Usage:\n",
    "  naval_fate.py ship new <name>...\n",
    "  naval_fate.py ship <name> move <x> <y> [--speed=<kn>]\n",
    "  naval_fate.py ship shoot <x> <y>\n",
    "  naval_fate.py mine (set|remove) <x> <y> [--moored | --drifting]\n",
    "  naval_fate.py (-h | --help)\n",
    "  naval_fate.py --version\n",
    "\n",
    "Options:\n",
    "  -h --help     Show this screen.\n",
    "  --version     Show version.\n",
    "  --speed=<kn>  Speed in knots [default: 10].\n",
    "  --moored      Moored (anchored) mine.\n",
    "  --drifting    Drifting mine.\n",
    "\n",
    "\"\"\"\n",
    "from docopt import docopt\n",
    "def main():\n",
    "    args = docopt(__doc__)\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cce1557e-851f-4fc1-bf98-31d529c1f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.ops import MLP\n",
    "\n",
    "class RndDataset(Dataset):\n",
    "\n",
    "    def __init__(self, size, num_samples):\n",
    "\n",
    "        self.len = num_samples\n",
    "        self.data = torch.randn(num_samples, size)\n",
    "        self.label = torch.randint(2,(num_samples,)).long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class BoringModel1(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network1 = MLP(4, [3])\n",
    "        self.network2 = MLP(4, [3])\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        inputs, labels = batch\n",
    "                \n",
    "        logits1 = self.network1(inputs)\n",
    "        logits2 = self.network2(inputs)\n",
    "        \n",
    "        sup1 = self.criterion(logits1, labels)\n",
    "        sup2 = self.criterion(logits2, labels)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, m2 = torch.max(logits2, dim=1)\n",
    "        unsup2=self.criterion(logits1, m2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, m1 = torch.max(logits1, dim=1)\n",
    "        unsup1=self.criterion(logits2, m1)\n",
    "        \n",
    "        loss = unsup1+unsup2+sup1+sup2\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=0.1)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "class BoringModel2(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network1 = MLP(4, [3])\n",
    "        self.network2 = MLP(4, [3])\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        opt1, opt2 = self.optimizers()\n",
    "        opt1.zero_grad()\n",
    "        opt2.zero_grad()\n",
    "                        \n",
    "        inputs, labels = batch\n",
    "                \n",
    "        logits1 = self.network1(inputs)\n",
    "        logits2 = self.network2(inputs)\n",
    "        \n",
    "        sup1 = self.criterion(logits1, labels)\n",
    "        sup2 = self.criterion(logits2, labels)\n",
    "        \n",
    "        _, m2 = torch.max(logits2, dim=1)\n",
    "        unsup2=self.criterion(logits1, m2)\n",
    "        _, m1 = torch.max(logits1, dim=1)\n",
    "        unsup1=self.criterion(logits2, m1)\n",
    "        loss = unsup1+unsup2+sup1+sup2\n",
    "        \n",
    "        self.manual_backward(loss)\n",
    "        opt1.step()\n",
    "        opt2.step()\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt1 = torch.optim.SGD(self.network1.parameters(), lr=0.1)\n",
    "        opt2 = torch.optim.SGD(self.network2.parameters(), lr=0.1)\n",
    "        lr_sch1 = torch.optim.lr_scheduler.StepLR(opt1, step_size=1)\n",
    "        lr_sch2 = torch.optim.lr_scheduler.StepLR(opt2, step_size=1)\n",
    "        return [opt1, opt2], [lr_sch1, lr_sch2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cc9bbea4-9b43-4db2-861d-3299fc19f00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | network1  | MLP              | 15    \n",
      "1 | network2  | MLP              | 15    \n",
      "2 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "30        Trainable params\n",
      "0         Non-trainable params\n",
      "30        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 186.32it/s, v_num=75]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 66.77it/s, v_num=75] \n",
      "\n",
      " Model 1 Network1 weights : Parameter containing:\n",
      "tensor([[ 0.4152,  0.0570, -0.2193,  0.3584],\n",
      "        [-0.5608,  0.0951,  0.0291,  0.1991],\n",
      "        [ 0.0020, -0.1342,  0.1291, -0.0438]], requires_grad=True)\n",
      "\n",
      " Model 1 Network1 bias : Parameter containing:\n",
      "tensor([[ 0.0792,  0.2773, -0.0794,  0.5394],\n",
      "        [ 0.0884, -0.3792, -0.1495, -0.1117],\n",
      "        [-0.2727, -0.1721, -0.0511,  0.2237]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "model1 = BoringModel1()\n",
    "\n",
    "#print('\\n Model 1 Network1 weights :', model1.network1.weight)\n",
    "#print('\\n Model 1 Network1 bias :', model1.network1.bias)\n",
    "\n",
    "trainer1 = pl.Trainer(\n",
    "    accelerator='cpu',\n",
    "    devices=1,\n",
    "    max_epochs=1,\n",
    ")\n",
    "\n",
    "trainset1 = RndDataset(4, 2)\n",
    "train1 = DataLoader(trainset1, shuffle=False, batch_size=1)\n",
    "\n",
    "trainer1.fit(model1, train1)\n",
    "\n",
    "print('\\n Model 1 Network1 weights :', model1.network1[0].weight)\n",
    "print('\\n Model 1 Network1 bias :', model1.network2[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c3621cdd-3b6e-4de3-a9a8-9819fad8134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | network1  | MLP              | 15    \n",
      "1 | network2  | MLP              | 15    \n",
      "2 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "30        Trainable params\n",
      "0         Non-trainable params\n",
      "30        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 186.52it/s, v_num=76]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 100.15it/s, v_num=76]\n",
      "\n",
      " Model 2 Network1 weights : Parameter containing:\n",
      "tensor([[ 0.4152,  0.0570, -0.2193,  0.3584],\n",
      "        [-0.5608,  0.0951,  0.0291,  0.1991],\n",
      "        [ 0.0020, -0.1342,  0.1291, -0.0438]], requires_grad=True)\n",
      "\n",
      " Model 2 Network1 bias : Parameter containing:\n",
      "tensor([[ 0.0792,  0.2773, -0.0794,  0.5394],\n",
      "        [ 0.0884, -0.3792, -0.1495, -0.1117],\n",
      "        [-0.2727, -0.1721, -0.0511,  0.2237]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "model2 = BoringModel2()\n",
    "\n",
    "trainer2 = pl.Trainer(\n",
    "    accelerator='cpu',\n",
    "    devices=1,\n",
    "    max_epochs=1,\n",
    ")\n",
    "\n",
    "trainset2 = RndDataset(4, 2)\n",
    "train2 = DataLoader(trainset2, shuffle=False, batch_size=1)\n",
    "\n",
    "trainer2.fit(model2, train2)\n",
    "\n",
    "print('\\n Model 2 Network1 weights :', model2.network1[0].weight)\n",
    "print('\\n Model 2 Network1 bias :', model2.network2[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a672535e-8732-4a75-805d-d1428990b0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:04<00:00, 5623426.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 55838918.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 4422102/4422102 [00:00<00:00, 5329747.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 21876673.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m logits\n\u001b[1;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetwork()\n\u001b[0;32m---> 43\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdataloader\u001b[49m\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(X)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e875ea6-0381-4332-a56d-9213159c2614",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'driver': 'GTiff', 'dtype': 'float32', 'nodata': None, 'width': 2048, 'height': 2048, 'count': 4, 'crs': CRS.from_epsg(32630), 'transform': Affine(0.4996690458118865, 0.0, 620734.67813581,\n",
      "       0.0, -0.4996690458118865, 4816930.568377791)}\n",
      "Band1 has shape (2048, 2048)\n",
      "lons shape (2048, 2048)\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "file_name = '/work/OT/ai4geo/DATA/DATASETS/DIGITANIE/Biarritz/Biarritz_EPSG32630_1.tif'\n",
    "file_name = '/data/DIGITANIE/Biarritz/Biarritz_EPSG32630_1.tif'\n",
    "with rasterio.open(file_name) as src:\n",
    "    print(src.meta)\n",
    "    band1 = src.read(1)\n",
    "    print('Band1 has shape', band1.shape)\n",
    "    height = band1.shape[0]\n",
    "    width = band1.shape[1]\n",
    "    cols, rows = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    xs, ys = rasterio.transform.xy(src.transform, rows, cols)\n",
    "    lons= np.array(xs)\n",
    "    lats = np.array(ys)\n",
    "    print('lons shape', lons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d6ad260-1375-4743-b74d-a7ab1ba5a8c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lons = lons[:100, :100]\n",
    "lats = lats[:100, :100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb4b012-aa55-4629-a856-47e3ccb0f04e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/tmp/ipykernel_3854905/320663268.py:9: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  input_lon, input_lat = proj.transform(crs_src, crs_dst, lons, lats)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-1.50669398, -1.5066878 , -1.50668162, ..., -1.5060947 ,\n",
       "         -1.50608852, -1.50608235],\n",
       "        [-1.50669409, -1.50668791, -1.50668173, ..., -1.50609481,\n",
       "         -1.50608864, -1.50608246],\n",
       "        [-1.5066942 , -1.50668802, -1.50668184, ..., -1.50609492,\n",
       "         -1.50608875, -1.50608257],\n",
       "        ...,\n",
       "        [-1.50670473, -1.50669855, -1.50669237, ..., -1.50610546,\n",
       "         -1.50609928, -1.5060931 ],\n",
       "        [-1.50670484, -1.50669866, -1.50669248, ..., -1.50610557,\n",
       "         -1.50609939, -1.50609321],\n",
       "        [-1.50670495, -1.50669877, -1.50669259, ..., -1.50610568,\n",
       "         -1.5060995 , -1.50609333]]),\n",
       " array([[43.49555068, 43.4955506 , 43.49555052, ..., 43.49554285,\n",
       "         43.49554277, 43.49554269],\n",
       "        [43.49554618, 43.4955461 , 43.49554602, ..., 43.49553835,\n",
       "         43.49553827, 43.49553819],\n",
       "        [43.49554168, 43.4955416 , 43.49554152, ..., 43.49553385,\n",
       "         43.49553377, 43.49553369],\n",
       "        ...,\n",
       "        [43.49511441, 43.49511433, 43.49511425, ..., 43.49510658,\n",
       "         43.4951065 , 43.49510642],\n",
       "        [43.49510991, 43.49510983, 43.49510975, ..., 43.49510208,\n",
       "         43.495102  , 43.49510192],\n",
       "        [43.49510541, 43.49510533, 43.49510525, ..., 43.49509758,\n",
       "         43.4950975 , 43.49509742]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyproj as proj\n",
    "import utm\n",
    "\n",
    "# setup your projections\n",
    "crs_src = proj.Proj(init='epsg:32630') # assuming you're using WGS84 geographic\n",
    "crs_dst = proj.Proj(init='epsg:4326 ') # use a locally appropriate projected CRS\n",
    "\n",
    "# then cast your geographic coordinate pair to the projected system\n",
    "input_lon, input_lat = proj.transform(crs_src, crs_dst, lons, lats)\n",
    "input_lon, input_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5934db9-4f0e-4c90-a6a7-400de428cabd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[620734.92797094, 620735.42763999, 620735.92730903, ...,\n",
       "         620783.39586839, 620783.89553743, 620784.39520648],\n",
       "        [620734.92797094, 620735.42763999, 620735.92730903, ...,\n",
       "         620783.39586839, 620783.89553743, 620784.39520648],\n",
       "        [620734.92797094, 620735.42763999, 620735.92730903, ...,\n",
       "         620783.39586839, 620783.89553743, 620784.39520648],\n",
       "        ...,\n",
       "        [620734.92797094, 620735.42763999, 620735.92730903, ...,\n",
       "         620783.39586839, 620783.89553743, 620784.39520648],\n",
       "        [620734.92797094, 620735.42763999, 620735.92730903, ...,\n",
       "         620783.39586839, 620783.89553743, 620784.39520648],\n",
       "        [620734.92797094, 620735.42763999, 620735.92730903, ...,\n",
       "         620783.39586839, 620783.89553743, 620784.39520648]]),\n",
       " array([[4816930.31872322, 4816930.31872322, 4816930.31872322, ...,\n",
       "         4816930.31872322, 4816930.31872322, 4816930.31872322],\n",
       "        [4816929.81905417, 4816929.81905417, 4816929.81905417, ...,\n",
       "         4816929.81905417, 4816929.81905417, 4816929.81905417],\n",
       "        [4816929.31938512, 4816929.31938512, 4816929.31938512, ...,\n",
       "         4816929.31938512, 4816929.31938512, 4816929.31938512],\n",
       "        ...,\n",
       "        [4816881.85082576, 4816881.85082576, 4816881.85082576, ...,\n",
       "         4816881.85082576, 4816881.85082576, 4816881.85082576],\n",
       "        [4816881.35115672, 4816881.35115672, 4816881.35115671, ...,\n",
       "         4816881.35115671, 4816881.35115672, 4816881.35115672],\n",
       "        [4816880.85148767, 4816880.85148767, 4816880.85148767, ...,\n",
       "         4816880.85148767, 4816880.85148767, 4816880.85148767]]),\n",
       " 30,\n",
       " 'T')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utm.from_latlon(input_lat, input_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775b387-f2be-448b-9ae3-d55b4b341b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(np.cos(lats), np.cos(lons))\n",
    "np.multiply(np.cos(lats), np.sin(lons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea16711-c4a8-452e-a1ee-d1781e46438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "\n",
    "def read_window_from_big_raster(window, path, raster_path):\n",
    "    with rasterio.open(path) as image_file:\n",
    "        with rasterio.open(raster_path) as raster_file:\n",
    "            left, bottom, right, top = rasterio.windows.bounds(\n",
    "                window, \n",
    "                transform=image_file.transform\n",
    "            )\n",
    "            rw = rasterio.windows.from_bounds(\n",
    "                left, bottom, right, top, \n",
    "                transform=raster_file.transform\n",
    "            )\n",
    "            image = raster_file.read(\n",
    "                window=rw, \n",
    "                out_dtype=np.float32\n",
    "            )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f18c74ff-4105-481b-a5e5-b3d1fc07dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.windows import Window\n",
    "import numpy as np\n",
    "window = Window(0, 0, 10000, 10000)\n",
    "path = p+f'/Biarritz_EPSG32630_9.tif'\n",
    "big_path = p+'/BIARRITZ_20140902_T_TOA_reproj-EPSG:32630.tif'\n",
    "img = read_window_from_big_raster(window, path, big_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73c3cf10-fcad-4ecb-ab1b-2463a29cb140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_window_basic(window, path):\n",
    "    with rasterio.open(path) as image_file:\n",
    "        image = image_file.read(window=window, out_dtype=np.float32)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4a349b1-0661-4beb-8788-c328dc8796cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window(0, 0, 10000, 10000)\n",
    "img2 = read_window_basic(window,big_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba333a3b-f426-45b8-971c-7f57649a2ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10000, 10000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e7b6f1-2bfd-4277-9180-354823f8b4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<open DatasetReader name='/work/OT/ai4usr/fournip/digitanie/Toulouse/toulouse_full_tiled.tif' mode='r'>\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "p = '/work/OT/ai4usr/fournip/digitanie/Toulouse/toulouse_full_tiled.tif'\n",
    "ds = rasterio.open(p)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bed089e-da60-4e12-844e-1ccc6570633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ds.statistics(bidx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b25969-3068-41ab-a369-50005600c0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Statistics(min=-0.0029133798088878393, max=1.5431257486343384, mean=0.09988372604258908, std=0.05581474629124038)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c80371-2428-4a0d-94bd-4eeb10ba24e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0029133798088878393"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4173b-3ef0-4527-ba70-c0eda7ffe58e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latest",
   "language": "python",
   "name": "latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
