{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80247a83-666b-4e1d-9137-785489e8e251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No mid pe\n",
      "Training 22731270 params out of 22731270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/src/yolos/backbone.py:423: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrained, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from yolos.detector import Detector, SetCriterion, PostProcess\n",
    "from yolos.matcher import HungarianMatcher\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "num_classes = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Detector(\n",
    "    num_classes=num_classes,\n",
    "    pre_trained='/d/pfournie/YOLOS/deit_small_patch16_224-cd65a155.pth',\n",
    "    det_token_num=100,\n",
    "    backbone_name='small',\n",
    "    init_pe_size=(560,560)\n",
    ")\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "nb_tot = sum([int(torch.numel(p)) for p in model.parameters()])\n",
    "print(f\"Training {n_parameters} params out of {nb_tot}\")\n",
    "model.to(device)\n",
    "matcher = HungarianMatcher(1,5,2)\n",
    "weight_dict = {'loss_ce': 1, 'loss_bbox': 5, 'loss_giou': 2}\n",
    "losses = ['labels', 'boxes', 'cardinality']\n",
    "criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict,\n",
    "                         eos_coef=0.1, losses=losses)\n",
    "criterion.to(device)\n",
    "postprocessors = {'bbox': PostProcess()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "962473af-cccc-4875-a7b8-075ae1b67cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_optimizer(model):\n",
    "    if hasattr(model.backbone, 'no_weight_decay'):\n",
    "        skip = model.backbone.no_weight_decay()\n",
    "    head = []\n",
    "    backbone_decay = []\n",
    "    backbone_no_decay = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"backbone\" not in name and param.requires_grad:\n",
    "            head.append(param)\n",
    "        if \"backbone\" in name and param.requires_grad:\n",
    "            if len(param.shape) == 1 or name.endswith(\".bias\") or name.split('.')[-1] in skip:\n",
    "                backbone_no_decay.append(param)\n",
    "            else:\n",
    "                backbone_decay.append(param)\n",
    "    param_dicts = [\n",
    "        {\"params\": head},\n",
    "        {\"params\": backbone_no_decay, \"weight_decay\": 0., \"lr\": 0.0001},\n",
    "        {\"params\": backbone_decay, \"lr\": 0.0001},\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=0.001,\n",
    "                              weight_decay=0.0001)\n",
    "    return optimizer\n",
    "\n",
    "optimizer = build_optimizer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7c09e-9929-4dc5-8f68-d4201a623ed4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset & dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9374da9c-ecba-49c1-97bc-30bf4e820eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pformat\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import gc \n",
    "\n",
    "from torchvision.transforms import v2 as T\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, RandomSampler\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = read_image(img_path)\n",
    "        mask = read_image(mask_path)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(masks)\n",
    "\n",
    "        # there is only one class: attention différent de fcos, A UNIFORMISER\n",
    "        labels = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = tv_tensors.Image(img)\n",
    "\n",
    "        target = {}\n",
    "        h, w = v2.functional.get_size(img)\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(h,w))\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.Tensor([image_id])\n",
    "        #target[\"area\"] = area\n",
    "        #target[\"iscrowd\"] = iscrowd\n",
    "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "            \n",
    "        return {'image': img, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "from dl_toolbox.transforms import NormalizeBB\n",
    "\n",
    "tf = T.Compose(\n",
    "    [\n",
    "        T.Resize(size=480, max_size=560),\n",
    "        T.RandomCrop(size=(560,560), pad_if_needed=True, fill=0),\n",
    "        T.ToDtype(torch.float, scale=True),\n",
    "        T.SanitizeBoundingBoxes(),\n",
    "        T.ConvertBoundingBoxFormat(format='CXCYWH'),\n",
    "        NormalizeBB(),\n",
    "        #T.ToPureTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "dataset = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "dataset_test = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_set = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "val_set = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def list_of_dicts_to_dict_of_lists(list_of_dicts):\n",
    "    dict_of_lists = defaultdict(list)\n",
    "    for dct in list_of_dicts:\n",
    "        for key, value in dct.items():\n",
    "            dict_of_lists[key].append(value)\n",
    "    res = dict(dict_of_lists)\n",
    "    return res\n",
    "\n",
    "def collate(batch):\n",
    "    batch = list_of_dicts_to_dict_of_lists(batch)\n",
    "    batch['image'] = torch.stack(batch['image'])\n",
    "    return batch\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    dataset=train_set,\n",
    "    sampler=RandomSampler(\n",
    "        train_set,\n",
    "        #replacement=True,\n",
    "        #num_samples=100*2\n",
    "    ),\n",
    "    drop_last=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    dataset=val_set,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41433fa2-d761-48a2-880c-d470c9930003",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer=optimizer,\n",
    "    start_factor=1.,\n",
    "    end_factor=0.01,\n",
    "    total_iters=150*30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aa85d72-d0bb-4725-8083-1a9a438170be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unnorm_bounding_boxes(inpt):\n",
    "    bounding_boxes = inpt.as_subclass(torch.Tensor)\n",
    "    in_dtype = bounding_boxes.dtype\n",
    "    bounding_boxes = bounding_boxes.clone() if bounding_boxes.is_floating_point() else bounding_boxes.float()\n",
    "    whwh = torch.Tensor(inpt.canvas_size).repeat(2).flip(dims=(0,)).to(inpt.device) # canvas_size is H,W hence the flip to WHWH\n",
    "    out_boxes = bounding_boxes*whwh \n",
    "    return tv_tensors.wrap(out_boxes.to(in_dtype), like=inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a13c370-b3be-4342-8ab3-7d172f8b3cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:01<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "valid_loss = 5.998540706634522\n",
      "valid_bbox_loss = 0.6899915099143982\n",
      "valid_giou_loss = 0.9110201692581177\n",
      "valid_ce_loss = 0.7265427708625793\n",
      "{'classes': tensor(0, dtype=torch.int32),\n",
      " 'map': tensor(4.4341e-05),\n",
      " 'map_50': tensor(0.0004),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(4.4341e-05),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0008),\n",
      " 'mar_10': tensor(0.0030),\n",
      " 'mar_100': tensor(0.0038),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.0043),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:09<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "train_loss = 3.1916798214117685\n",
      "train_bbox_loss = 0.3167770400643349\n",
      "train_giou_loss = 0.5768394584457079\n",
      "train_ce_loss = 0.4541156828403473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:01<00:00, 13.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1\n",
      "valid_loss = 3.219658088684082\n",
      "valid_bbox_loss = 0.3059837365150452\n",
      "valid_giou_loss = 0.5565073442459106\n",
      "valid_ce_loss = 0.5767246913909913\n",
      "{'classes': tensor(0, dtype=torch.int32),\n",
      " 'map': tensor(0.0034),\n",
      " 'map_50': tensor(0.0184),\n",
      " 'map_75': tensor(0.0003),\n",
      " 'map_large': tensor(0.0038),\n",
      " 'map_medium': tensor(0.0018),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0098),\n",
      " 'mar_10': tensor(0.0523),\n",
      " 'mar_100': tensor(0.1485),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.1658),\n",
      " 'mar_medium': tensor(0.0154),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████████████████▊                                                                      | 21/60 [00:03<00:06,  6.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     63\u001b[0m criterion\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m     65\u001b[0m     image \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     66\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(image)\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36mPennFudanDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPNGImages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs[idx])\n\u001b[1;32m     33\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPedMasks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks[idx])\n\u001b[0;32m---> 34\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mread_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m mask \u001b[38;5;241m=\u001b[39m read_image(mask_path)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# instances are encoded as different colors\u001b[39;00m\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torchvision/io/image.py:298\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode, apply_exif_orientation)\u001b[0m\n\u001b[1;32m    296\u001b[0m     _log_api_usage_once(read_image)\n\u001b[1;32m    297\u001b[0m data \u001b[38;5;241m=\u001b[39m read_file(path)\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_exif_orientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapply_exif_orientation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torchvision/io/image.py:269\u001b[0m, in \u001b[0;36mdecode_image\u001b[0;34m(input, mode, apply_exif_orientation)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    268\u001b[0m     _log_api_usage_once(decode_image)\n\u001b[0;32m--> 269\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_exif_orientation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[0;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from yolos.box_ops import box_cxcywh_to_xyxy\n",
    "gc.collect()\n",
    "#torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "start_epoch = 0\n",
    "for epoch in range(start_epoch, 150):\n",
    "    time_ep = time.time()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_bbox_loss = 0\n",
    "    valid_giou_loss = 0\n",
    "    valid_ce_loss = 0\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "    with torch.no_grad():\n",
    "        map_metric = MeanAveragePrecision(\n",
    "            box_format='xyxy', # make sure your dataset outputs target in xywh format\n",
    "            backend='faster_coco_eval'\n",
    "        )\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            image = batch[\"image\"].to(device)\n",
    "            targets = batch['target']\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model(image)\n",
    "            loss_dict = criterion(outputs, targets)\n",
    "            weight_dict = criterion.weight_dict\n",
    "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "            b,c,h,w = image.shape\n",
    "            results = postprocessors['bbox'](outputs, (h,w))\n",
    "            for t in targets:\n",
    "                t['boxes'] = unnorm_bounding_boxes(t['boxes'])\n",
    "                t['boxes'] = box_cxcywh_to_xyxy(t['boxes'])\n",
    "            map_metric.update(results, targets)\n",
    "            valid_loss += losses.detach().item()\n",
    "            valid_bbox_loss += loss_dict[\"loss_bbox\"].detach().item()\n",
    "            valid_giou_loss += loss_dict[\"loss_giou\"].detach().item()\n",
    "            valid_ce_loss += loss_dict[\"loss_ce\"].detach().item()\n",
    "        valid_loss /= len(val_dataloader)\n",
    "        valid_bbox_loss /= len(val_dataloader)\n",
    "        valid_giou_loss /= len(val_dataloader)\n",
    "        valid_ce_loss /= len(val_dataloader)\n",
    "        mapmetrics = map_metric.compute()\n",
    "        print(f\"{epoch = }\")\n",
    "        print(f\"{valid_loss = }\")\n",
    "        print(f\"{valid_bbox_loss = }\")\n",
    "        print(f\"{valid_giou_loss = }\")\n",
    "        print(f\"{valid_ce_loss = }\")\n",
    "        print(pformat(mapmetrics))\n",
    "        map_metric.reset()\n",
    "        \n",
    "    train_loss = 0\n",
    "    train_bbox_loss = 0\n",
    "    train_giou_loss = 0\n",
    "    train_ce_loss = 0\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        image = batch[\"image\"].to(device)\n",
    "        outputs = model(image)\n",
    "        targets = batch['target']\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        train_loss += losses.detach().item()\n",
    "        train_bbox_loss += loss_dict[\"loss_bbox\"].detach().item()\n",
    "        train_giou_loss += loss_dict[\"loss_giou\"].detach().item()\n",
    "        train_ce_loss += loss_dict[\"loss_ce\"].detach().item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_bbox_loss /= len(train_dataloader)\n",
    "    train_giou_loss /= len(train_dataloader)\n",
    "    train_ce_loss /= len(train_dataloader)\n",
    "    print(f\"{epoch = }\")\n",
    "    #print(f\"lr = {lr_scheduler.get_last_lr()[0]}\"),\n",
    "    print(f\"{train_loss = }\")\n",
    "    print(f\"{train_bbox_loss = }\")\n",
    "    print(f\"{train_giou_loss = }\")\n",
    "    print(f\"{train_ce_loss = }\")\n",
    "    #lr_scheduler.step(epoch)\n",
    "    time_ep = time.time() - time_ep"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_38",
   "language": "python",
   "name": "dl_toolbox_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
