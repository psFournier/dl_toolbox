{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9374da9c-ecba-49c1-97bc-30bf4e820eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import gc \n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Subset, RandomSampler\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torchvision\n",
    "from torchvision.ops import box_convert, generalized_box_iou\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelP6P7, ExtraFPNBlock\n",
    "from torchvision.ops.misc import Conv2dNormActivation\n",
    "import timm\n",
    "from timm.layers import resample_abs_pos_embed \n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pformat\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import rasterio\n",
    "\n",
    "from dl_toolbox.transforms import NormalizeBB\n",
    "from dl_toolbox.utils import list_of_dicts_to_dict_of_lists\n",
    "from dl_toolbox.utils import label, merge_labels_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70a27c-fad7-44ad-9724-4fe987a941a2",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5eb7083-dd77-43bf-8fe8-d08a5f6f2323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Scale(nn.Module):\n",
    "\n",
    "    def __init__(self, init_value=1.0):\n",
    "        super(Scale, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([init_value]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.scale\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, n_classes, n_share_convs=4, n_feat_levels=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        tower = []\n",
    "        for _ in range(n_share_convs):\n",
    "            tower.append(\n",
    "                nn.Conv2d(in_channels,\n",
    "                          in_channels,\n",
    "                          kernel_size=3,\n",
    "                          stride=1,\n",
    "                          padding=1,\n",
    "                          bias=True))\n",
    "            tower.append(nn.GroupNorm(32, in_channels))\n",
    "            tower.append(nn.ReLU())\n",
    "        self.shared_layers = nn.Sequential(*tower)\n",
    "\n",
    "        self.cls_logits = nn.Conv2d(in_channels,\n",
    "                                    n_classes,\n",
    "                                    kernel_size=3,\n",
    "                                    stride=1,\n",
    "                                    padding=1)\n",
    "        self.bbox_pred = nn.Conv2d(in_channels,\n",
    "                                   4,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1)\n",
    "        self.ctrness = nn.Conv2d(in_channels,\n",
    "                                 1,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1,\n",
    "                                 padding=1)\n",
    "\n",
    "        self.scales = nn.ModuleList([Scale(init_value=1.0) for _ in range(n_feat_levels)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_logits = []\n",
    "        bbox_preds = []\n",
    "        cness_preds = []\n",
    "        for l, features in enumerate(x):\n",
    "            features = self.shared_layers(features)\n",
    "            cls_logits.append(self.cls_logits(features).flatten(-2))\n",
    "            cness_preds.append(self.ctrness(features).flatten(-2))\n",
    "            reg = self.bbox_pred(features)\n",
    "            reg = self.scales[l](reg)\n",
    "            bbox_preds.append(nn.functional.relu(reg).flatten(-2))\n",
    "        all_logits = torch.cat(cls_logits, dim=-1).permute(0,2,1) # BxNumAnchorsxC\n",
    "        all_box_regs = torch.cat(bbox_preds, dim=-1).permute(0,2,1) # BxNumAnchorsx4\n",
    "        all_cness = torch.cat(cness_preds, dim=-1).permute(0,2,1) # BxNumAnchorsx1\n",
    "        return all_logits, all_box_regs, all_cness\n",
    "\n",
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    \"\"\" LayerNorm for channels of '2D' spatial NCHW tensors \"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6, affine=True):\n",
    "        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.layer_norm(\n",
    "            x.permute(0, 2, 3, 1), self.normalized_shape, self.weight, self.bias, self.eps).permute(0, 3, 1, 2)\n",
    "\n",
    "class SimpleFeaturePyramidNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Module that adds a Simple FPN from on top of a set of feature maps. This is based on\n",
    "    `\"Exploring Plain Vision Transformer Backbones for Object Detection\" <https://arxiv.org/abs/2203.16527>`_.\n",
    "\n",
    "    Unlike regular FPN, Simple FPN expects a single feature map,\n",
    "    on which the Simple FPN will be added.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of channels for the input feature map that\n",
    "            is passed to the module\n",
    "        out_channels (int): number of channels of the Simple FPN representation\n",
    "        extra_blocks (ExtraFPNBlock or None): if provided, extra operations will\n",
    "            be performed. It is expected to take the fpn features, the original\n",
    "            features and the names of the original features as input, and returns\n",
    "            a new list of feature maps and their corresponding names\n",
    "        norm_layer (callable, optional): Module specifying the normalization layer to use. Default: LayerNorm\n",
    "\n",
    "    Examples::\n",
    "    \n",
    "        >>> vitdet = ViTDet(256, 10)\n",
    "        >>> x = torch.rand(2, 3, 224, 224)\n",
    "        >>> feat_dict = vitdet.forward_feat(x)\n",
    "        >>> features = list(feat_dict.values())\n",
    "        >>> print(f'{[f.shape for f in features] = }')\n",
    "        >>> box_cls, box_regression, centerness = vitdet.head(features)\n",
    "        >>> print(f'{box_cls.shape = }')\n",
    "        >>> assert sum([f.shape[2]*f.shape[3] for f in features])==box_cls.shape[1]\n",
    "\n",
    "        DOES NOT WORK BELOW\n",
    "        >>> m = torchvision.ops.SimpleFeaturePyramidNetwork(10, 5)\n",
    "        >>> # get some dummy data\n",
    "        >>> x = torch.rand(1, 10, 64, 64)\n",
    "        >>> # compute the Simple FPN on top of x\n",
    "        >>> output = m(x)\n",
    "        >>> print([(k, v.shape) for k, v in output.items()])\n",
    "        >>> # returns\n",
    "        >>>   [('feat0', torch.Size([1, 5, 64, 64])),\n",
    "        >>>    ('feat2', torch.Size([1, 5, 16, 16])),\n",
    "        >>>    ('feat3', torch.Size([1, 5, 8, 8]))]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        extra_blocks: Optional[ExtraFPNBlock] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for block_index in range(0,4):\n",
    "            layers = []\n",
    "            current_in_channels = in_channels\n",
    "            if block_index == 0:\n",
    "                layers.extend([\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels,\n",
    "                        in_channels // 2,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                    norm_layer(in_channels // 2),\n",
    "                    nn.GELU(),\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels // 2,\n",
    "                        in_channels // 4,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                ])\n",
    "                current_in_channels = in_channels // 4\n",
    "            elif block_index == 1:\n",
    "                layers.append(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels,\n",
    "                        in_channels // 2,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                )\n",
    "                current_in_channels = in_channels // 2\n",
    "            elif block_index == 2:\n",
    "                # nothing to do for this scale\n",
    "                pass\n",
    "            elif block_index == 3:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "            layers.extend([\n",
    "                Conv2dNormActivation(\n",
    "                    current_in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=1,\n",
    "                    padding=0,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=None\n",
    "                ),\n",
    "                Conv2dNormActivation(\n",
    "                    out_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=None\n",
    "                )\n",
    "            ])\n",
    "            self.blocks.append(nn.Sequential(*layers))\n",
    "\n",
    "        if extra_blocks is not None:\n",
    "            if not isinstance(extra_blocks, ExtraFPNBlock):\n",
    "                raise TypeError(f\"extra_blocks should be of type ExtraFPNBlock not {type(extra_blocks)}\")\n",
    "        self.extra_blocks = extra_blocks\n",
    "\n",
    "    def forward(self, x: Tensor) -> Dict[str, Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the Simple FPN for a feature map.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): input feature map.\n",
    "\n",
    "        Returns:\n",
    "            results (list[Tensor]): feature maps after FPN layers.\n",
    "                They are ordered from highest resolution first.\n",
    "        \"\"\"\n",
    "        results = [block(x) for block in self.blocks]\n",
    "        names = [f\"{i}\" for i in range(len(self.blocks))]\n",
    "\n",
    "        if self.extra_blocks is not None:\n",
    "            results, names = self.extra_blocks(results, [x], names)\n",
    "\n",
    "        # make it back an OrderedDict\n",
    "        out = OrderedDict([(k, v) for k, v in zip(names, results)])\n",
    "\n",
    "        return out\n",
    "    \n",
    "class ViTDet(nn.Module):\n",
    "    \n",
    "    def __init__(self, out_channels, num_classes):\n",
    "        super(ViTDet, self).__init__()\n",
    "        #self.backbone = timm.create_model('samvit_base_patch16.sa1b', pretrained=True)\n",
    "        self.backbone = timm.create_model(\n",
    "            'vit_tiny_patch16_224',\n",
    "            pretrained=True,\n",
    "            dynamic_img_size=True #Deals with inputs of other size than pretraining\n",
    "        )\n",
    "        self.sfpn = SimpleFeaturePyramidNetwork(\n",
    "            in_channels=192,\n",
    "            out_channels=out_channels,\n",
    "            #extra_blocks=LastLevelP6P7(out_channels,out_channels),\n",
    "            norm_layer=LayerNorm2d\n",
    "        )\n",
    "        self.head = Head(out_channels, num_classes, n_feat_levels=4) # 6=4+2extrablocks\n",
    "        \n",
    "    def forward_feat(self, x):\n",
    "        intermediates = self.backbone.forward_intermediates(x, indices=1, norm=False, intermediates_only=True)\n",
    "        features = self.sfpn(intermediates[0])\n",
    "        return features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat_dict = self.forward_feat(x)\n",
    "        features = list(feat_dict.values())\n",
    "        box_cls, box_regression, centerness = self.head(features)\n",
    "        return box_cls, box_regression, centerness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7c09e-9929-4dc5-8f68-d4201a623ed4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3393812-b4bc-4456-90ba-d765d7be142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class xView1(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Requires the `COCO API to be installed <https://github.com/pdollar/coco/tree/master/PythonAPI>`_.\n",
    "\n",
    "    Args:\n",
    "        root (str or ``pathlib.Path``): Root directory where images are downloaded to.\n",
    "        annFile (string): Path to json annotation file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, coco_dataset, ids, transforms=None, merge='all'):\n",
    "        self.root = Path(root)\n",
    "        self.coco = coco_dataset\n",
    "        self.ids = ids\n",
    "        self.transforms = transforms\n",
    "        self.class_list = [label(\"building\", (0, 255, 0), {73})]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = self.ids[index]\n",
    "        path = self.root/self.coco.loadImgs(idx)[0][\"file_name\"]\n",
    "        with rasterio.open(path, \"r\") as file:\n",
    "            image = file.read(out_dtype=np.uint8)\n",
    "        tv_image = tv_tensors.Image(torch.from_numpy(image))\n",
    "                \n",
    "        target = self.coco.loadAnns(self.coco.getAnnIds(idx))\n",
    "        target = list_of_dicts_to_dict_of_lists(target)\n",
    "        tv_target = {}\n",
    "        \n",
    "        labels = torch.tensor(target[\"category_id\"])\n",
    "        boxes = torch.as_tensor(target[\"bbox\"]).float()\n",
    "        merged_labels, merged_boxes = merge_labels_boxes(labels, boxes, self.class_list)\n",
    "        \n",
    "        tv_target[\"boxes\"] = tv_tensors.BoundingBoxes(\n",
    "            merged_boxes,\n",
    "            format=tv_tensors.BoundingBoxFormat.XYWH,\n",
    "            canvas_size=tuple(T.functional.get_size(tv_image)),\n",
    "        )\n",
    "        tv_target['labels'] = merged_labels.long()\n",
    "        if self.transforms is not None:\n",
    "            tv_image, tv_target = self.transforms(tv_image, tv_target)\n",
    "        return {'image': tv_image, 'target': tv_target, 'path': path}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    @classmethod\n",
    "    def collate(cls, batch):\n",
    "        batch = list_of_dicts_to_dict_of_lists(batch)\n",
    "        batch['image'] = torch.stack(batch['image'])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae2350-24d5-45dc-a239-aa1223c4fb69",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58abad9-3d22-4433-802b-9242a825c794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LossEvaluator(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(LossEvaluator, self).__init__()\n",
    "        self.centerness_loss_func = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "        self.num_classes = num_classes\n",
    "                \n",
    "    def __call__(self, cls_logits, reg_preds, cness_preds, cls_tgts, reg_tgts):\n",
    "        pos_inds_b, pos_inds_loc = torch.nonzero(cls_tgts > 0, as_tuple=True)\n",
    "        num_pos = len(pos_inds_b)\n",
    "        reg_preds = reg_preds[pos_inds_b, pos_inds_loc, :]\n",
    "        reg_tgts = reg_tgts[pos_inds_b, pos_inds_loc, :]\n",
    "        cness_preds = cness_preds[pos_inds_b, pos_inds_loc, :].squeeze(-1)\n",
    "        cness_tgts = self._compute_centerness_targets(reg_tgts)\n",
    "        cls_loss = self._get_cls_loss(cls_logits, cls_tgts, max(num_pos, 1.))\n",
    "        #reg_loss, centerness_loss = 0,0\n",
    "        #if num_pos > 0:\n",
    "        reg_loss = self._get_reg_loss(\n",
    "            reg_preds, reg_tgts, cness_tgts)\n",
    "        centerness_loss = self._get_centerness_loss(\n",
    "            cness_preds, cness_tgts, max(num_pos, 1.))\n",
    "        losses = {}\n",
    "        losses[\"cls_loss\"] = cls_loss\n",
    "        losses[\"reg_loss\"] = reg_loss\n",
    "        losses[\"centerness_loss\"] = centerness_loss\n",
    "        losses[\"combined_loss\"] = cls_loss + reg_loss + centerness_loss\n",
    "        return losses\n",
    "    \n",
    "    def _compute_centerness_targets(self, reg_tgts):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            reg_tgts: l, t, r, b values to regress, shape BxNumAx4\n",
    "        Returns:\n",
    "            A tensor of shape BxNumA giving how centered each anchor is for the bbox it must regress\n",
    "        \"\"\"\n",
    "        if len(reg_tgts) == 0:\n",
    "            return reg_tgts.new_zeros(len(reg_tgts))\n",
    "        left_right = reg_tgts[..., [0, 2]]\n",
    "        top_bottom = reg_tgts[..., [1, 3]]\n",
    "        centerness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\\n",
    "                    (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0])\n",
    "        return torch.sqrt(centerness)\n",
    "\n",
    "    def _get_cls_loss(self, cls_preds, cls_targets, num_pos_samples):\n",
    "        \"\"\"\n",
    "        cls_targets takes values in 0...C, 0 only when there is no obj to be detected for the anchor\n",
    "        \"\"\"\n",
    "        onehot = nn.functional.one_hot(cls_targets.long(), self.num_classes+1)[...,1:].float()\n",
    "        cls_loss = torchvision.ops.sigmoid_focal_loss(cls_preds, onehot)\n",
    "        return cls_loss.sum() / num_pos_samples\n",
    "\n",
    "    def _get_reg_loss(self, reg_preds, reg_targets, centerness_targets):\n",
    "        ltrb_preds = reg_preds.reshape(-1, 4)\n",
    "        ltrb_tgts = reg_targets.reshape(-1, 4)\n",
    "        xyxy_preds = torch.cat([-ltrb_preds[:,:2], ltrb_preds[:,2:]], dim=1) \n",
    "        xyxy_tgts = torch.cat([-ltrb_tgts[:,:2], ltrb_tgts[:,2:]], dim=1)\n",
    "        reg_losses = torchvision.ops.distance_box_iou_loss(xyxy_preds, xyxy_tgts, reduction='none')\n",
    "        sum_centerness_targets = centerness_targets.sum()\n",
    "        reg_loss = (reg_losses * centerness_targets).sum() / (sum_centerness_targets+1e-8)\n",
    "        return reg_loss\n",
    "\n",
    "    def _get_centerness_loss(self, centerness_preds, centerness_targets,\n",
    "                             num_pos_samples):\n",
    "        centerness_loss = self.centerness_loss_func(centerness_preds,\n",
    "                                                    centerness_targets)\n",
    "        return centerness_loss / num_pos_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c47b3-dd85-422f-b1f4-79c2af239f3b",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96751d23-bad3-438e-aa34-478b6a6faafe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INF = 100000000\n",
    "\n",
    "def get_fm_anchors(h, w, s):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        h, w: height, width of the feat map\n",
    "        s: stride of the featmap = size reduction factor relative to image\n",
    "    Returns:\n",
    "        Tensor NumAnchorsInFeatMap x 2, ordered by column \n",
    "        TODO: check why: DONE: it corresponds to how locs are computed in \n",
    "        https://github.com/tianzhi0549/FCOS/blob/master/fcos_core/modeling/rpn/fcos/fcos.py\n",
    "        When flattening feat maps, we see first the line at H(=y) fixed and W(=x) moving\n",
    "        \n",
    "    \"\"\"\n",
    "    locs_x = [s / 2 + x * s for x in range(w)]\n",
    "    locs_y = [s / 2 + y * s for y in range(h)]\n",
    "    locs = [(x, y) for y in locs_y for x in locs_x] # order !\n",
    "    return torch.tensor(locs)\n",
    "\n",
    "def get_all_anchors_bb_sizes(fm_sizes, fm_strides, bb_sizes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fm_sizes: seq of feature_maps sizes\n",
    "        fm_strides: seq of corresponding strides\n",
    "        bb_sizes: seq of bbox sizes feature maps are associated with, len = len(fm) + 1\n",
    "    Returns:\n",
    "        anchors: list of num_featmaps elem, where each elem indicates the tensor of anchors of size Nx2 in the original image corresponding to each location in the feature map at this level\n",
    "        anchors_bb_sizes: sizes of the bbox each anchor is authorized/supposed to detect\n",
    "    \"\"\"\n",
    "    bb_sizes = [-1] + bb_sizes + [INF]\n",
    "    anchors, anchors_bb_sizes = [], []\n",
    "    for l, ((h,w), s) in enumerate(zip(fm_sizes, fm_strides)):\n",
    "        fm_anchors = get_fm_anchors(h, w, s)\n",
    "        sizes = torch.tensor([bb_sizes[l], bb_sizes[l+1]], dtype=torch.float32)\n",
    "        sizes = sizes.repeat(len(fm_anchors)).view(len(fm_anchors), 2)\n",
    "        anchors.append(fm_anchors)\n",
    "        anchors_bb_sizes.append(sizes)\n",
    "    return torch.cat(anchors, 0), torch.cat(anchors_bb_sizes, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e372b006-de62-412d-af86-c779553f2a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_reg_targets(anchors, bbox):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        anchors: Lx2, anchors coordinates\n",
    "        bbox: tensor of bbox Tx4, format should be xywh\n",
    "    Returns:\n",
    "        reg_tgt: l,t,r,b values to regress for each pair (anchor, bbox)\n",
    "        anchor_in_box: whether anchor is in bbox for each pair (anchor, bbox)\n",
    "    \"\"\"\n",
    "    xs, ys = anchors[:, 0], anchors[:, 1] # L & L, x & y reversed ?? x means position on x-axis\n",
    "    l = xs[:, None] - bbox[:, 0][None] # Lx1 - 1xT -> LxT\n",
    "    t = ys[:, None] - bbox[:, 1][None]\n",
    "    r = bbox[:, 2][None] + bbox[:, 0][None] - xs[:, None]\n",
    "    b = bbox[:, 3][None] + bbox[:, 1][None] - ys[:, None]  \n",
    "    #print(xs[0], ys[0], l[0], t[0], r[0], b[0])\n",
    "    return torch.stack([l, t, r, b], dim=2) # LxTx4\n",
    "\n",
    "def apply_distance_constraints(reg_targets, anchor_sizes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        reg_targets: LxTx4\n",
    "        anchor_bb_sizes: Lx2\n",
    "    Returns:\n",
    "        A LxT tensor where value at (anchor, bbox) is true if the max value to regress at this anchor for this bbox is inside the bounds associated to this anchor\n",
    "        If other values to regress than the max are negatives, it is dealt with anchor_in_boxes.\n",
    "    \"\"\"\n",
    "    max_reg_targets, _ = reg_targets.max(dim=2) # LxT\n",
    "    min_reg_targets, _ = reg_targets.min(dim=2) # LxT\n",
    "    dist_constraints = torch.stack([\n",
    "        min_reg_targets > 0,\n",
    "        max_reg_targets >= anchor_sizes[:, None, 0],\n",
    "        max_reg_targets <= anchor_sizes[:, None, 1]\n",
    "    ])\n",
    "    return torch.all(dist_constraints, dim=0)\n",
    "\n",
    "def anchor_bbox_area(bbox, anchors, fits_to_feature_level):\n",
    "    \"\"\"\n",
    "    Args: bbox is XYWH\n",
    "    Returns: \n",
    "        Tensor LxT where value at (anchor, bbox) is the area of bbox if anchor is in bbox and anchor is associated with bbox of that size\n",
    "        Else INF.\n",
    "    \"\"\"\n",
    "    #bbox_areas = _calc_bbox_area(bbox_targets) # T\n",
    "    bbox_areas = bbox[:, 2] * bbox[:, 3] # T\n",
    "    # area of each target bbox repeated for each loc with inf where the the loc is not \n",
    "    # in the target bbox or if the loc is not at the right level for this bbox size\n",
    "    anchor_bbox_area = bbox_areas[None].repeat(len(anchors), 1) # LxT\n",
    "    anchor_bbox_area[~fits_to_feature_level] = INF\n",
    "    return anchor_bbox_area\n",
    "\n",
    "def associate_targets_to_anchors(targets_batch, anchors, anchors_bb_sizes):\n",
    "    \"\"\"\n",
    "    Associate one target cls/bbox to regress ONLY to each anchor: among the bboxes that contain the anchor and have the right size, pick that of min area.\n",
    "    If no tgt exists for an anchor, the tgt class is 0.\n",
    "    inputs:\n",
    "        targets_batch: list of dict of tv_tensors {'labels':, 'boxes':}; boxes should be in XYWH format\n",
    "        anchors: \n",
    "        anchor_bb_sizes:\n",
    "    outputs:\n",
    "        all class targets: BxNumAnchors\n",
    "        all bbox targets: BxNumAnchorsx4\n",
    "    \"\"\"\n",
    "    all_reg_targets, all_cls_targets = [], []\n",
    "    for targets in targets_batch:\n",
    "        bbox_targets = targets['boxes'] # Tx4, format XYWH\n",
    "        cls_targets = targets['labels'] # T\n",
    "        reg_targets = calculate_reg_targets(\n",
    "            anchors, bbox_targets) # LxTx4, LxT\n",
    "        fits_to_feature_level = apply_distance_constraints(\n",
    "            reg_targets, anchors_bb_sizes) # LxT\n",
    "        locations_to_gt_area = anchor_bbox_area(\n",
    "            bbox_targets, anchors, fits_to_feature_level)\n",
    "        # Core of the anchor/target association\n",
    "        if cls_targets.shape[0]>0:\n",
    "            loc_min_area, loc_min_idxs = locations_to_gt_area.min(dim=1) #L,idx in [0,T-1],T must be>0\n",
    "            reg_targets = reg_targets[range(len(anchors)), loc_min_idxs] # Lx4\n",
    "            cls_targets = cls_targets[loc_min_idxs] # L\n",
    "            cls_targets[loc_min_area == INF] = 0 # 0 is no-obj category\n",
    "        else:\n",
    "            cls_targets = cls_targets.new_zeros((len(anchors),))\n",
    "            reg_targets = reg_targets.new_zeros((len(anchors),4))\n",
    "        all_cls_targets.append(cls_targets)\n",
    "        all_reg_targets.append(reg_targets)\n",
    "    # BxL & BxLx4\n",
    "    return torch.stack(all_cls_targets), torch.stack(all_reg_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfde0d8-2db1-4c7a-8fa9-b86c9c25c907",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Post-processing predictions to boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea21012-a1cc-4b43-b168-ade9619b1f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_nms_thresh=0.3\n",
    "pre_nms_top_n=100000\n",
    "nms_thresh=0.45\n",
    "fpn_post_nms_top_n=50\n",
    "min_size=0\n",
    "\n",
    "def post_process(logits, ltrb, cness, input_size):\n",
    "    probas = logits.sigmoid() # LxC\n",
    "    high_probas = probas > pre_nms_thresh # LxC\n",
    "    # Indices on L and C axis of high prob pairs anchor/class\n",
    "    high_prob_anchors_idx, high_prob_cls = high_probas.nonzero(as_tuple=True) # dim l <= L*C\n",
    "    high_prob_cls += 1 # 0 is for no object\n",
    "    high_prob_ltrb = ltrb[high_prob_anchors_idx] # lx4\n",
    "    high_prob_anchors = anchors[high_prob_anchors_idx] # lx2\n",
    "    # Tensor shape l with values from logits*cness such that logits > pre_nms_thresh \n",
    "    cness_modulated_probas = probas * cness.sigmoid() # LxC\n",
    "    high_prob_scores = cness_modulated_probas[high_probas] # l\n",
    "    # si l est trop longue\n",
    "    if high_probas.sum().item() > pre_nms_top_n:\n",
    "        # Filter the pre_nms_top_n most probable pairs \n",
    "        high_prob_scores, top_k_indices = high_prob_scores.topk(\n",
    "            pre_nms_top_n, sorted=False) \n",
    "        high_prob_cls = high_prob_cls[top_k_indices]\n",
    "        high_prob_ltrb = high_prob_ltrb[top_k_indices]\n",
    "        high_prob_anchors = high_prob_anchors[top_k_indices]\n",
    "\n",
    "    # Rewrites bbox (x0,y0,x1,y1) from reg targets (l,t,r,b) following eq (1) in paper\n",
    "    high_prob_boxes = torch.stack([\n",
    "        high_prob_anchors[:, 0] - high_prob_ltrb[:, 0],\n",
    "        high_prob_anchors[:, 1] - high_prob_ltrb[:, 1],\n",
    "        high_prob_anchors[:, 0] + high_prob_ltrb[:, 2],\n",
    "        high_prob_anchors[:, 1] + high_prob_ltrb[:, 3],\n",
    "    ], dim=1)\n",
    "\n",
    "    high_prob_boxes = torchvision.ops.clip_boxes_to_image(high_prob_boxes, input_size)\n",
    "    big_enough_box_idxs = torchvision.ops.remove_small_boxes(high_prob_boxes, min_size)\n",
    "    boxes = high_prob_boxes[big_enough_box_idxs]\n",
    "    # Why not do that on scores and classes too ? \n",
    "    classes = high_prob_cls[big_enough_box_idxs]\n",
    "    scores = high_prob_scores[big_enough_box_idxs]\n",
    "    #high_prob_scores = torch.sqrt(high_prob_scores) # WHY SQRT ? REmOVED\n",
    "    # NMS expects boxes to be in xyxy format\n",
    "    nms_idxs = torchvision.ops.nms(boxes, scores, nms_thresh)\n",
    "    boxes = boxes[nms_idxs]\n",
    "    scores = scores[nms_idxs]\n",
    "    classes = classes[nms_idxs]\n",
    "    if len(nms_idxs) > fpn_post_nms_top_n:\n",
    "        image_thresh, _ = torch.kthvalue(\n",
    "            scores.cpu(),\n",
    "            len(nms_idxs) - fpn_post_nms_top_n + 1)\n",
    "        keep = scores >= image_thresh.item()\n",
    "        #keep = torch.nonzero(keep).squeeze(1)\n",
    "        boxes, scores, classes = boxes[keep], scores[keep], classes[keep]\n",
    "    # Then back to xywh boxes for preds and metric computation\n",
    "    boxes[:, 2] -= boxes[:, 0]\n",
    "    boxes[:, 3] -= boxes[:, 1]\n",
    "    \n",
    "    # Isn't this cond auto valid from the beginning filter ?\n",
    "    #keep = scores >= pre_nms_thresh\n",
    "    #boxes, scores, classes = boxes[keep], scores[keep], classes[keep]\n",
    "    return boxes, scores, classes \n",
    "\n",
    "def post_process_batch(\n",
    "    cls_preds, # B x L x C \n",
    "    reg_preds, # B x L x 4\n",
    "    cness_preds, # B x L x 1\n",
    "    input_size\n",
    "): \n",
    "    preds = []\n",
    "    for logits, ltrb, cness in zip(cls_preds, reg_preds, cness_preds):\n",
    "        boxes, scores, classes = post_process(logits, ltrb, cness, input_size)\n",
    "        preds.append({'boxes': boxes, 'scores': scores, 'labels': classes})\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10d1a2-e538-4b4a-9874-da06c893f786",
   "metadata": {},
   "source": [
    "### Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96fb4d70-c871-4804-b6de-d2dd2cbae880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472246b-884d-43ed-9619-785a40b174ec",
   "metadata": {},
   "source": [
    "### Instanciations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf81ff54-3815-491f-8636-a79822706505",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.62s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "tf = T.Compose(\n",
    "    [\n",
    "        T.ToDtype(torch.float, scale=True),\n",
    "        T.RandomCrop(size=(448,448), pad_if_needed=True, fill=0),\n",
    "        T.ConvertBoundingBoxFormat(format='XYWH'),\n",
    "        T.SanitizeBoundingBoxes(),\n",
    "        #T.ToPureTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "coco = COCO(Path('/data')/'XVIEW1'/'xView_train.json')\n",
    "all_ids = set(coco.imgs.keys())\n",
    "ids = []\n",
    "merges = [list(l.values) for l in [label(\"building\", (0, 255, 0), {73})]]\n",
    "for merge in merges:\n",
    "    ids += [id for id in coco.getImgIds(catIds=merge) if id in all_ids]\n",
    "nb_imgs = len(ids)\n",
    "val_start, test_start = int(0.7*nb_imgs), int(0.8*nb_imgs)\n",
    "train_ids, val_ids = ids[:val_start], ids[val_start:test_start]\n",
    "\n",
    "train_set = xView1(\n",
    "    merge='building',\n",
    "    coco_dataset=coco,\n",
    "    root=Path('/data')/'XVIEW1'/'train_images',\n",
    "    ids=train_ids,\n",
    "    transforms=tf\n",
    ")\n",
    "val_set = xView1(\n",
    "    merge='building',\n",
    "    coco_dataset=coco,\n",
    "    root=Path('/data')/'XVIEW1'/'train_images',\n",
    "    ids=val_ids,\n",
    "    transforms=tf\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    dataset=train_set,\n",
    "    sampler=RandomSampler(\n",
    "        train_set,\n",
    "        replacement=True,\n",
    "        num_samples=100*2\n",
    "    ),\n",
    "    drop_last=True,\n",
    "    collate_fn=xView1.collate\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    dataset=val_set,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=xView1.collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "262b6a09-8eaa-496d-a60b-5aa802902cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 10758498 params out of 10758498\n"
     ]
    }
   ],
   "source": [
    "# Freeze params here if needed\n",
    "    \n",
    "#for param in model.feature_extractor.parameters():\n",
    "#    param.requires_grad = False\n",
    "strides=[4, 8, 16, 32]\n",
    "feature_maps_sizes = [(448//s,448//s) for s in strides]\n",
    "anchors, anchor_sizes = get_all_anchors_bb_sizes(\n",
    "    fm_sizes=feature_maps_sizes, # 640/16 * [4,2,1,0.5]\n",
    "    fm_strides=strides,\n",
    "    bb_sizes=[128, 256, 512]\n",
    ")\n",
    "\n",
    "model = ViTDet(num_classes=1, out_channels=256)\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#dev = torch.device(\"cpu\")\n",
    "model.to(dev)\n",
    "eval_losses = LossEvaluator(\n",
    "    num_classes=1\n",
    ")\n",
    "\n",
    "train_params = list(filter(lambda p: p[1].requires_grad, model.named_parameters()))\n",
    "nb_train = sum([int(torch.numel(p[1])) for p in train_params])\n",
    "nb_tot = sum([int(torch.numel(p)) for p in model.parameters()])\n",
    "print(f\"Training {nb_train} params out of {nb_tot}\")\n",
    "\n",
    "#optimizer = torch.optim.SGD(\n",
    "#    params=[p[1] for p in train_params],\n",
    "#    lr=0.005,\n",
    "#    momentum=0.9,\n",
    "#    weight_decay=0.0005\n",
    "#)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-3,\n",
    "    betas=(0.9,0.999),\n",
    "    weight_decay=5e-2,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=100)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "#    optimizer=optimizer,\n",
    "#    start_factor=1.,\n",
    "#    end_factor=0.1,\n",
    "#    total_iters=20\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655752a3-bf41-4efa-a4ca-c99069bb9435",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a13c370-b3be-4342-8ab3-7d172f8b3cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:49<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "valid_loss = 1475.2560201850201\n",
      "valid_cls_loss = 1474.1951662616598\n",
      "valid_reg_loss = 0.6246537880765067\n",
      "valid_cen_loss = 0.4362000334593985\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:42<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "lr = 4.263124248945712e-05\n",
      "train_loss = 9.24427308678627\n",
      "train_cls_loss = 8.153335734307767\n",
      "train_reg_loss = 0.6640194785594941\n",
      "train_cen_loss = 0.4269178706407547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:16<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1\n",
      "valid_loss = 1.744783642805285\n",
      "valid_cls_loss = 0.7593451999127865\n",
      "valid_reg_loss = 0.6007231713996993\n",
      "valid_cen_loss = 0.3847152648700608\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:42<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1\n",
      "lr = 5.0496122303502204e-05\n",
      "train_loss = 2.0289007595181463\n",
      "train_cls_loss = 0.8891513547301293\n",
      "train_reg_loss = 0.6928079789876938\n",
      "train_cen_loss = 0.4469414436817169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:06<00:00, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2\n",
      "valid_loss = 1.508865802238385\n",
      "valid_cls_loss = 0.6395525050659975\n",
      "valid_reg_loss = 0.5221726165877448\n",
      "valid_cen_loss = 0.3471406747897466\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:38<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2\n",
      "lr = 6.350841275071359e-05\n",
      "train_loss = 2.05889381557703\n",
      "train_cls_loss = 0.854083516895771\n",
      "train_reg_loss = 0.7158566337823867\n",
      "train_cen_loss = 0.4889536607265472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:05<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3\n",
      "valid_loss = 1.5560015481379297\n",
      "valid_cls_loss = 0.6418872856431537\n",
      "valid_reg_loss = 0.5302486932939954\n",
      "valid_cen_loss = 0.3838655774792035\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:39<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3\n",
      "lr = 8.15254534498001e-05\n",
      "train_loss = 1.8168608197569847\n",
      "train_cls_loss = 0.7609367647767067\n",
      "train_reg_loss = 0.5995105886459351\n",
      "train_cen_loss = 0.4564134728908539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:06<00:00, 11.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4\n",
      "valid_loss = 1.2202262345494495\n",
      "valid_cls_loss = 0.5125491036516097\n",
      "valid_reg_loss = 0.3980617614256011\n",
      "valid_cen_loss = 0.3096153686443965\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:36<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4\n",
      "lr = 0.00010434971438817134\n",
      "train_loss = 1.651231820732355\n",
      "train_cls_loss = 0.7062670631706714\n",
      "train_reg_loss = 0.5177991700172424\n",
      "train_cen_loss = 0.4271655839681625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:12<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5\n",
      "valid_loss = 1.3434535726490948\n",
      "valid_cls_loss = 0.561612059465713\n",
      "valid_reg_loss = 0.4296385132604175\n",
      "valid_cen_loss = 0.35220298833317226\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:35<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5\n",
      "lr = 0.00013173096154427957\n",
      "train_loss = 1.7014901845157147\n",
      "train_cls_loss = 0.7121558050811291\n",
      "train_reg_loss = 0.5358045542240143\n",
      "train_cen_loss = 0.4535298389196396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:09<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6\n",
      "valid_loss = 1.3382003725402885\n",
      "valid_cls_loss = 0.5651768251425691\n",
      "valid_reg_loss = 0.40236198984914356\n",
      "valid_cen_loss = 0.37066155506504905\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:38<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6\n",
      "lr = 0.0001633690003309099\n",
      "train_loss = 1.8038368101418019\n",
      "train_cls_loss = 0.7273059310019017\n",
      "train_reg_loss = 0.5698357474803925\n",
      "train_cen_loss = 0.50669513463974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:09<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7\n",
      "valid_loss = 1.3271408392530348\n",
      "valid_cls_loss = 0.5120558230620291\n",
      "valid_reg_loss = 0.43687218841579223\n",
      "valid_cen_loss = 0.378212823636002\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:37<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7\n",
      "lr = 0.0001989169667816524\n",
      "train_loss = 1.5243466837704183\n",
      "train_cls_loss = 0.6179986347258091\n",
      "train_reg_loss = 0.4842854505777359\n",
      "train_cen_loss = 0.42206259846687316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:05<00:00, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8\n",
      "valid_loss = 1.3053326343393161\n",
      "valid_cls_loss = 0.4870233992632065\n",
      "valid_reg_loss = 0.4465284314420488\n",
      "valid_cen_loss = 0.37178079701132244\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:36<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8\n",
      "lr = 0.00023798513039759433\n",
      "train_loss = 1.67718686029315\n",
      "train_cls_loss = 0.6711435829102993\n",
      "train_reg_loss = 0.5282270604372025\n",
      "train_cen_loss = 0.47781621396541596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:06<00:00, 11.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9\n",
      "valid_loss = 1.2289362967324753\n",
      "valid_cls_loss = 0.4652687977068126\n",
      "valid_reg_loss = 0.3988022514515453\n",
      "valid_cen_loss = 0.3648652484019597\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:36<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9\n",
      "lr = 0.0002801451669616888\n",
      "train_loss = 1.6131564035266637\n",
      "train_cls_loss = 0.6629585923999548\n",
      "train_reg_loss = 0.49049837768077853\n",
      "train_cen_loss = 0.45969944179058075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:09<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10\n",
      "valid_loss = 1.2087673150623839\n",
      "valid_cls_loss = 0.49131692066374755\n",
      "valid_reg_loss = 0.3678065513571103\n",
      "valid_cen_loss = 0.3496438389023145\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:35<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10\n",
      "lr = 0.00032493485447575006\n",
      "train_loss = 1.863589130192995\n",
      "train_cls_loss = 0.7688201792538166\n",
      "train_reg_loss = 0.5776241517066956\n",
      "train_cen_loss = 0.5171448069810868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:07<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 11\n",
      "valid_loss = 1.2737451032218006\n",
      "valid_cls_loss = 0.5423949867900875\n",
      "valid_reg_loss = 0.38612168033917743\n",
      "valid_cen_loss = 0.3452284344368511\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:34<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 11\n",
      "lr = 0.00037186314073612604\n",
      "train_loss = 1.750725113041699\n",
      "train_cls_loss = 0.7664653842523694\n",
      "train_reg_loss = 0.514029067158699\n",
      "train_cen_loss = 0.4702306747436523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:05<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 12\n",
      "valid_loss = 1.2993738752686315\n",
      "valid_cls_loss = 0.537122479122546\n",
      "valid_reg_loss = 0.4024225084318055\n",
      "valid_cen_loss = 0.359828893509176\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:32<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 12\n",
      "lr = 0.00042041552698962625\n",
      "train_loss = 1.550360903274268\n",
      "train_cls_loss = 0.6560483647696674\n",
      "train_reg_loss = 0.4658178150653839\n",
      "train_cen_loss = 0.42849471867084504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:08<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 13\n",
      "valid_loss = 1.1730653965949185\n",
      "valid_cls_loss = 0.4739370535955661\n",
      "valid_reg_loss = 0.3726374689075682\n",
      "valid_cen_loss = 0.3264908782309956\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:32<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 13\n",
      "lr = 0.00047005970864593945\n",
      "train_loss = 1.626783284265548\n",
      "train_cls_loss = 0.6844435855932534\n",
      "train_reg_loss = 0.49320424020290377\n",
      "train_cen_loss = 0.44913546144962313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 14\n",
      "valid_loss = 1.2111871202683284\n",
      "valid_cls_loss = 0.517009481942902\n",
      "valid_reg_loss = 0.36705590618981254\n",
      "valid_cen_loss = 0.3271217379305098\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:35<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 14\n",
      "lr = 0.0005202514112045305\n",
      "train_loss = 1.6697411048412323\n",
      "train_cls_loss = 0.7262987250089645\n",
      "train_reg_loss = 0.4893845021724701\n",
      "train_cen_loss = 0.4540578788518906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:14<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 15\n",
      "valid_loss = 1.2908360637310479\n",
      "valid_cls_loss = 0.5513988445616431\n",
      "valid_reg_loss = 0.3925357229179806\n",
      "valid_cen_loss = 0.34690148217810524\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:33<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 15\n",
      "lr = 0.000570440357413773\n",
      "train_loss = 1.6648733330518006\n",
      "train_cls_loss = 0.7287253881245852\n",
      "train_reg_loss = 0.483240904211998\n",
      "train_cen_loss = 0.45290704905986784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:11<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 16\n",
      "valid_loss = 1.4431619112276368\n",
      "valid_cls_loss = 0.6079233543326458\n",
      "valid_reg_loss = 0.43873361663685906\n",
      "valid_cen_loss = 0.3965049551592933\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                                                      | 1/100 [00:00<01:07,  1.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m train_cen_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     63\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)):\n\u001b[1;32m     65\u001b[0m     image \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(dev)               \n\u001b[1;32m     66\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mxView1.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     19\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco\u001b[38;5;241m.\u001b[39mloadImgs(idx)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m rasterio\u001b[38;5;241m.\u001b[39mopen(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 21\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m tv_image \u001b[38;5;241m=\u001b[39m tv_tensors\u001b[38;5;241m.\u001b[39mImage(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(image))\n\u001b[1;32m     24\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco\u001b[38;5;241m.\u001b[39mloadAnns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco\u001b[38;5;241m.\u001b[39mgetAnnIds(idx))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "start_epoch = 0\n",
    "for epoch in range(start_epoch, 100):\n",
    "    time_ep = time.time()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_cls_loss = 0\n",
    "    valid_reg_loss = 0\n",
    "    valid_cen_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        map_metric = MeanAveragePrecision(\n",
    "            box_format='xywh', # make sure your dataset outputs target in xywh format\n",
    "            backend='faster_coco_eval'\n",
    "        )\n",
    "        for batch in tqdm(val_dataloader, total=len(val_dataloader)):\n",
    "            cls_tgts, reg_tgts = associate_targets_to_anchors(\n",
    "                batch['target'],\n",
    "                anchors,\n",
    "                anchor_sizes\n",
    "            )\n",
    "            image = batch[\"image\"].to(dev)\n",
    "            cls_logits, bbox_reg, centerness = model(image)\n",
    "            losses = eval_losses(\n",
    "                cls_logits,\n",
    "                bbox_reg,\n",
    "                centerness,\n",
    "                cls_tgts.to(dev),\n",
    "                reg_tgts.to(dev)\n",
    "            )\n",
    "            loss = losses['combined_loss']\n",
    "            valid_loss += loss.detach().item()\n",
    "            valid_cls_loss += losses[\"cls_loss\"].detach().item()\n",
    "            valid_reg_loss += losses[\"reg_loss\"].detach().item()\n",
    "            valid_cen_loss += losses[\"centerness_loss\"].detach().item()\n",
    "            b,c,h,w = image.shape\n",
    "            preds = post_process_batch(\n",
    "                cls_logits.to(\"cpu\"),\n",
    "                bbox_reg.to(\"cpu\"),\n",
    "                centerness.to(\"cpu\"),\n",
    "                (h,w)\n",
    "            )\n",
    "            map_metric.update(preds, batch['target'])\n",
    "        valid_loss /= len(val_dataloader)\n",
    "        valid_cls_loss /= len(val_dataloader)\n",
    "        valid_reg_loss /= len(val_dataloader)\n",
    "        valid_cen_loss /= len(val_dataloader)\n",
    "        mapmetrics = map_metric.compute()\n",
    "        print(f\"{epoch = }\")\n",
    "        print(f\"{valid_loss = }\")\n",
    "        print(f\"{valid_cls_loss = }\")\n",
    "        print(f\"{valid_reg_loss = }\")\n",
    "        print(f\"{valid_cen_loss = }\")\n",
    "        print(pformat(mapmetrics))\n",
    "        map_metric.reset()\n",
    "    train_loss = 0\n",
    "    train_cls_loss = 0\n",
    "    train_reg_loss = 0\n",
    "    train_cen_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "        image = batch[\"image\"].to(dev)               \n",
    "        optimizer.zero_grad()\n",
    "        cls_logits, bbox_reg, centerness = model(image)\n",
    "        cls_tgts, reg_tgts = associate_targets_to_anchors(\n",
    "            batch['target'],\n",
    "            anchors,\n",
    "            anchor_sizes\n",
    "        ) # BxNumAnchors, BxNumAnchorsx4    \n",
    "        losses = eval_losses(\n",
    "            cls_logits,\n",
    "            bbox_reg,\n",
    "            centerness,\n",
    "            cls_tgts.to(dev),\n",
    "            reg_tgts.to(dev)\n",
    "        )\n",
    "        loss = losses['combined_loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        train_loss += loss.detach().item()\n",
    "        train_cls_loss += losses[\"cls_loss\"].detach().item()\n",
    "        train_reg_loss += losses[\"reg_loss\"].detach().item()\n",
    "        train_cen_loss += losses[\"centerness_loss\"].detach().item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_cls_loss /= len(train_dataloader)\n",
    "    train_reg_loss /= len(train_dataloader)\n",
    "    train_cen_loss /= len(train_dataloader)\n",
    "    print(f\"{epoch = }\")\n",
    "    print(f\"lr = {lr_scheduler.get_last_lr()[0]}\"),\n",
    "    print(f\"{train_loss = }\")\n",
    "    print(f\"{train_cls_loss = }\")\n",
    "    print(f\"{train_reg_loss = }\")\n",
    "    print(f\"{train_cen_loss = }\")\n",
    "    \n",
    "    time_ep = time.time() - time_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee43032-e8a9-49f7-9857-2ffbfff51e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_38",
   "language": "python",
   "name": "dl_toolbox_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
