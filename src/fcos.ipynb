{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9374da9c-ecba-49c1-97bc-30bf4e820eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import gc \n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset, RandomSampler\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torchvision\n",
    "from torchvision.ops import box_convert, generalized_box_iou\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelP6P7\n",
    "import timm\n",
    "from timm.layers import resample_abs_pos_embed \n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pformat\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "from dl_toolbox.transforms import NormalizeBB\n",
    "from dl_toolbox.utils import list_of_dicts_to_dict_of_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7c09e-9929-4dc5-8f68-d4201a623ed4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3393812-b4bc-4456-90ba-d765d7be142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = read_image(img_path)\n",
    "        mask = read_image(mask_path)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(masks)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = tv_tensors.Image(img)\n",
    "        h, w = T.functional.get_size(img)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(h,w))\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return {'image': img, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "def collate(batch):\n",
    "    batch = list_of_dicts_to_dict_of_lists(batch)\n",
    "    batch['image'] = torch.stack(batch['image'])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af4706-7663-4545-8917-bbcc28ad7d96",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83f0308-4d1b-469e-af70-81340e14baa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Scale(nn.Module):\n",
    "\n",
    "    def __init__(self, init_value=1.0):\n",
    "        super(Scale, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([init_value]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.scale\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, n_classes, n_share_convs=4, n_feat_levels=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        tower = []\n",
    "        for _ in range(n_share_convs):\n",
    "            tower.append(\n",
    "                nn.Conv2d(in_channels,\n",
    "                          in_channels,\n",
    "                          kernel_size=3,\n",
    "                          stride=1,\n",
    "                          padding=1,\n",
    "                          bias=True))\n",
    "            tower.append(nn.GroupNorm(32, in_channels))\n",
    "            tower.append(nn.ReLU())\n",
    "        self.shared_layers = nn.Sequential(*tower)\n",
    "\n",
    "        self.cls_logits = nn.Conv2d(in_channels,\n",
    "                                    n_classes,\n",
    "                                    kernel_size=3,\n",
    "                                    stride=1,\n",
    "                                    padding=1)\n",
    "        self.bbox_pred = nn.Conv2d(in_channels,\n",
    "                                   4,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1)\n",
    "        self.ctrness = nn.Conv2d(in_channels,\n",
    "                                 1,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1,\n",
    "                                 padding=1)\n",
    "\n",
    "        self.scales = nn.ModuleList([Scale(init_value=1.0) for _ in range(5)])\n",
    "\n",
    "        #cls_tower = []\n",
    "        #bbox_tower = []\n",
    "        #for _ in range(n_share_convs):\n",
    "        #    cls_tower.append(\n",
    "        #        nn.Conv2d(in_channels,\n",
    "        #                  in_channels,\n",
    "        #                  kernel_size=3,\n",
    "        #                  stride=1,\n",
    "        #                  padding=1,\n",
    "        #                  bias=True))\n",
    "        #    cls_tower.append(nn.GroupNorm(32, in_channels))\n",
    "        #    cls_tower.append(nn.ReLU())\n",
    "        #    bbox_tower.append(\n",
    "        #        nn.Conv2d(in_channels,\n",
    "        #                  in_channels,\n",
    "        #                  kernel_size=3,\n",
    "        #                  stride=1,\n",
    "        #                  padding=1,\n",
    "        #                  bias=True))\n",
    "        #    bbox_tower.append(nn.GroupNorm(32, in_channels))\n",
    "        #    bbox_tower.append(nn.ReLU())\n",
    "        #self.cls_layers = nn.Sequential(*cls_tower)\n",
    "        #self.bbox_layers = nn.Sequential(*bbox_tower)\n",
    "#\n",
    "        #self.cls_logits = nn.Conv2d(in_channels,\n",
    "        #                            n_classes,\n",
    "        #                            kernel_size=3,\n",
    "        #                            stride=1,\n",
    "        #                            padding=1)\n",
    "        #self.bbox_pred = nn.Conv2d(in_channels,\n",
    "        #                           4,\n",
    "        #                           kernel_size=3,\n",
    "        #                           stride=1,\n",
    "        #                           padding=1)\n",
    "        #self.ctrness = nn.Conv2d(in_channels,\n",
    "        #                         1,\n",
    "        #                         kernel_size=3,\n",
    "        #                         stride=1,\n",
    "        #                         padding=1)\n",
    "        #self.scales = nn.ModuleList([Scale(init_value=1.0) for _ in range(n_feat_levels)])\n",
    "\n",
    "        # initialize the bias for focal loss\n",
    "        # Not in basara repo but it is done in the paper fcos repo\n",
    "        #prior_prob = 0.01\n",
    "        #bias_value = -math.log((1 - prior_prob) / prior_prob)\n",
    "        #torch.nn.init.constant_(self.cls_logits.bias, bias_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_logits = []\n",
    "        bbox_preds = []\n",
    "        cness_preds = []\n",
    "        for l, features in enumerate(x):\n",
    "            features = self.shared_layers(features)\n",
    "            cls_logits.append(self.cls_logits(features).flatten(-2))\n",
    "            cness_preds.append(self.ctrness(features).flatten(-2))\n",
    "            reg = self.bbox_pred(features)\n",
    "            reg = self.scales[l](reg)\n",
    "            bbox_preds.append(nn.functional.relu(reg).flatten(-2))\n",
    "        #for l, features in enumerate(x):\n",
    "        #    cls_features = self.cls_layers(features) # BxinChannelsxfeatSize\n",
    "        #    bbox_features = self.bbox_layers(features)\n",
    "        #    # This flatten must flatten things in the same order anchors are flattened \n",
    "        #    cls_logits.append(self.cls_logits(cls_features).flatten(-2)) # BxNumClsxHf*Wf\n",
    "        #    cness_preds.append(self.ctrness(bbox_features).flatten(-2)) # Bx1xHf*Wf\n",
    "        #    reg = self.bbox_pred(bbox_features) # Bx4xFeatSize\n",
    "        #    reg = self.scales[l](reg)\n",
    "        #    bbox_preds.append(nn.functional.relu(reg).flatten(-2))\n",
    "        all_logits = torch.cat(cls_logits, dim=-1).permute(0,2,1) # BxNumAnchorsxC\n",
    "        all_box_regs = torch.cat(bbox_preds, dim=-1).permute(0,2,1) # BxNumAnchorsx4\n",
    "        all_cness = torch.cat(cness_preds, dim=-1).permute(0,2,1) # BxNumAnchorsx1\n",
    "        return all_logits, all_box_regs, all_cness\n",
    "    \n",
    "class FCOS(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, input_size):\n",
    "        super().__init__()\n",
    "        self.num_classes = n_classes\n",
    "        self.input_size = input_size\n",
    "        self.feature_extractor = create_feature_extractor(\n",
    "            torchvision.models.convnext_tiny(\n",
    "                weights=torchvision.models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1\n",
    "            ), \n",
    "            {\n",
    "                'features.3.2.add': 'layer2', # 1/8th feat map\n",
    "                'features.5.8.add': 'layer3', # 1/16\n",
    "                'features.7.2.add': 'layer4', # 1/32\n",
    "            }\n",
    "        )        \n",
    "        self.fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list=[192, 384, 768],\n",
    "            out_channels=256,\n",
    "            extra_blocks=LastLevelP6P7(256,256) # adds feature maps at strides 64, 128\n",
    "        )\n",
    "        self.fm_strides = [8, 16, 32, 64, 128] \n",
    "        features = nn.Sequential(self.feature_extractor, self.fpn)\n",
    "        self.head = Head(256, self.num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        feature_maps = list(self.fpn(features).values()) # feature maps from FPN\n",
    "        box_cls, box_regression, centerness = self.head(feature_maps)\n",
    "        return box_cls, box_regression, centerness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae2350-24d5-45dc-a239-aa1223c4fb69",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58abad9-3d22-4433-802b-9242a825c794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LossEvaluator(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(LossEvaluator, self).__init__()\n",
    "        self.centerness_loss_func = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "        self.num_classes = num_classes\n",
    "                \n",
    "    def __call__(self, cls_logits, reg_preds, cness_preds, cls_tgts, reg_tgts):\n",
    "        pos_inds_b, pos_inds_loc = torch.nonzero(cls_tgts > 0, as_tuple=True)\n",
    "        num_pos = len(pos_inds_b)\n",
    "        reg_preds = reg_preds[pos_inds_b, pos_inds_loc, :]\n",
    "        reg_tgts = reg_tgts[pos_inds_b, pos_inds_loc, :]\n",
    "        cness_preds = cness_preds[pos_inds_b, pos_inds_loc, :].squeeze(-1)\n",
    "        cness_tgts = self._compute_centerness_targets(reg_tgts)\n",
    "        cls_loss = self._get_cls_loss(cls_logits, cls_tgts, max(num_pos, 1.))\n",
    "        reg_loss, centerness_loss = 0,0\n",
    "        if num_pos > 0:\n",
    "            reg_loss = self._get_reg_loss(\n",
    "                reg_preds, reg_tgts, cness_tgts)\n",
    "            centerness_loss = self._get_centerness_loss(\n",
    "                cness_preds, cness_tgts, num_pos)\n",
    "        losses = {}\n",
    "        losses[\"cls_loss\"] = cls_loss\n",
    "        losses[\"reg_loss\"] = reg_loss\n",
    "        losses[\"centerness_loss\"] = centerness_loss\n",
    "        losses[\"combined_loss\"] = cls_loss + reg_loss + centerness_loss\n",
    "        return losses\n",
    "    \n",
    "    def _compute_centerness_targets(self, reg_tgts):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            reg_tgts: l, t, r, b values to regress, shape BxNumAx4\n",
    "        Returns:\n",
    "            A tensor of shape BxNumA giving how centered each anchor is for the bbox it must regress\n",
    "        \"\"\"\n",
    "        if len(reg_tgts) == 0:\n",
    "            return reg_tgts.new_zeros(len(reg_tgts))\n",
    "        left_right = reg_tgts[..., [0, 2]]\n",
    "        top_bottom = reg_tgts[..., [1, 3]]\n",
    "        centerness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\\n",
    "                    (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0])\n",
    "        return torch.sqrt(centerness)\n",
    "\n",
    "    def _get_cls_loss(self, cls_preds, cls_targets, num_pos_samples):\n",
    "        \"\"\"\n",
    "        cls_targets takes values in 0...C, 0 only when there is no obj to be detected for the anchor\n",
    "        \"\"\"\n",
    "        onehot = nn.functional.one_hot(cls_targets.long(), self.num_classes+1)[...,1:].float()\n",
    "        cls_loss = torchvision.ops.sigmoid_focal_loss(cls_preds, onehot)\n",
    "        return cls_loss.sum() / num_pos_samples\n",
    "\n",
    "    def _get_reg_loss(self, reg_preds, reg_targets, centerness_targets):\n",
    "        ltrb_preds = reg_preds.reshape(-1, 4)\n",
    "        ltrb_tgts = reg_targets.reshape(-1, 4)\n",
    "        xyxy_preds = torch.cat([-ltrb_preds[:,:2], ltrb_preds[:,2:]], dim=1) \n",
    "        xyxy_tgts = torch.cat([-ltrb_tgts[:,:2], ltrb_tgts[:,2:]], dim=1)\n",
    "        reg_losses = torchvision.ops.distance_box_iou_loss(xyxy_preds, xyxy_tgts, reduction='none')\n",
    "        sum_centerness_targets = centerness_targets.sum()\n",
    "        reg_loss = (reg_losses * centerness_targets).sum() / sum_centerness_targets\n",
    "        return reg_loss\n",
    "\n",
    "    def _get_centerness_loss(self, centerness_preds, centerness_targets,\n",
    "                             num_pos_samples):\n",
    "        centerness_loss = self.centerness_loss_func(centerness_preds,\n",
    "                                                    centerness_targets)\n",
    "        return centerness_loss / num_pos_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c47b3-dd85-422f-b1f4-79c2af239f3b",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96751d23-bad3-438e-aa34-478b6a6faafe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INF = 100000000\n",
    "\n",
    "def get_fm_anchors(h, w, s):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        h, w: height, width of the feat map\n",
    "        s: stride of the featmap = size reduction factor relative to image\n",
    "    Returns:\n",
    "        Tensor NumAnchorsInFeatMap x 2, ordered by column \n",
    "        TODO: check why: DONE: it corresponds to how locs are computed in \n",
    "        https://github.com/tianzhi0549/FCOS/blob/master/fcos_core/modeling/rpn/fcos/fcos.py\n",
    "        When flattening feat maps, we see first the line at H(=y) fixed and W(=x) moving\n",
    "        \n",
    "    \"\"\"\n",
    "    locs_x = [s / 2 + x * s for x in range(w)]\n",
    "    locs_y = [s / 2 + y * s for y in range(h)]\n",
    "    locs = [(x, y) for y in locs_y for x in locs_x] # order !\n",
    "    return torch.tensor(locs)\n",
    "\n",
    "def get_all_anchors_bb_sizes(fm_sizes, fm_strides, bb_sizes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fm_sizes: seq of feature_maps sizes\n",
    "        fm_strides: seq of corresponding strides\n",
    "        bb_sizes: seq of bbox sizes feature maps are associated with, len = len(fm) + 1\n",
    "    Returns:\n",
    "        anchors: list of num_featmaps elem, where each elem indicates the tensor of anchors of size Nx2 in the original image corresponding to each location in the feature map at this level\n",
    "        anchors_bb_sizes: sizes of the bbox each anchor is authorized/supposed to detect\n",
    "    \"\"\"\n",
    "    bb_sizes = [-1] + bb_sizes + [INF]\n",
    "    anchors, anchors_bb_sizes = [], []\n",
    "    for l, ((h,w), s) in enumerate(zip(fm_sizes, fm_strides)):\n",
    "        fm_anchors = get_fm_anchors(h, w, s)\n",
    "        sizes = torch.tensor([bb_sizes[l], bb_sizes[l+1]], dtype=torch.float32)\n",
    "        sizes = sizes.repeat(len(fm_anchors)).view(len(fm_anchors), 2)\n",
    "        anchors.append(fm_anchors)\n",
    "        anchors_bb_sizes.append(sizes)\n",
    "    return torch.cat(anchors, 0), torch.cat(anchors_bb_sizes, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e372b006-de62-412d-af86-c779553f2a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_reg_targets(anchors, bbox):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        anchors: Lx2, anchors coordinates\n",
    "        bbox: tensor of bbox Tx4, format should be xywh\n",
    "    Returns:\n",
    "        reg_tgt: l,t,r,b values to regress for each pair (anchor, bbox)\n",
    "        anchor_in_box: whether anchor is in bbox for each pair (anchor, bbox)\n",
    "    \"\"\"\n",
    "    xs, ys = anchors[:, 0], anchors[:, 1] # L & L, x & y reversed ?? x means position on x-axis\n",
    "    l = xs[:, None] - bbox[:, 0][None] # Lx1 - 1xT -> LxT\n",
    "    t = ys[:, None] - bbox[:, 1][None]\n",
    "    r = bbox[:, 2][None] + bbox[:, 0][None] - xs[:, None]\n",
    "    b = bbox[:, 3][None] + bbox[:, 1][None] - ys[:, None]  \n",
    "    #print(xs[0], ys[0], l[0], t[0], r[0], b[0])\n",
    "    return torch.stack([l, t, r, b], dim=2) # LxTx4\n",
    "\n",
    "def apply_distance_constraints(reg_targets, anchor_sizes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        reg_targets: LxTx4\n",
    "        anchor_bb_sizes: Lx2\n",
    "    Returns:\n",
    "        A LxT tensor where value at (anchor, bbox) is true if the max value to regress at this anchor for this bbox is inside the bounds associated to this anchor\n",
    "        If other values to regress than the max are negatives, it is dealt with anchor_in_boxes.\n",
    "    \"\"\"\n",
    "    max_reg_targets, _ = reg_targets.max(dim=2) # LxT\n",
    "    min_reg_targets, _ = reg_targets.min(dim=2) # LxT\n",
    "    dist_constraints = torch.stack([\n",
    "        min_reg_targets > 0,\n",
    "        max_reg_targets >= anchor_sizes[:, None, 0],\n",
    "        max_reg_targets <= anchor_sizes[:, None, 1]\n",
    "    ])\n",
    "    return torch.all(dist_constraints, dim=0)\n",
    "\n",
    "def anchor_bbox_area(bbox, anchors, fits_to_feature_level):\n",
    "    \"\"\"\n",
    "    Args: bbox is XYWH\n",
    "    Returns: \n",
    "        Tensor LxT where value at (anchor, bbox) is the area of bbox if anchor is in bbox and anchor is associated with bbox of that size\n",
    "        Else INF.\n",
    "    \"\"\"\n",
    "    #bbox_areas = _calc_bbox_area(bbox_targets) # T\n",
    "    bbox_areas = bbox[:, 2] * bbox[:, 3] # T\n",
    "    # area of each target bbox repeated for each loc with inf where the the loc is not \n",
    "    # in the target bbox or if the loc is not at the right level for this bbox size\n",
    "    anchor_bbox_area = bbox_areas[None].repeat(len(anchors), 1) # LxT\n",
    "    anchor_bbox_area[~fits_to_feature_level] = INF\n",
    "    return anchor_bbox_area\n",
    "\n",
    "def associate_targets_to_anchors(targets_batch, anchors, anchors_bb_sizes):\n",
    "    \"\"\"\n",
    "    Associate one target cls/bbox to regress ONLY to each anchor: among the bboxes that contain the anchor and have the right size, pick that of min area.\n",
    "    If no tgt exists for an anchor, the tgt class is 0.\n",
    "    inputs:\n",
    "        targets_batch: list of dict of tv_tensors {'labels':, 'boxes':}; boxes should be in XYWH format\n",
    "        anchors: \n",
    "        anchor_bb_sizes:\n",
    "    outputs:\n",
    "        all class targets: BxNumAnchors\n",
    "        all bbox targets: BxNumAnchorsx4\n",
    "    \"\"\"\n",
    "    all_reg_targets, all_cls_targets = [], []\n",
    "    for targets in targets_batch:\n",
    "        bbox_targets = targets['boxes'] # Tx4, format XYWH\n",
    "        cls_targets = targets['labels'] # T\n",
    "        reg_targets = calculate_reg_targets(\n",
    "            anchors, bbox_targets) # LxTx4, LxT\n",
    "        fits_to_feature_level = apply_distance_constraints(\n",
    "            reg_targets, anchors_bb_sizes) # LxT\n",
    "        locations_to_gt_area = anchor_bbox_area(\n",
    "            bbox_targets, anchors, fits_to_feature_level)\n",
    "        # Core of the anchor/target association\n",
    "        if cls_targets.shape[0]>0:\n",
    "            loc_min_area, loc_min_idxs = locations_to_gt_area.min(dim=1) #L,idx in [0,T-1],T must be>0\n",
    "            reg_targets = reg_targets[range(len(anchors)), loc_min_idxs] # Lx4\n",
    "            cls_targets = cls_targets[loc_min_idxs] # L\n",
    "            cls_targets[loc_min_area == INF] = 0 # 0 is no-obj category\n",
    "        else:\n",
    "            cls_targets = cls_targets.new_zeros((len(anchors),))\n",
    "            reg_targets = reg_targets.new_zeros((len(anchors),4))\n",
    "        all_cls_targets.append(cls_targets)\n",
    "        all_reg_targets.append(reg_targets)\n",
    "    # BxL & BxLx4\n",
    "    return torch.stack(all_cls_targets), torch.stack(all_reg_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfde0d8-2db1-4c7a-8fa9-b86c9c25c907",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Post-processing predictions to boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea21012-a1cc-4b43-b168-ade9619b1f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_nms_thresh=0.3\n",
    "pre_nms_top_n=100000\n",
    "nms_thresh=0.45\n",
    "fpn_post_nms_top_n=50\n",
    "min_size=0\n",
    "\n",
    "def post_process(logits, ltrb, cness, input_size):\n",
    "    probas = logits.sigmoid() # LxC\n",
    "    high_probas = probas > pre_nms_thresh # LxC\n",
    "    # Indices on L and C axis of high prob pairs anchor/class\n",
    "    high_prob_anchors_idx, high_prob_cls = high_probas.nonzero(as_tuple=True) # dim l <= L*C\n",
    "    high_prob_cls += 1 # 0 is for no object\n",
    "    high_prob_ltrb = ltrb[high_prob_anchors_idx] # lx4\n",
    "    high_prob_anchors = anchors[high_prob_anchors_idx] # lx2\n",
    "    # Tensor shape l with values from logits*cness such that logits > pre_nms_thresh \n",
    "    cness_modulated_probas = probas * cness.sigmoid() # LxC\n",
    "    high_prob_scores = cness_modulated_probas[high_probas] # l\n",
    "    # si l est trop longue\n",
    "    if high_probas.sum().item() > pre_nms_top_n:\n",
    "        # Filter the pre_nms_top_n most probable pairs \n",
    "        high_prob_scores, top_k_indices = high_prob_scores.topk(\n",
    "            pre_nms_top_n, sorted=False) \n",
    "        high_prob_cls = high_prob_cls[top_k_indices]\n",
    "        high_prob_ltrb = high_prob_ltrb[top_k_indices]\n",
    "        high_prob_anchors = high_prob_anchors[top_k_indices]\n",
    "\n",
    "    # Rewrites bbox (x0,y0,x1,y1) from reg targets (l,t,r,b) following eq (1) in paper\n",
    "    high_prob_boxes = torch.stack([\n",
    "        high_prob_anchors[:, 0] - high_prob_ltrb[:, 0],\n",
    "        high_prob_anchors[:, 1] - high_prob_ltrb[:, 1],\n",
    "        high_prob_anchors[:, 0] + high_prob_ltrb[:, 2],\n",
    "        high_prob_anchors[:, 1] + high_prob_ltrb[:, 3],\n",
    "    ], dim=1)\n",
    "\n",
    "    high_prob_boxes = torchvision.ops.clip_boxes_to_image(high_prob_boxes, input_size)\n",
    "    big_enough_box_idxs = torchvision.ops.remove_small_boxes(high_prob_boxes, min_size)\n",
    "    boxes = high_prob_boxes[big_enough_box_idxs]\n",
    "    # Why not do that on scores and classes too ? \n",
    "    classes = high_prob_cls[big_enough_box_idxs]\n",
    "    scores = high_prob_scores[big_enough_box_idxs]\n",
    "    #high_prob_scores = torch.sqrt(high_prob_scores) # WHY SQRT ? REmOVED\n",
    "    # NMS expects boxes to be in xyxy format\n",
    "    nms_idxs = torchvision.ops.nms(boxes, scores, nms_thresh)\n",
    "    boxes = boxes[nms_idxs]\n",
    "    scores = scores[nms_idxs]\n",
    "    classes = classes[nms_idxs]\n",
    "    if len(nms_idxs) > fpn_post_nms_top_n:\n",
    "        image_thresh, _ = torch.kthvalue(\n",
    "            scores.cpu(),\n",
    "            len(nms_idxs) - fpn_post_nms_top_n + 1)\n",
    "        keep = scores >= image_thresh.item()\n",
    "        #keep = torch.nonzero(keep).squeeze(1)\n",
    "        boxes, scores, classes = boxes[keep], scores[keep], classes[keep]\n",
    "    # Then back to xywh boxes for preds and metric computation\n",
    "    boxes[:, 2] -= boxes[:, 0]\n",
    "    boxes[:, 3] -= boxes[:, 1]\n",
    "    \n",
    "    # Isn't this cond auto valid from the beginning filter ?\n",
    "    #keep = scores >= pre_nms_thresh\n",
    "    #boxes, scores, classes = boxes[keep], scores[keep], classes[keep]\n",
    "    return boxes, scores, classes \n",
    "\n",
    "def post_process_batch(\n",
    "    cls_preds, # B x L x C \n",
    "    reg_preds, # B x L x 4\n",
    "    cness_preds, # B x L x 1\n",
    "    input_size\n",
    "): \n",
    "    preds = []\n",
    "    for logits, ltrb, cness in zip(cls_preds, reg_preds, cness_preds):\n",
    "        boxes, scores, classes = post_process(logits, ltrb, cness, input_size)\n",
    "        preds.append({'boxes': boxes, 'scores': scores, 'labels': classes})\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10d1a2-e538-4b4a-9874-da06c893f786",
   "metadata": {},
   "source": [
    "### Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96fb4d70-c871-4804-b6de-d2dd2cbae880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472246b-884d-43ed-9619-785a40b174ec",
   "metadata": {},
   "source": [
    "### Instanciations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf81ff54-3815-491f-8636-a79822706505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf = T.Compose(\n",
    "    [\n",
    "        T.ToDtype(torch.float, scale=True),\n",
    "        T.Resize(size=480, max_size=640),\n",
    "        T.RandomCrop(size=(640,640), pad_if_needed=True, fill=0),\n",
    "        T.ConvertBoundingBoxFormat(format='XYWH'),\n",
    "        T.SanitizeBoundingBoxes(),\n",
    "        #T.ToPureTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "dataset_test = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_set = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "val_set = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    dataset=train_set,\n",
    "    sampler=RandomSampler(\n",
    "        train_set,\n",
    "        replacement=True,\n",
    "        num_samples=100*2\n",
    "    ),\n",
    "    drop_last=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    batch_size=5,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    dataset=val_set,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "262b6a09-8eaa-496d-a60b-5aa802902cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 33490027 params out of 33490027\n"
     ]
    }
   ],
   "source": [
    "# Freeze params here if needed\n",
    "    \n",
    "#for param in model.feature_extractor.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "anchors, anchor_sizes = get_all_anchors_bb_sizes(\n",
    "    fm_sizes=[(80,80),(40,40),(20,20),(10,10),(5,5)],\n",
    "    fm_strides=[8, 16, 32, 64, 128],\n",
    "    bb_sizes=[64, 128, 256, 512]\n",
    ")\n",
    "\n",
    "model = FCOS(n_classes=1, input_size=(640,640))\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#dev = torch.device(\"cpu\")\n",
    "model.to(dev)\n",
    "eval_losses = LossEvaluator(\n",
    "    num_classes=1\n",
    ")\n",
    "\n",
    "train_params = list(filter(lambda p: p[1].requires_grad, model.named_parameters()))\n",
    "nb_train = sum([int(torch.numel(p[1])) for p in train_params])\n",
    "nb_tot = sum([int(torch.numel(p)) for p in model.parameters()])\n",
    "print(f\"Training {nb_train} params out of {nb_tot}\")\n",
    "\n",
    "#optimizer = torch.optim.SGD(\n",
    "#    params=[p[1] for p in train_params],\n",
    "#    lr=0.005,\n",
    "#    momentum=0.9,\n",
    "#    weight_decay=0.0005\n",
    "#)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-3,\n",
    "    betas=(0.9,0.999),\n",
    "    weight_decay=5e-2,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=100)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "#    optimizer=optimizer,\n",
    "#    start_factor=1.,\n",
    "#    end_factor=0.1,\n",
    "#    total_iters=20\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655752a3-bf41-4efa-a4ca-c99069bb9435",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a13c370-b3be-4342-8ab3-7d172f8b3cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:07<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "valid_loss = 14.82506217956543\n",
      "valid_cls_loss = 13.09198055267334\n",
      "valid_reg_loss = 1.0352440237998963\n",
      "valid_cen_loss = 0.6978376269340515\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:27<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "lr = 4.263299650727223e-05\n",
      "train_loss = 2.417167649269104\n",
      "train_cls_loss = 0.719734572172165\n",
      "train_reg_loss = 1.0356085968017579\n",
      "train_cen_loss = 0.6618244934082032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1\n",
      "valid_loss = 2.0221211314201355\n",
      "valid_cls_loss = 0.3325942486524582\n",
      "valid_reg_loss = 1.035195004940033\n",
      "valid_cen_loss = 0.6543318688869476\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:27<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1\n",
      "lr = 5.050309990155808e-05\n",
      "train_loss = 1.9198297452926636\n",
      "train_cls_loss = 0.2388559240102768\n",
      "train_reg_loss = 1.0358405208587647\n",
      "train_cen_loss = 0.645133306980133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2\n",
      "valid_loss = 1.920315182209015\n",
      "valid_cls_loss = 0.2506189927458763\n",
      "valid_reg_loss = 1.0346094131469727\n",
      "valid_cen_loss = 0.6350867629051209\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:27<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2\n",
      "lr = 6.35239687047371e-05\n",
      "train_loss = 1.8325779342651367\n",
      "train_cls_loss = 0.18071975752711297\n",
      "train_reg_loss = 1.0350324892997742\n",
      "train_cen_loss = 0.616825704574585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3\n",
      "valid_loss = 1.8712484240531921\n",
      "valid_cls_loss = 0.22511969804763793\n",
      "valid_reg_loss = 1.0339999675750733\n",
      "valid_cen_loss = 0.612128734588623\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:26<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3\n",
      "lr = 8.155275332480708e-05\n",
      "train_loss = 1.7905151915550233\n",
      "train_cls_loss = 0.15047323271632196\n",
      "train_reg_loss = 1.036021852493286\n",
      "train_cen_loss = 0.604020105600357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4\n",
      "valid_loss = 1.8499295949935912\n",
      "valid_cls_loss = 0.20602262988686562\n",
      "valid_reg_loss = 1.032843542098999\n",
      "valid_cen_loss = 0.611063402891159\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:26<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4\n",
      "lr = 0.0001043916632328725\n",
      "train_loss = 1.750505928993225\n",
      "train_cls_loss = 0.12604007415473462\n",
      "train_reg_loss = 1.0296986556053163\n",
      "train_cen_loss = 0.5947671914100647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5\n",
      "valid_loss = 1.846329128742218\n",
      "valid_cls_loss = 0.22150112241506575\n",
      "valid_reg_loss = 1.0132998168468474\n",
      "valid_cen_loss = 0.6115281820297241\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:27<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5\n",
      "lr = 0.00013179013688719224\n",
      "train_loss = 1.5944072461128236\n",
      "train_cls_loss = 0.11711918726563454\n",
      "train_reg_loss = 0.8811774945259094\n",
      "train_cen_loss = 0.5961105728149414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6\n",
      "valid_loss = 1.4950928688049316\n",
      "valid_cls_loss = 0.17357703447341918\n",
      "valid_reg_loss = 0.7125472903251648\n",
      "valid_cen_loss = 0.6089685082435607\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.0041),\n",
      " 'map_50': tensor(0.0376),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.0048),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0090),\n",
      " 'mar_10': tensor(0.0203),\n",
      " 'mar_100': tensor(0.0263),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.0310),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:26<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6\n",
      "lr = 0.0001634475905984478\n",
      "train_loss = 1.3139541459083557\n",
      "train_cls_loss = 0.09564495906233787\n",
      "train_reg_loss = 0.6324558925628662\n",
      "train_cen_loss = 0.5858532929420471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7\n",
      "valid_loss = 1.3440670251846314\n",
      "valid_cls_loss = 0.17379260882735253\n",
      "valid_reg_loss = 0.5676292240619659\n",
      "valid_cen_loss = 0.602645194530487\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.2407),\n",
      " 'map_50': tensor(0.7173),\n",
      " 'map_75': tensor(0.0755),\n",
      " 'map_large': tensor(0.2793),\n",
      " 'map_medium': tensor(0.0169),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1556),\n",
      " 'mar_10': tensor(0.3263),\n",
      " 'mar_100': tensor(0.3346),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3832),\n",
      " 'mar_medium': tensor(0.0800),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:27<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7\n",
      "lr = 0.00019901671617892735\n",
      "train_loss = 1.1946789288520814\n",
      "train_cls_loss = 0.09079403638839721\n",
      "train_reg_loss = 0.5218484050035477\n",
      "train_cen_loss = 0.5820364904403686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8\n",
      "valid_loss = 1.3013135194778442\n",
      "valid_cls_loss = 0.20389912873506547\n",
      "valid_reg_loss = 0.5009279400110245\n",
      "valid_cen_loss = 0.5964864492416382\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.2426),\n",
      " 'map_50': tensor(0.6824),\n",
      " 'map_75': tensor(0.0845),\n",
      " 'map_large': tensor(0.2806),\n",
      " 'map_medium': tensor(0.0492),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1361),\n",
      " 'mar_10': tensor(0.3774),\n",
      " 'mar_100': tensor(0.3835),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.4425),\n",
      " 'mar_medium': tensor(0.0667),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:26<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8\n",
      "lr = 0.00023810729119771383\n",
      "train_loss = 1.1606859064102173\n",
      "train_cls_loss = 0.09730872012674809\n",
      "train_reg_loss = 0.47939042031764983\n",
      "train_cen_loss = 0.5839867722988129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9\n",
      "valid_loss = 1.3110824942588806\n",
      "valid_cls_loss = 0.23785126879811286\n",
      "valid_reg_loss = 0.47374489307403567\n",
      "valid_cen_loss = 0.5994863510131836\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.2586),\n",
      " 'map_50': tensor(0.6904),\n",
      " 'map_75': tensor(0.1205),\n",
      " 'map_large': tensor(0.3040),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1617),\n",
      " 'mar_10': tensor(0.3331),\n",
      " 'mar_100': tensor(0.3361),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3956),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:27<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9\n",
      "lr = 0.00028029046004025805\n",
      "train_loss = 1.076534835100174\n",
      "train_cls_loss = 0.09240377202630043\n",
      "train_reg_loss = 0.3995469397306442\n",
      "train_cen_loss = 0.5845841181278228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10\n",
      "valid_loss = 1.122015082836151\n",
      "valid_cls_loss = 0.13450675159692765\n",
      "valid_reg_loss = 0.3954452157020569\n",
      "valid_cen_loss = 0.5920631289482117\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.3304),\n",
      " 'map_50': tensor(0.7984),\n",
      " 'map_75': tensor(0.1482),\n",
      " 'map_large': tensor(0.3835),\n",
      " 'map_medium': tensor(0.0492),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1895),\n",
      " 'mar_10': tensor(0.4038),\n",
      " 'mar_100': tensor(0.4098),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.4699),\n",
      " 'mar_medium': tensor(0.0933),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|█████████████████████████████████████████████▌                                                                          | 19/50 [00:10<00:17,  1.75it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     82\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 83\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m train_cls_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     85\u001b[0m train_reg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#gc.collect()\n",
    "#torch.cuda.empty_cache()\n",
    "#gc.collect()\n",
    "\n",
    "start_epoch = 0\n",
    "for epoch in range(start_epoch, 100):\n",
    "    time_ep = time.time()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_cls_loss = 0\n",
    "    valid_reg_loss = 0\n",
    "    valid_cen_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        map_metric = MeanAveragePrecision(\n",
    "            box_format='xywh', # make sure your dataset outputs target in xywh format\n",
    "            backend='faster_coco_eval'\n",
    "        )\n",
    "        for batch in tqdm(val_dataloader, total=len(val_dataloader)):\n",
    "            cls_tgts, reg_tgts = associate_targets_to_anchors(\n",
    "                batch['target'],\n",
    "                anchors,\n",
    "                anchor_sizes\n",
    "            )\n",
    "            image = batch[\"image\"].to(dev)\n",
    "            cls_logits, bbox_reg, centerness = model(image)\n",
    "            losses = eval_losses(\n",
    "                cls_logits,\n",
    "                bbox_reg,\n",
    "                centerness,\n",
    "                cls_tgts.to(dev),\n",
    "                reg_tgts.to(dev)\n",
    "            )\n",
    "            loss = losses['combined_loss']\n",
    "            valid_loss += loss.detach().item()\n",
    "            valid_cls_loss += losses[\"cls_loss\"].detach().item()\n",
    "            valid_reg_loss += losses[\"reg_loss\"].detach().item()\n",
    "            valid_cen_loss += losses[\"centerness_loss\"].detach().item()\n",
    "            b,c,h,w = image.shape\n",
    "            preds = post_process_batch(\n",
    "                cls_logits.to(\"cpu\"),\n",
    "                bbox_reg.to(\"cpu\"),\n",
    "                centerness.to(\"cpu\"),\n",
    "                (h,w)\n",
    "            )\n",
    "            map_metric.update(preds, batch['target'])\n",
    "        valid_loss /= len(val_dataloader)\n",
    "        valid_cls_loss /= len(val_dataloader)\n",
    "        valid_reg_loss /= len(val_dataloader)\n",
    "        valid_cen_loss /= len(val_dataloader)\n",
    "        mapmetrics = map_metric.compute()\n",
    "        print(f\"{epoch = }\")\n",
    "        print(f\"{valid_loss = }\")\n",
    "        print(f\"{valid_cls_loss = }\")\n",
    "        print(f\"{valid_reg_loss = }\")\n",
    "        print(f\"{valid_cen_loss = }\")\n",
    "        print(pformat(mapmetrics))\n",
    "        map_metric.reset()\n",
    "    train_loss = 0\n",
    "    train_cls_loss = 0\n",
    "    train_reg_loss = 0\n",
    "    train_cen_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "        image = batch[\"image\"].to(dev)               \n",
    "        optimizer.zero_grad()\n",
    "        cls_logits, bbox_reg, centerness = model(image)\n",
    "        cls_tgts, reg_tgts = associate_targets_to_anchors(\n",
    "            batch['target'],\n",
    "            anchors,\n",
    "            anchor_sizes\n",
    "        ) # BxNumAnchors, BxNumAnchorsx4    \n",
    "        losses = eval_losses(\n",
    "            cls_logits,\n",
    "            bbox_reg,\n",
    "            centerness,\n",
    "            cls_tgts.to(dev),\n",
    "            reg_tgts.to(dev)\n",
    "        )\n",
    "        loss = losses['combined_loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        train_loss += loss.detach().item()\n",
    "        train_cls_loss += losses[\"cls_loss\"].detach().item()\n",
    "        train_reg_loss += losses[\"reg_loss\"].detach().item()\n",
    "        train_cen_loss += losses[\"centerness_loss\"].detach().item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_cls_loss /= len(train_dataloader)\n",
    "    train_reg_loss /= len(train_dataloader)\n",
    "    train_cen_loss /= len(train_dataloader)\n",
    "    print(f\"{epoch = }\")\n",
    "    print(f\"lr = {lr_scheduler.get_last_lr()[0]}\"),\n",
    "    print(f\"{train_loss = }\")\n",
    "    print(f\"{train_cls_loss = }\")\n",
    "    print(f\"{train_reg_loss = }\")\n",
    "    print(f\"{train_cen_loss = }\")\n",
    "    \n",
    "    time_ep = time.time() - time_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f633afe-0349-4224-9d70-2d7c6e034893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_38",
   "language": "python",
   "name": "dl_toolbox_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
