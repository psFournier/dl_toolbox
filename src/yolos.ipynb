{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80247a83-666b-4e1d-9137-785489e8e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import gc \n",
    "from collections import defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset, RandomSampler\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torchvision\n",
    "from torchvision.ops import box_convert, generalized_box_iou\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import v2 as T\n",
    "import timm\n",
    "from timm.layers import resample_abs_pos_embed \n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pformat\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "=from dl_toolbox.transforms import NormalizeBB\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc133752-6f73-432c-a732-ec2b3aa013d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "#def hungarian_matching(pred_cls, pred_boxes, target_cls, target_boxes):\n",
    "def hungarian_matching(pred_logits, pred_boxes, tgt_cls, tgt_boxes):\n",
    "    \"\"\" \n",
    "    Params:\n",
    "        pred_cls: Tensor of dim [B, num_queries, num_classes] with the class logits\n",
    "        pred_boxes: Tensor of dim [B, num_queries, 4] with the pred box coord\n",
    "        target_cls: list (len=batchsize) of Tensors of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth objects in the target) containing the class labels\n",
    "        target_boxes: list (len=batchsize) of Tensors of dim [num_target_boxes, 4] containing the target box coord\n",
    "\n",
    "    Returns:\n",
    "        A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "            - index_i is the indices of the selected predictions (in order)\n",
    "            - index_j is the indices of the corresponding selected targets (in order)\n",
    "        For each batch element, it holds:\n",
    "            len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Also concat the target labels and boxes\n",
    "    tgt_ids = torch.cat(tgt_cls)\n",
    "    tgt_bbox = torch.cat(tgt_boxes)\n",
    "    \n",
    "    # For each query box in the batch, the output proba of all classes\n",
    "    all_query_probs = pred_logits.flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "    # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "    # but approximate it in 1 - proba[target class].\n",
    "    # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "    # For each query box in the batch, the output prob of the classes of all targets of the batch\n",
    "    cost_class = -all_query_probs[:, tgt_ids] # bs*num_q x tot num targets over batch\n",
    "\n",
    "    # Compute the L1 cost between boxes\n",
    "    out_bbox = pred_boxes.flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "    cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1) # bs*num_q x tot num targets over batch\n",
    "\n",
    "    # Compute the giou cost betwen boxes\n",
    "    out_bbox = box_convert(out_bbox, 'cxcywh', 'xyxy')\n",
    "    tgt_bbox = box_convert(tgt_bbox, 'cxcywh', 'xyxy')\n",
    "    cost_giou = -generalized_box_iou(out_bbox, tgt_bbox)\n",
    "\n",
    "    # Final cost matrix\n",
    "    C = 5 * cost_bbox + cost_class + 2 * cost_giou\n",
    "    B, Q = pred_logits.shape[:2]\n",
    "    C = C.view(B, Q, -1).cpu() # bs x num_q x tot num targets over batch\n",
    "\n",
    "    sizes = [len(bbox) for bbox in tgt_boxes] # num_tgt per img \n",
    "\n",
    "    # Finds the minimum cost detection token/target assignment per img\n",
    "    indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "    int64 = lambda x: torch.as_tensor(x, dtype=torch.int64)\n",
    "    return [(int64(i), int64(j)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ecdce1-f26f-456a-86ba-d79cda74d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SetCriterion(nn.Module):\n",
    "    def __init__(self, num_classes, eos_coef):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            eos_coef: relative classification weight applied to the no-object category\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eos_coef = eos_coef\n",
    "        self.weight_dict = {\n",
    "            'loss_ce': 1,\n",
    "            'loss_bbox': 5,\n",
    "            'loss_giou': 2\n",
    "        }\n",
    "        empty_weight = torch.ones(self.num_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "    def loss_labels(self, pred_logits, tgt_cls, matches):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            matches: list of batch_size pairs (I, J) of arrays such that output bbox I[n] must be matched with target bbox J[n]\n",
    "        \"\"\"\n",
    "                \n",
    "        # all N tgt labels are reordered following the matches and concatenated  \n",
    "        reordered_labels = [t[J] for t, (_, J) in zip(tgt_cls, matches)]\n",
    "        reordered_labels = torch.cat(reordered_labels) # Nx1\n",
    "        #print(f'{reordered_labels.shape =}')\n",
    "        \n",
    "        # batch_idxs[i] is the idx in the batch of the img to which the i-th elem in the new order corresponds\n",
    "        batch_idxs = [torch.full_like(pred, i) for i, (pred, _) in enumerate(matches)]\n",
    "        batch_idxs = torch.cat(batch_idxs) # Nx1\n",
    "        \n",
    "        # src_idxs[i] is the idx of the preds for img batch_idxs[i] to which the i-th elem in the new order corresponds\n",
    "        pred_idxs = torch.cat([pred for (pred, _) in matches]) # Nx1\n",
    "        \n",
    "        # target_classes is of shape batch_size x num det tokens, and is num_classes (=no_obj) everywhere, except for each token that is matched to a tgt, where it is the label of the matched tgt\n",
    "        target_classes = torch.full(\n",
    "            pred_logits.shape[:2], #BxNdetTok\n",
    "            self.num_classes, #Filled with num_cls\n",
    "            dtype=torch.int64, \n",
    "            device=pred_logits.device\n",
    "        )\n",
    "        target_classes[(batch_idxs, pred_idxs)] = reordered_labels\n",
    "        loss_ce = nn.functional.cross_entropy(\n",
    "            pred_logits.transpose(1, 2), #BxNclsxd1xd2...\n",
    "            target_classes, #Bxd1xd2...\n",
    "            self.empty_weight\n",
    "        )\n",
    "        \n",
    "        ## If we did as follows, then there would be no incentive for the network to output small logits for non-matched tokens\n",
    "        #reordered_pred_logits = pred_logits[(batch_idxs, pred_idxs)] # NxNcls\n",
    "        #other_loss_ce = F.cross_entropy(\n",
    "        #    reordered_pred_logits,\n",
    "        #    reordered_labels\n",
    "        #)\n",
    "        \n",
    "        losses = {'loss_ce': loss_ce}\n",
    "        return losses\n",
    "\n",
    "    def loss_boxes(self, pred_boxes, tgt_boxes, matches, num_boxes):\n",
    "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
    "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
    "        \"\"\"\n",
    "        reordered_target_boxes = [t[i] for t, (_, i) in zip(tgt_boxes, matches)]\n",
    "        reordered_target_boxes = torch.cat(reordered_target_boxes) # Nx4\n",
    "        #print(f'{reordered_target_boxes.shape =}')\n",
    "        \n",
    "        # batch_idxs[i] is the idx in the batch of the img to which the i-th elem in the new order corresponds\n",
    "        batch_idxs = [torch.full_like(pred, i) for i, (pred, _) in enumerate(matches)]\n",
    "        batch_idxs = torch.cat(batch_idxs) # Nx1\n",
    "        \n",
    "        # src_idxs[i] is the idx of the preds for img batch_idxs[i] to which the i-th elem in the new order corresponds\n",
    "        pred_idxs = torch.cat([pred for (pred, _) in matches]) # Nx1\n",
    "        \n",
    "        #print(f'{pred_boxes.shape =}')\n",
    "        reordered_pred_boxes = pred_boxes[(batch_idxs, pred_idxs)] # Nx4\n",
    "        #print(f'{reordered_pred_boxes.shape =}')\n",
    "\n",
    "        losses = {}\n",
    "        loss_bbox = nn.functional.l1_loss(\n",
    "            reordered_pred_boxes,\n",
    "            reordered_target_boxes,\n",
    "            reduction='none'\n",
    "        )\n",
    "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
    "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
    "            box_convert(reordered_pred_boxes, 'cxcywh', 'xyxy'),\n",
    "            box_convert(reordered_target_boxes, 'cxcywh', 'xyxy')))\n",
    "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
    "        return losses\n",
    "\n",
    "    def forward(self, pred_logits, pred_boxes, tgt_cls, tgt_boxes, matches):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "            outputs: dict of tensors, see the output specification of the model for the format\n",
    "            targets: list of dicts, such that len(targets) == batch_size. The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "            matches: list of batch_size pairs (I, J) of arrays such that for pair (I,J) output bbox I[n] must be matched with target bbox J[n]\n",
    "        \"\"\"\n",
    "        # Compute the average (?) number of target boxes accross all nodes, for normalization purposes\n",
    "        #num_boxes = sum(len(boxes) for boxes in tgt_boxes)\n",
    "        #device = next(iter(outputs.values())).device\n",
    "        #num_boxes = torch.as_tensor(\n",
    "        #    [num_boxes],\n",
    "        #    dtype=torch.float,\n",
    "        #    device=device\n",
    "        #)\n",
    "        num_boxes = sum(len(boxes) for boxes in tgt_boxes)\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        losses.update(self.loss_labels(pred_logits, tgt_cls, matches))\n",
    "        losses.update(self.loss_boxes(pred_boxes, tgt_boxes, matches, float(num_boxes)))\n",
    "        losses['combined_loss'] = sum(losses[k] * self.weight_dict[k] for k in losses.keys() if k in self.weight_dict)\n",
    "        return losses   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095dbb98-18bd-4441-bb63-fc35381f7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def post_process(pred_logits, pred_boxes, size):\n",
    "    prob = nn.functional.softmax(pred_logits, -1) # bxNdetTokxNcls\n",
    "    # Most prob cls (except no-obj: NOOBJ is class Ncls ?) and its score per img per token\n",
    "    scores, labels = prob[..., :-1].max(-1) # bxNdetTok\n",
    "    \n",
    "    #pred_boxes = box_convert(pred_boxes, 'cxcywh', 'xyxy')\n",
    "    # and from relative [0, 1] to absolute [0, height] coordinates\n",
    "    #img_h, img_w = target_sizes.unbind(1)\n",
    "    #scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
    "    h,w = size\n",
    "    scale_fct = torch.tensor([w, h, w, h], device=pred_boxes.device)\n",
    "    boxes = pred_boxes * scale_fct[None, None, :]\n",
    "\n",
    "    results = [{'scores': s, 'labels': l, 'boxes': b}\n",
    "               for s, l, b in zip(scores, labels, boxes)]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84fe0f36-e105-4388-9437-ebf86c346111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "\n",
    "class Detector(nn.Module):\n",
    "    def __init__(self, num_classes, det_token_num=100):\n",
    "        super().__init__()\n",
    "        self.det_token_num = det_token_num\n",
    "        self.num_classes = num_classes\n",
    "        #self.backbone, hidden_dim = small(pretrained=pre_trained)   \n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            'vit_tiny_patch16_224',\n",
    "            pretrained=True,\n",
    "            dynamic_img_size=True #Deals with inputs of other size than pretraining\n",
    "        )\n",
    "        self.embed_dim = self.backbone.embed_dim \n",
    "        \n",
    "        self.finetune_det()\n",
    "        #self.class_embed = MLP(hidden_dim, hidden_dim, num_classes + 1, 3)\n",
    "        #self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
    "        self.class_embed = torchvision.ops.MLP(\n",
    "            self.embed_dim,\n",
    "            [self.embed_dim, self.embed_dim, self.num_classes+1]\n",
    "        ) #Num_classes + 1 to deal with no_obj category\n",
    "        self.bbox_embed = torchvision.ops.MLP(\n",
    "            self.embed_dim,\n",
    "            [self.embed_dim, self.embed_dim, 4]\n",
    "        )\n",
    "        \n",
    "    def finetune_det(self):\n",
    "\n",
    "         #import math\n",
    "         #g = math.pow(self.backbone.pos_embed.size(1) - 1, 0.5)\n",
    "         #if int(g) - g != 0:\n",
    "         #    self.backbone.pos_embed = torch.nn.Parameter(self.backbone.pos_embed[:, 1:, :])\n",
    "\n",
    "        #self.backbone.det_token_num = self.det_token_num\n",
    "        \n",
    "        self.det_token = nn.Parameter(torch.zeros(1, self.det_token_num, self.embed_dim))\n",
    "        self.det_token = nn.init.trunc_normal_(self.det_token, std=.02)\n",
    "        \n",
    "        #self.cls_pos_embed = nn.Parameter(self.backbone.pos_embed[:, 0, :][:,None])\n",
    "        \n",
    "        self.det_pos_embed = nn.Parameter(torch.zeros(1, self.det_token_num, self.embed_dim))\n",
    "        self.det_pos_embed = nn.init.trunc_normal_(self.det_pos_embed, std=.02)\n",
    "        \n",
    "        #patch_pos_embed = self.backbone.pos_embed[:, 1:, :]\n",
    "        #patch_pos_embed = patch_pos_embed.transpose(1,2)\n",
    "        #B, E, Q = patch_pos_embed.shape\n",
    "        #P_H, P_W = self.backbone.img_size[0] // self.backbone.patch_size, self.backbone.img_size[1] // self.backbone.patch_size\n",
    "        #patch_pos_embed = patch_pos_embed.view(B, E, P_H, P_W)\n",
    "        #H, W = img_size\n",
    "        #new_P_H, new_P_W = H//self.backbone.patch_size, W//self.backbone.patch_size\n",
    "        #patch_pos_embed = nn.functional.interpolate(patch_pos_embed, size=(new_P_H,new_P_W), mode='bicubic', align_corners=False)\n",
    "        #self.patch_pos_embed = nn.Parameter(patch_pos_embed.flatten(2).transpose(1, 2))\n",
    "        \n",
    "        #self.backbone.pos_embed = torch.nn.Parameter(torch.cat((cls_pos_embed, patch_pos_embed, det_pos_embed), dim=1))\n",
    "        \n",
    "        #self.backbone.img_size = img_size\n",
    "        #self.backbone.has_mid_pe = False\n",
    "        #self.backbone.use_checkpoint=False\n",
    "        \n",
    "        self.backbone.num_prefix_tokens += self.det_token_num\n",
    "        \n",
    "    #def forward_features(self, x):\n",
    "    #    B, H, W = x.shape[0], x.shape[2], x.shape[3]\n",
    "#\n",
    "    #    x = self.backbone.patch_embed(x)\n",
    "    #    temp_pos_embed = torch.cat((self.cls_pos_embed, self.patch_pos_embed, self.det_pos_embed), dim=1)\n",
    "    #    # interpolate init pe\n",
    "    #    if (temp_pos_embed.shape[1] - 1 - self.det_token_num) != x.shape[1]:\n",
    "    #        temp_pos_embed = self.backbone.InterpolateInitPosEmbed(temp_pos_embed, img_size=(H,W))\n",
    "#\n",
    "    #    cls_tokens = self.backbone.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "    #    det_token = self.det_token.expand(B, -1, -1)\n",
    "    #    x = torch.cat((cls_tokens, x, det_token), dim=1)\n",
    "    #    x = x + temp_pos_embed\n",
    "    #    x = self.backbone.pos_drop(x)\n",
    "#\n",
    "    #    for i in range(len((self.backbone.blocks))):\n",
    "    #        x = self.backbone.blocks[i](x)\n",
    "    #    x = self.backbone.norm(x)\n",
    "#\n",
    "    #    return x[:, -self.det_token_num:, :]\n",
    "    #\n",
    "    #def forward(self, samples: NestedTensor):\n",
    "    #    # import pdb;pdb.set_trace()\n",
    "    #    if isinstance(samples, (list, torch.Tensor)):\n",
    "    #        samples = nested_tensor_from_tensor_list(samples)\n",
    "    #    x = self.forward_features(samples.tensors)\n",
    "    #    # x = x[:, 1:,:]\n",
    "    #    outputs_class = self.class_embed(x)\n",
    "    #    outputs_coord = self.bbox_embed(x).sigmoid()\n",
    "    #    out = {'pred_logits': outputs_class, 'pred_boxes': outputs_coord}\n",
    "    #    return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" This code relies on class_token=True in ViT\n",
    "        \"\"\"\n",
    "        x = self.backbone.patch_embed(x)\n",
    "        \n",
    "        # Inserting position embedding for detection tokens and resampling if dynamic\n",
    "        cls_pos_embed = self.backbone.pos_embed[:, 0, :][:,None] # size 1x1xembed_dim\n",
    "        patch_pos_embed = self.backbone.pos_embed[:, 1:, :] # 1xnum_patchxembed_dim\n",
    "        pos_embed = torch.cat((self.det_pos_embed, cls_pos_embed, patch_pos_embed), dim=1)\n",
    "        if self.backbone.dynamic_img_size:\n",
    "            B, H, W, C = x.shape\n",
    "            pos_embed = resample_abs_pos_embed(\n",
    "                pos_embed,\n",
    "                (H, W),\n",
    "                num_prefix_tokens=self.backbone.num_prefix_tokens,\n",
    "            )\n",
    "            x = x.view(B, -1, C)\n",
    "            \n",
    "        # Inserting detection tokens    \n",
    "        cls_token = self.backbone.cls_token.expand(x.shape[0], -1, -1) \n",
    "        det_token = self.det_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat([det_token, cls_token, x], dim=1)\n",
    "        \n",
    "        # Forward ViT\n",
    "        x += pos_embed\n",
    "        x = self.backbone.pos_drop(x)\n",
    "        x = self.backbone.patch_drop(x)\n",
    "        x = self.backbone.norm_pre(x)\n",
    "        x = self.backbone.blocks(x)\n",
    "        x = self.backbone.norm(x)\n",
    "        \n",
    "        # Extracting processed detection tokens + forward heads\n",
    "        x = x[:,:self.det_token_num,...]\n",
    "        pred_logits = self.class_embed(x)\n",
    "        pred_boxes = self.bbox_embed(x).sigmoid()\n",
    "        \n",
    "        return pred_logits, pred_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab609671-26a6-4c65-975d-7ff2bbf2b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 5905198 params out of 5905198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SetCriterion()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Detector(\n",
    "    num_classes=num_classes,\n",
    "    #pre_trained='/d/pfournie/YOLOS/deit_small_patch16_224-cd65a155.pth',\n",
    "    det_token_num=100,\n",
    "    #backbone_name='small',\n",
    "    #init_pe_size=(560,560)\n",
    ")\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "nb_tot = sum([int(torch.numel(p)) for p in model.parameters()])\n",
    "print(f\"Training {n_parameters} params out of {nb_tot}\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = SetCriterion(num_classes, eos_coef=0.1)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962473af-cccc-4875-a7b8-075ae1b67cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det_token in head\n",
      "det_pos_embed in head\n",
      "class_embed.0.weight in head\n",
      "class_embed.0.bias in head\n",
      "class_embed.3.weight in head\n",
      "class_embed.3.bias in head\n",
      "class_embed.6.weight in head\n",
      "class_embed.6.bias in head\n",
      "bbox_embed.0.weight in head\n",
      "bbox_embed.0.bias in head\n",
      "bbox_embed.3.weight in head\n",
      "bbox_embed.3.bias in head\n",
      "bbox_embed.6.weight in head\n",
      "bbox_embed.6.bias in head\n"
     ]
    }
   ],
   "source": [
    "def build_optimizer(model, lr):\n",
    "    if hasattr(model.backbone, 'no_weight_decay'):\n",
    "        skip = model.backbone.no_weight_decay()\n",
    "    head = []\n",
    "    backbone_decay = []\n",
    "    backbone_no_decay = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"backbone\" not in name and param.requires_grad:\n",
    "            print(f\"{name} in head\")\n",
    "            head.append(param)\n",
    "        if \"backbone\" in name and param.requires_grad:\n",
    "            #print(f'{name} in backbone')\n",
    "            if len(param.shape) == 1 or name.endswith(\".bias\") or name.split('.')[-1] in skip:\n",
    "                backbone_no_decay.append(param)\n",
    "            else:\n",
    "                backbone_decay.append(param)\n",
    "    param_dicts = [\n",
    "        {\"params\": head, \"lr\": lr},\n",
    "        {\"params\": backbone_no_decay, \"weight_decay\": 0., \"lr\": lr/100.},\n",
    "        {\"params\": backbone_decay, \"lr\": lr/100.},\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(param_dicts, weight_decay=0.0001)\n",
    "    return optimizer\n",
    "\n",
    "optimizer = build_optimizer(model, lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7c09e-9929-4dc5-8f68-d4201a623ed4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset & dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9374da9c-ecba-49c1-97bc-30bf4e820eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = read_image(img_path)\n",
    "        mask = read_image(mask_path)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(masks)\n",
    "\n",
    "        # there is only one class: attention différent de fcos, A UNIFORMISER\n",
    "        labels = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = tv_tensors.Image(img)\n",
    "\n",
    "        target = {}\n",
    "        h, w = T.functional.get_size(img)\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(h,w))\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.Tensor([image_id])\n",
    "        #target[\"area\"] = area\n",
    "        #target[\"iscrowd\"] = iscrowd\n",
    "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "            \n",
    "        return {'image': img, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "tf = T.Compose(\n",
    "    [\n",
    "        T.Resize(size=480, max_size=560),\n",
    "        T.RandomCrop(size=(560,560), pad_if_needed=True, fill=0),\n",
    "        T.ToDtype(torch.float, scale=True),\n",
    "        T.SanitizeBoundingBoxes(),\n",
    "        T.ConvertBoundingBoxFormat(format='CXCYWH'),\n",
    "        NormalizeBB(),\n",
    "        #T.ToPureTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "dataset = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "dataset_test = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_set = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "val_set = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "\n",
    "def list_of_dicts_to_dict_of_lists(list_of_dicts):\n",
    "    dict_of_lists = defaultdict(list)\n",
    "    for dct in list_of_dicts:\n",
    "        for key, value in dct.items():\n",
    "            dict_of_lists[key].append(value)\n",
    "    res = dict(dict_of_lists)\n",
    "    return res\n",
    "\n",
    "def collate(batch):\n",
    "    batch = list_of_dicts_to_dict_of_lists(batch)\n",
    "    batch['image'] = torch.stack(batch['image'])\n",
    "    return batch\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    pin_memory=False,\n",
    "    dataset=train_set,\n",
    "    sampler=RandomSampler(\n",
    "        train_set,\n",
    "        #replacement=True,\n",
    "        #num_samples=100*2\n",
    "    ),\n",
    "    drop_last=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    pin_memory=False,\n",
    "    dataset=val_set,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41433fa2-d761-48a2-880c-d470c9930003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "#    optimizer=optimizer,\n",
    "#    start_factor=1.,\n",
    "#    end_factor=0.01,\n",
    "#    total_iters=150*30\n",
    "#)\n",
    "#\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa85d72-d0bb-4725-8083-1a9a438170be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unnorm_bounding_boxes(inpt):\n",
    "    bounding_boxes = inpt.as_subclass(torch.Tensor)\n",
    "    in_dtype = bounding_boxes.dtype\n",
    "    bounding_boxes = bounding_boxes.clone() if bounding_boxes.is_floating_point() else bounding_boxes.float()\n",
    "    whwh = torch.Tensor(inpt.canvas_size).repeat(2).flip(dims=(0,)).to(inpt.device) # canvas_size is H,W hence the flip to WHWH\n",
    "    out_boxes = bounding_boxes*whwh \n",
    "    return tv_tensors.wrap(out_boxes.to(in_dtype), like=inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a13c370-b3be-4342-8ab3-7d172f8b3cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.9999999999999996e-05, 3.9999999999999996e-05, 3.9999999999999996e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00, 11.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:04<00:00,  6.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 14.23it/s]\n",
      " 60%|████████████████████████████████████████████████████████████████████████                                                | 18/30 [00:02<00:01,  6.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 98\u001b[0m\n\u001b[1;32m     96\u001b[0m pred_logits, pred_boxes \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[1;32m     97\u001b[0m targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 98\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m    100\u001b[0m matches \u001b[38;5;241m=\u001b[39m hungarian_matching(\n\u001b[1;32m    101\u001b[0m     pred_logits,\n\u001b[1;32m    102\u001b[0m     pred_boxes,\n\u001b[1;32m    103\u001b[0m     tgt_cls\u001b[38;5;241m=\u001b[39m[tgt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tgt \u001b[38;5;129;01min\u001b[39;00m targets],\n\u001b[1;32m    104\u001b[0m     tgt_boxes\u001b[38;5;241m=\u001b[39m[tgt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tgt \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m    107\u001b[0m     pred_logits,\n\u001b[1;32m    108\u001b[0m     pred_boxes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m     matches\u001b[38;5;241m=\u001b[39mmatches\n\u001b[1;32m    112\u001b[0m )\n",
      "Cell \u001b[0;32mIn[11], line 98\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m pred_logits, pred_boxes \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[1;32m     97\u001b[0m targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 98\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m    100\u001b[0m matches \u001b[38;5;241m=\u001b[39m hungarian_matching(\n\u001b[1;32m    101\u001b[0m     pred_logits,\n\u001b[1;32m    102\u001b[0m     pred_boxes,\n\u001b[1;32m    103\u001b[0m     tgt_cls\u001b[38;5;241m=\u001b[39m[tgt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tgt \u001b[38;5;129;01min\u001b[39;00m targets],\n\u001b[1;32m    104\u001b[0m     tgt_boxes\u001b[38;5;241m=\u001b[39m[tgt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tgt \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m    107\u001b[0m     pred_logits,\n\u001b[1;32m    108\u001b[0m     pred_boxes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m     matches\u001b[38;5;241m=\u001b[39mmatches\n\u001b[1;32m    112\u001b[0m )\n",
      "Cell \u001b[0;32mIn[11], line 98\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m pred_logits, pred_boxes \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[1;32m     97\u001b[0m targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 98\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m    100\u001b[0m matches \u001b[38;5;241m=\u001b[39m hungarian_matching(\n\u001b[1;32m    101\u001b[0m     pred_logits,\n\u001b[1;32m    102\u001b[0m     pred_boxes,\n\u001b[1;32m    103\u001b[0m     tgt_cls\u001b[38;5;241m=\u001b[39m[tgt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tgt \u001b[38;5;129;01min\u001b[39;00m targets],\n\u001b[1;32m    104\u001b[0m     tgt_boxes\u001b[38;5;241m=\u001b[39m[tgt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tgt \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m    107\u001b[0m     pred_logits,\n\u001b[1;32m    108\u001b[0m     pred_boxes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m     matches\u001b[38;5;241m=\u001b[39mmatches\n\u001b[1;32m    112\u001b[0m )\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torchvision/tv_tensors/_tv_tensor.py:77\u001b[0m, in \u001b[0;36mTVTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Like in the base Tensor.__torch_function__ implementation, it's easier to always use\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# DisableTorchFunctionSubclass and then manually re-wrap the output if necessary\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DisableTorchFunctionSubclass():\n\u001b[0;32m---> 77\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m must_return_subclass \u001b[38;5;241m=\u001b[39m _must_return_subclass()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m must_return_subclass \u001b[38;5;129;01mor\u001b[39;00m (func \u001b[38;5;129;01min\u001b[39;00m _FORCE_TORCHFUNCTION_SUBCLASS \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mcls\u001b[39m)):\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# If you're wondering why we need the `isinstance(args[0], cls)` check, remove it and see what fails\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# in test_to_tv_tensor_reference().\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# `args = (a_pure_tensor, an_image)` first. Without this guard, `out` would\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# be wrapped into an `Image`.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x7fc7be446790> (for post_execute):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "gc.collect()\n",
    "#torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "epochs = []\n",
    "valid_losses = []\n",
    "train_losses = []\n",
    "valid_map = []\n",
    "\n",
    "fig, axes = plt.subplots(1,2)\n",
    "dh = display.display(fig, display_id=True)\n",
    "print(lr_scheduler.get_last_lr())\n",
    "for epoch in range(150):\n",
    "    epochs.append(epoch)\n",
    "    time_ep = time.time()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_bbox_loss = 0\n",
    "    valid_giou_loss = 0\n",
    "    valid_ce_loss = 0\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "    with torch.no_grad():\n",
    "        map_metric = MeanAveragePrecision(\n",
    "            box_format='cxcywh', # make sure your dataset outputs target in xywh format\n",
    "            backend='faster_coco_eval'\n",
    "        )\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            image = batch[\"image\"].to(device)\n",
    "            targets = batch['target']\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            pred_logits, pred_boxes = model(image)\n",
    "            \n",
    "            matches = hungarian_matching(\n",
    "                pred_logits,\n",
    "                pred_boxes,\n",
    "                tgt_cls=[tgt[\"labels\"] for tgt in targets],\n",
    "                tgt_boxes=[tgt[\"boxes\"] for tgt in targets]\n",
    "            )\n",
    "            loss_dict = criterion(\n",
    "                pred_logits,\n",
    "                pred_boxes,\n",
    "                tgt_cls=[tgt[\"labels\"] for tgt in targets],\n",
    "                tgt_boxes=[tgt[\"boxes\"] for tgt in targets],\n",
    "                matches=matches\n",
    "            )\n",
    "            loss = loss_dict['combined_loss']\n",
    "            \n",
    "            b,c,h,w = image.shape\n",
    "            results = post_process(\n",
    "                pred_logits,\n",
    "                pred_boxes,\n",
    "                size=(h,w)\n",
    "            )\n",
    "            for t in targets:\n",
    "                t['boxes'] = unnorm_bounding_boxes(t['boxes'])\n",
    "            map_metric.update(results, targets)\n",
    "            \n",
    "            valid_loss += loss.detach().item()\n",
    "            valid_bbox_loss += loss_dict[\"loss_bbox\"].detach().item()\n",
    "            valid_giou_loss += loss_dict[\"loss_giou\"].detach().item()\n",
    "            valid_ce_loss += loss_dict[\"loss_ce\"].detach().item()\n",
    "        valid_loss /= len(val_dataloader)\n",
    "        valid_bbox_loss /= len(val_dataloader)\n",
    "        valid_giou_loss /= len(val_dataloader)\n",
    "        valid_ce_loss /= len(val_dataloader)\n",
    "        mapmetrics = map_metric.compute()\n",
    "        #print(f\"{epoch = }\")\n",
    "        #print(f\"{valid_loss = }\")\n",
    "        #print(f\"{valid_bbox_loss = }\")\n",
    "        #print(f\"{valid_giou_loss = }\")\n",
    "        #print(f\"{valid_ce_loss = }\")\n",
    "        #print(pformat(mapmetrics))\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_map.append(mapmetrics['map'])\n",
    "        map_metric.reset()\n",
    "        \n",
    "    train_loss = 0\n",
    "    train_bbox_loss = 0\n",
    "    train_giou_loss = 0\n",
    "    train_ce_loss = 0\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        image = batch[\"image\"].to(device)\n",
    "        pred_logits, pred_boxes = model(image)\n",
    "        targets = batch['target']\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        matches = hungarian_matching(\n",
    "            pred_logits,\n",
    "            pred_boxes,\n",
    "            tgt_cls=[tgt[\"labels\"] for tgt in targets],\n",
    "            tgt_boxes=[tgt[\"boxes\"] for tgt in targets]\n",
    "        )\n",
    "        loss_dict = criterion(\n",
    "            pred_logits,\n",
    "            pred_boxes,\n",
    "            tgt_cls=[tgt[\"labels\"] for tgt in targets],\n",
    "            tgt_boxes=[tgt[\"boxes\"] for tgt in targets],\n",
    "            matches=matches\n",
    "        )\n",
    "        loss = loss_dict['combined_loss']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        train_loss += loss.detach().item()\n",
    "        train_bbox_loss += loss_dict[\"loss_bbox\"].detach().item()\n",
    "        train_giou_loss += loss_dict[\"loss_giou\"].detach().item()\n",
    "        train_ce_loss += loss_dict[\"loss_ce\"].detach().item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_bbox_loss /= len(train_dataloader)\n",
    "    train_giou_loss /= len(train_dataloader)\n",
    "    train_ce_loss /= len(train_dataloader)\n",
    "    #print(f\"{epoch = }\")\n",
    "    #print(f\"lr = {lr_scheduler.get_last_lr()[0]}\"),\n",
    "    #print(f\"{train_loss = }\")\n",
    "    #print(f\"{train_bbox_loss = }\")\n",
    "    #print(f\"{train_giou_loss = }\")\n",
    "    #print(f\"{train_ce_loss = }\")\n",
    "    #lr_scheduler.step(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    axes[0].plot(epochs, valid_losses, 'r', label='val_loss')\n",
    "    axes[0].plot(epochs, train_losses, 'g', label='train_loss')\n",
    "    axes[1].plot(epochs, valid_map, 'b', label='map')\n",
    "    if epoch==0: axes[0].legend()\n",
    "    dh.update(fig)\n",
    "    time_ep = time.time() - time_ep\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902d635-4a1a-4552-aee8-0c6f389fb757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_38",
   "language": "python",
   "name": "dl_toolbox_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
