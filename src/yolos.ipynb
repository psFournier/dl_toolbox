{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea7c09e-9929-4dc5-8f68-d4201a623ed4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9374da9c-ecba-49c1-97bc-30bf4e820eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = read_image(img_path)\n",
    "        mask = read_image(mask_path)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(masks)\n",
    "\n",
    "        # there is only one class: attention diff√©rent de fcos, A UNIFORMISER\n",
    "        labels = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = tv_tensors.Image(img)\n",
    "\n",
    "        target = {}\n",
    "        h, w = v2.functional.get_size(img)\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(h,w))\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.Tensor([image_id])\n",
    "        #target[\"area\"] = area\n",
    "        #target[\"iscrowd\"] = iscrowd\n",
    "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "            \n",
    "        return {'image': img, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af4706-7663-4545-8917-bbcc28ad7d96",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a83f0308-4d1b-469e-af70-81340e14baa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from timm.layers import resample_abs_pos_embed     \n",
    "\n",
    "    \n",
    "class YOLOS(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        det_token_num,\n",
    "        backbone,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone,\n",
    "            pretrained=True,\n",
    "            dynamic_img_size=True #Deals with inputs of other size than pretraining\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = self.backbone.embed_dim \n",
    "        self.det_token_num = det_token_num\n",
    "        self.add_det_tokens_to_backbone()\n",
    "        self.class_embed = torchvision.ops.MLP(\n",
    "            self.embed_dim,\n",
    "            [self.embed_dim, self.embed_dim, self.num_classes+1]\n",
    "        ) #Num_classes + 1 to deal with no_obj category\n",
    "        self.bbox_embed = torchvision.ops.MLP(\n",
    "            self.embed_dim,\n",
    "            [self.embed_dim, self.embed_dim, 4]\n",
    "        )\n",
    "        \n",
    "    def add_det_tokens_to_backbone(self):\n",
    "        det_token = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                1,\n",
    "                self.det_token_num,\n",
    "                self.backbone.embed_dim\n",
    "            )\n",
    "        )\n",
    "        self.det_token = torch.nn.init.trunc_normal_(\n",
    "            det_token,\n",
    "            std=.02\n",
    "        )\n",
    "        det_pos_embed = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                1,\n",
    "                self.det_token_num,\n",
    "                self.backbone.embed_dim\n",
    "            )\n",
    "        )\n",
    "        self.det_pos_embed = torch.nn.init.trunc_normal_(\n",
    "            det_pos_embed,\n",
    "            std=.02\n",
    "        )\n",
    "        #The ViT needs to know how many input tokens are not for patch embeddings\n",
    "        self.backbone.num_prefix_tokens += self.det_token_num\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" This code relies on class_token=True in ViT\n",
    "        \"\"\"\n",
    "        x = self.backbone.patch_embed(x)\n",
    "        \n",
    "        # Inserting position embedding for detection tokens and resampling if dynamic\n",
    "        cls_pos_embed = self.backbone.pos_embed[:, 0, :][:,None] # size 1x1xembed_dim\n",
    "        patch_pos_embed = self.backbone.pos_embed[:, 1:, :] # 1xnum_patchxembed_dim\n",
    "        pos_embed = torch.cat((self.det_pos_embed, cls_pos_embed, patch_pos_embed), dim=1)\n",
    "        if self.backbone.dynamic_img_size:\n",
    "            B, H, W, C = x.shape\n",
    "            pos_embed = resample_abs_pos_embed(\n",
    "                pos_embed,\n",
    "                (H, W),\n",
    "                num_prefix_tokens=self.backbone.num_prefix_tokens,\n",
    "            )\n",
    "            x = x.view(B, -1, C)\n",
    "            \n",
    "        # Inserting detection tokens    \n",
    "        cls_token = self.backbone.cls_token.expand(x.shape[0], -1, -1) \n",
    "        det_token = self.det_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat([det_token, cls_token, x], dim=1)\n",
    "        \n",
    "        # Forward ViT\n",
    "        x += pos_embed\n",
    "        x = self.backbone.pos_drop(x)\n",
    "        x = self.backbone.patch_drop(x)\n",
    "        x = self.backbone.norm_pre(x)\n",
    "        x = self.backbone.blocks(x)\n",
    "        x = self.backbone.norm(x)\n",
    "        \n",
    "        # Extracting processed detection tokens + forward heads\n",
    "        x = x[:,:self.det_token_num,...]\n",
    "        outputs_class = self.class_embed(x)\n",
    "        outputs_coord = self.bbox_embed(x).sigmoid()\n",
    "        \n",
    "        #out = {'pred_logits': outputs_class, 'pred_boxes': outputs_coord}\n",
    "        return outputs_class, outputs_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae2350-24d5-45dc-a239-aa1223c4fb69",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a58abad9-3d22-4433-802b-9242a825c794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import box_convert, generalized_box_iou\n",
    "\n",
    "class SetCriterion(nn.Module):\n",
    "    def __init__(self, num_classes, eos_coef):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            eos_coef: relative classification weight applied to the no-object category\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eos_coef = eos_coef\n",
    "        self.weight_dict = {\n",
    "            'loss_ce': 1,\n",
    "            'loss_bbox': 5,\n",
    "            'loss_giou': 2\n",
    "        }\n",
    "        empty_weight = torch.ones(self.num_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "    def loss_labels(self, pred_logits, tgt_cls, matches):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            matches: list of batch_size pairs (I, J) of arrays such that output bbox I[n] must be matched with target bbox J[n]\n",
    "        \"\"\"\n",
    "                \n",
    "        # all N tgt labels are reordered following the matches and concatenated  \n",
    "        reordered_labels = [t[J] for t, (_, J) in zip(tgt_cls, matches)]\n",
    "        reordered_labels = torch.cat(reordered_labels) # Nx1\n",
    "        #print(f'{reordered_labels.shape =}')\n",
    "        \n",
    "        # batch_idxs[i] is the idx in the batch of the img to which the i-th elem in the new order corresponds\n",
    "        batch_idxs = [torch.full_like(pred, i) for i, (pred, _) in enumerate(matches)]\n",
    "        batch_idxs = torch.cat(batch_idxs) # Nx1\n",
    "        \n",
    "        # src_idxs[i] is the idx of the preds for img batch_idxs[i] to which the i-th elem in the new order corresponds\n",
    "        pred_idxs = torch.cat([pred for (pred, _) in matches]) # Nx1\n",
    "        \n",
    "        # target_classes is of shape batch_size x num det tokens, and is num_classes (=no_obj) everywhere, except for each token that is matched to a tgt, where it is the label of the matched tgt\n",
    "        target_classes = torch.full(\n",
    "            pred_logits.shape[:2], #BxNdetTok\n",
    "            self.num_classes, #Filled with num_cls\n",
    "            dtype=torch.int64, \n",
    "            device=pred_logits.device\n",
    "        )\n",
    "        target_classes[(batch_idxs, pred_idxs)] = reordered_labels\n",
    "        loss_ce = nn.functional.cross_entropy(\n",
    "            pred_logits.transpose(1, 2), #BxNclsxd1xd2...\n",
    "            target_classes, #Bxd1xd2...\n",
    "            self.empty_weight\n",
    "        )\n",
    "        \n",
    "        ## If we did as follows, then there would be no incentive for the network to output small logits for non-matched tokens\n",
    "        #reordered_pred_logits = pred_logits[(batch_idxs, pred_idxs)] # NxNcls\n",
    "        #other_loss_ce = F.cross_entropy(\n",
    "        #    reordered_pred_logits,\n",
    "        #    reordered_labels\n",
    "        #)\n",
    "        \n",
    "        losses = {'loss_ce': loss_ce}\n",
    "        return losses\n",
    "\n",
    "    def loss_boxes(self, pred_boxes, tgt_boxes, matches, num_boxes):\n",
    "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
    "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
    "        \"\"\"\n",
    "        reordered_target_boxes = [t[i] for t, (_, i) in zip(tgt_boxes, matches)]\n",
    "        reordered_target_boxes = torch.cat(reordered_target_boxes) # Nx4\n",
    "        #print(f'{reordered_target_boxes.shape =}')\n",
    "        \n",
    "        # batch_idxs[i] is the idx in the batch of the img to which the i-th elem in the new order corresponds\n",
    "        batch_idxs = [torch.full_like(pred, i) for i, (pred, _) in enumerate(matches)]\n",
    "        batch_idxs = torch.cat(batch_idxs) # Nx1\n",
    "        \n",
    "        # src_idxs[i] is the idx of the preds for img batch_idxs[i] to which the i-th elem in the new order corresponds\n",
    "        pred_idxs = torch.cat([pred for (pred, _) in matches]) # Nx1\n",
    "        \n",
    "        #print(f'{pred_boxes.shape =}')\n",
    "        reordered_pred_boxes = pred_boxes[(batch_idxs, pred_idxs)] # Nx4\n",
    "        #print(f'{reordered_pred_boxes.shape =}')\n",
    "\n",
    "        losses = {}\n",
    "        loss_bbox = nn.functional.l1_loss(\n",
    "            reordered_pred_boxes,\n",
    "            reordered_target_boxes,\n",
    "            reduction='none'\n",
    "        )\n",
    "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
    "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
    "            box_convert(reordered_pred_boxes, 'cxcywh', 'xyxy'),\n",
    "            box_convert(reordered_target_boxes, 'cxcywh', 'xyxy')))\n",
    "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
    "        return losses\n",
    "\n",
    "    def forward(self, pred_cls, pred_boxes, tgt_cls, tgt_boxes, matches):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "            outputs: dict of tensors, see the output specification of the model for the format\n",
    "            targets: list of dicts, such that len(targets) == batch_size. The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "            matches: list of batch_size pairs (I, J) of arrays such that for pair (I,J) output bbox I[n] must be matched with target bbox J[n]\n",
    "        \"\"\"\n",
    "        # Compute the average (?) number of target boxes accross all nodes, for normalization purposes\n",
    "        #num_boxes = sum(len(boxes) for boxes in tgt_boxes)\n",
    "        #device = next(iter(outputs.values())).device\n",
    "        #num_boxes = torch.as_tensor(\n",
    "        #    [num_boxes],\n",
    "        #    dtype=torch.float,\n",
    "        #    device=device\n",
    "        #)\n",
    "        num_boxes = sum(len(boxes) for boxes in tgt_boxes)\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        losses.update(self.loss_labels(pred_cls, tgt_cls, matches))\n",
    "        losses.update(self.loss_boxes(pred_boxes, tgt_boxes, matches, float(num_boxes)))\n",
    "        losses['combined_loss'] = sum(losses[k] * self.weight_dict[k] for k in losses.keys() if k in self.weight_dict)\n",
    "        return losses   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c47b3-dd85-422f-b1f4-79c2af239f3b",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96751d23-bad3-438e-aa34-478b6a6faafe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "@torch.no_grad()\n",
    "def hungarian_matching(pred_cls, pred_boxes, target_cls, target_boxes):\n",
    "    \"\"\" \n",
    "    Params:\n",
    "        pred_cls: Tensor of dim [B, num_queries, num_classes] with the class logits\n",
    "        pred_boxes: Tensor of dim [B, num_queries, 4] with the pred box coord\n",
    "        target_cls: list (len=batchsize) of Tensors of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth objects in the target) containing the class labels\n",
    "        target_boxes: list (len=batchsize) of Tensors of dim [num_target_boxes, 4] containing the target box coord\n",
    "\n",
    "    Returns:\n",
    "        A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "            - index_i is the indices of the selected predictions (in order)\n",
    "            - index_j is the indices of the corresponding selected targets (in order)\n",
    "        For each batch element, it holds:\n",
    "            len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Also concat the target labels and boxes\n",
    "    tgt_ids = torch.cat(target_cls)\n",
    "    tgt_bbox = torch.cat(target_boxes)\n",
    "    \n",
    "    # For each query box in the batch, the output proba of all classes\n",
    "    all_query_probs = pred_cls.flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "    # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "    # but approximate it in 1 - proba[target class].\n",
    "    # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "    # For each query box in the batch, the output prob of the classes of all targets of the batch\n",
    "    cost_class = -all_query_probs[:, tgt_ids] # bs*num_q x tot num targets over batch\n",
    "\n",
    "    # Compute the L1 cost between boxes\n",
    "    out_bbox = pred_boxes.flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "    cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1) # bs*num_q x tot num targets over batch\n",
    "\n",
    "    # Compute the giou cost betwen boxes\n",
    "    out_bbox = box_convert(out_bbox, 'cxcywh', 'xyxy')\n",
    "    tgt_bbox = box_convert(tgt_bbox, 'cxcywh', 'xyxy')\n",
    "    cost_giou = -generalized_box_iou(out_bbox, tgt_bbox)\n",
    "\n",
    "    # Final cost matrix\n",
    "    C = 5 * cost_bbox + cost_class + 2 * cost_giou\n",
    "    B, Q = pred_cls.shape[:2]\n",
    "    C = C.view(B, Q, -1).cpu() # bs x num_q x tot num targets over batch\n",
    "\n",
    "    sizes = [len(bbox) for bbox in target_boxes] # num_tgt per img \n",
    "\n",
    "    # Finds the minimum cost detection token/target assignment per img\n",
    "    indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "    int64 = lambda x: torch.as_tensor(x, dtype=torch.int64)\n",
    "    return [(int64(i), int64(j)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfde0d8-2db1-4c7a-8fa9-b86c9c25c907",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Post-processing predictions to boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ea21012-a1cc-4b43-b168-ade9619b1f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def post_process(logits, boxes, size):\n",
    "    prob = nn.functional.softmax(logits, -1) # bxNdetTokxNcls\n",
    "    # Most prob cls (except no-obj: NOOBJ is class Ncls ?) and its score per img per token\n",
    "    scores, labels = prob[..., :-1].max(-1) # bxNdetTok\n",
    "    \n",
    "    #boxes = box_convert(boxes, 'cxcywh', 'xyxy')\n",
    "    # and from relative [0, 1] to absolute [0, height] coordinates\n",
    "    #img_h, img_w = target_sizes.unbind(1)\n",
    "    #scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
    "    h,w = size\n",
    "    scale_fct = torch.tensor([w, h, w, h], device=boxes.device)\n",
    "    boxes = boxes * scale_fct[None, None, :]\n",
    "\n",
    "    results = [{'scores': s, 'labels': l, 'boxes': b}\n",
    "               for s, l, b in zip(scores, labels, boxes)]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472246b-884d-43ed-9619-785a40b174ec",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0c25057-cd4b-432c-b08e-e5ae087ab39d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pformat\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import gc \n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.transforms import v2 as T\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96fb4d70-c871-4804-b6de-d2dd2cbae880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf81ff54-3815-491f-8636-a79822706505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dl_toolbox.transforms import NormalizeBB\n",
    "\n",
    "tf = T.Compose(\n",
    "    [\n",
    "        T.Resize(size=480, max_size=560),\n",
    "        T.RandomCrop(size=(560,560), pad_if_needed=True, fill=0),\n",
    "        T.ToDtype(torch.float, scale=True),\n",
    "        T.SanitizeBoundingBoxes(),\n",
    "        T.ConvertBoundingBoxFormat(format='CXCYWH'),\n",
    "        NormalizeBB(),\n",
    "        #T.ToPureTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "dataset = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "dataset_test = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_set = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "val_set = torch.utils.data.Subset(dataset_test, indices[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c02fb07-3820-428b-b221-d6ece8bad95f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def list_of_dicts_to_dict_of_lists(list_of_dicts):\n",
    "    dict_of_lists = defaultdict(list)\n",
    "    for dct in list_of_dicts:\n",
    "        for key, value in dct.items():\n",
    "            dict_of_lists[key].append(value)\n",
    "    res = dict(dict_of_lists)\n",
    "    return res\n",
    "\n",
    "def collate(batch):\n",
    "    batch = list_of_dicts_to_dict_of_lists(batch)\n",
    "    batch['image'] = torch.stack(batch['image'])\n",
    "    return batch\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    batch_size=4,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    dataset=train_set,\n",
    "    sampler=RandomSampler(\n",
    "        train_set,\n",
    "    ),\n",
    "    drop_last=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    dataset=val_set,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6fb65b5-ebc3-414e-a3e3-4570637ca235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class Scheduler:\n",
    "    \"\"\" Parameter Scheduler Base Class\n",
    "    A scheduler base class that can be used to schedule any optimizer parameter groups.\n",
    "\n",
    "    Unlike the builtin PyTorch schedulers, this is intended to be consistently called\n",
    "    * At the END of each epoch, before incrementing the epoch count, to calculate next epoch's value\n",
    "    * At the END of each optimizer update, after incrementing the update count, to calculate next update's value\n",
    "\n",
    "    The schedulers built on this should try to remain as stateless as possible (for simplicity).\n",
    "\n",
    "    This family of schedulers is attempting to avoid the confusion of the meaning of 'last_epoch'\n",
    "    and -1 values for special behaviour. All epoch and update counts must be tracked in the training\n",
    "    code and explicitly passed in to the schedulers on the corresponding step or step_update call.\n",
    "\n",
    "    Based on ideas from:\n",
    "     * https://github.com/pytorch/fairseq/tree/master/fairseq/optim/lr_scheduler\n",
    "     * https://github.com/allenai/allennlp/tree/master/allennlp/training/learning_rate_schedulers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 param_group_field: str,\n",
    "                 noise_range_t=None,\n",
    "                 noise_type='normal',\n",
    "                 noise_pct=0.67,\n",
    "                 noise_std=1.0,\n",
    "                 noise_seed=None,\n",
    "                 initialize: bool = True) -> None:\n",
    "        self.optimizer = optimizer\n",
    "        self.param_group_field = param_group_field\n",
    "        self._initial_param_group_field = f\"initial_{param_group_field}\"\n",
    "        if initialize:\n",
    "            for i, group in enumerate(self.optimizer.param_groups):\n",
    "                if param_group_field not in group:\n",
    "                    raise KeyError(f\"{param_group_field} missing from param_groups[{i}]\")\n",
    "                group.setdefault(self._initial_param_group_field, group[param_group_field])\n",
    "        else:\n",
    "            for i, group in enumerate(self.optimizer.param_groups):\n",
    "                if self._initial_param_group_field not in group:\n",
    "                    raise KeyError(f\"{self._initial_param_group_field} missing from param_groups[{i}]\")\n",
    "        self.base_values = [group[self._initial_param_group_field] for group in self.optimizer.param_groups]\n",
    "        self.metric = None  # any point to having this for all?\n",
    "        self.noise_range_t = noise_range_t\n",
    "        self.noise_pct = noise_pct\n",
    "        self.noise_type = noise_type\n",
    "        self.noise_std = noise_std\n",
    "        self.noise_seed = noise_seed if noise_seed is not None else 42\n",
    "        self.update_groups(self.base_values)\n",
    "\n",
    "    def state_dict(self) -> Dict[str, Any]:\n",
    "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
    "\n",
    "    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n",
    "        self.__dict__.update(state_dict)\n",
    "\n",
    "    def get_epoch_values(self, epoch: int):\n",
    "        return None\n",
    "\n",
    "    def get_update_values(self, num_updates: int):\n",
    "        return None\n",
    "\n",
    "    def step(self, epoch: int, metric: float = None) -> None:\n",
    "        self.metric = metric\n",
    "        values = self.get_epoch_values(epoch)\n",
    "        if values is not None:\n",
    "            values = self._add_noise(values, epoch)\n",
    "            self.update_groups(values)\n",
    "\n",
    "    def step_update(self, num_updates: int, metric: float = None):\n",
    "        self.metric = metric\n",
    "        values = self.get_update_values(num_updates)\n",
    "        if values is not None:\n",
    "            values = self._add_noise(values, num_updates)\n",
    "            self.update_groups(values)\n",
    "\n",
    "    def update_groups(self, values):\n",
    "        if not isinstance(values, (list, tuple)):\n",
    "            values = [values] * len(self.optimizer.param_groups)\n",
    "        for param_group, value in zip(self.optimizer.param_groups, values):\n",
    "            param_group[self.param_group_field] = value\n",
    "\n",
    "    def _add_noise(self, lrs, t):\n",
    "        if self.noise_range_t is not None:\n",
    "            if isinstance(self.noise_range_t, (list, tuple)):\n",
    "                apply_noise = self.noise_range_t[0] <= t < self.noise_range_t[1]\n",
    "            else:\n",
    "                apply_noise = t >= self.noise_range_t\n",
    "            if apply_noise:\n",
    "                g = torch.Generator()\n",
    "                g.manual_seed(self.noise_seed + t)\n",
    "                if self.noise_type == 'normal':\n",
    "                    while True:\n",
    "                        # resample if noise out of percent limit, brute force but shouldn't spin much\n",
    "                        noise = torch.randn(1, generator=g).item()\n",
    "                        if abs(noise) < self.noise_pct:\n",
    "                            break\n",
    "                else:\n",
    "                    noise = 2 * (torch.rand(1, generator=g).item() - 0.5) * self.noise_pct\n",
    "                lrs = [v + v * noise for v in lrs]\n",
    "        return lrs\n",
    "\n",
    "\n",
    "class CosineLRScheduler(Scheduler):\n",
    "    \"\"\"\n",
    "    Cosine decay with restarts.\n",
    "    This is described in the paper https://arxiv.org/abs/1608.03983.\n",
    "\n",
    "    Inspiration from\n",
    "    https://github.com/allenai/allennlp/blob/master/allennlp/training/learning_rate_schedulers/cosine.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 t_initial: int,\n",
    "                 t_mul: float = 1.,\n",
    "                 lr_min: float = 0.,\n",
    "                 decay_rate: float = 1.,\n",
    "                 warmup_t=0,\n",
    "                 warmup_lr_init=0,\n",
    "                 warmup_prefix=False,\n",
    "                 cycle_limit=0,\n",
    "                 t_in_epochs=True,\n",
    "                 noise_range_t=None,\n",
    "                 noise_pct=0.67,\n",
    "                 noise_std=1.0,\n",
    "                 noise_seed=42,\n",
    "                 initialize=True) -> None:\n",
    "        super().__init__(\n",
    "            optimizer, param_group_field=\"lr\",\n",
    "            noise_range_t=noise_range_t, noise_pct=noise_pct, noise_std=noise_std, noise_seed=noise_seed,\n",
    "            initialize=initialize)\n",
    "\n",
    "        assert t_initial > 0\n",
    "        assert lr_min >= 0\n",
    "        if t_initial == 1 and t_mul == 1 and decay_rate == 1:\n",
    "            _logger.warning(\"Cosine annealing scheduler will have no effect on the learning \"\n",
    "                           \"rate since t_initial = t_mul = eta_mul = 1.\")\n",
    "        self.t_initial = t_initial\n",
    "        self.t_mul = t_mul\n",
    "        self.lr_min = lr_min\n",
    "        self.decay_rate = decay_rate\n",
    "        self.cycle_limit = cycle_limit\n",
    "        self.warmup_t = warmup_t\n",
    "        self.warmup_lr_init = warmup_lr_init\n",
    "        self.warmup_prefix = warmup_prefix\n",
    "        self.t_in_epochs = t_in_epochs\n",
    "        if self.warmup_t:\n",
    "            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n",
    "            super().update_groups(self.warmup_lr_init)\n",
    "        else:\n",
    "            self.warmup_steps = [1 for _ in self.base_values]\n",
    "\n",
    "    def _get_lr(self, t):\n",
    "        if t < self.warmup_t:\n",
    "            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n",
    "        else:\n",
    "            if self.warmup_prefix:\n",
    "                t = t - self.warmup_t\n",
    "\n",
    "            if self.t_mul != 1:\n",
    "                i = math.floor(math.log(1 - t / self.t_initial * (1 - self.t_mul), self.t_mul))\n",
    "                t_i = self.t_mul ** i * self.t_initial\n",
    "                t_curr = t - (1 - self.t_mul ** i) / (1 - self.t_mul) * self.t_initial\n",
    "            else:\n",
    "                i = t // self.t_initial\n",
    "                t_i = self.t_initial\n",
    "                t_curr = t - (self.t_initial * i)\n",
    "\n",
    "            gamma = self.decay_rate ** i\n",
    "            lr_min = self.lr_min * gamma\n",
    "            lr_max_values = [v * gamma for v in self.base_values]\n",
    "\n",
    "            if self.cycle_limit == 0 or (self.cycle_limit > 0 and i < self.cycle_limit):\n",
    "                lrs = [\n",
    "                    lr_min + 0.5 * (lr_max - lr_min) * (1 + math.cos(math.pi * t_curr / t_i)) for lr_max in lr_max_values\n",
    "                ]\n",
    "            else:\n",
    "                lrs = [self.lr_min for _ in self.base_values]\n",
    "\n",
    "        return lrs\n",
    "\n",
    "    def get_epoch_values(self, epoch: int):\n",
    "        if self.t_in_epochs:\n",
    "            return self._get_lr(epoch)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_update_values(self, num_updates: int):\n",
    "        if not self.t_in_epochs:\n",
    "            return self._get_lr(num_updates)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_cycle_length(self, cycles=0):\n",
    "        if not cycles:\n",
    "            cycles = self.cycle_limit\n",
    "        cycles = max(1, cycles)\n",
    "        if self.t_mul == 1.0:\n",
    "            return self.t_initial * cycles\n",
    "        else:\n",
    "            return int(math.floor(-self.t_initial * (self.t_mul ** cycles - 1) / (1 - self.t_mul)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "262b6a09-8eaa-496d-a60b-5aa802902cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 5905198 params out of 5905198\n"
     ]
    }
   ],
   "source": [
    "# Freeze params here if needed\n",
    "\n",
    "model = YOLOS(\n",
    "    num_classes=1,\n",
    "    det_token_num=100,\n",
    "    backbone='vit_tiny_patch16_224',\n",
    ")\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#dev = torch.device(\"cpu\")\n",
    "model.to(dev)\n",
    "eval_losses = SetCriterion(\n",
    "    num_classes=1,\n",
    "    eos_coef=0.1\n",
    ")\n",
    "eval_losses.to(dev)\n",
    "\n",
    "#for param in model.feature_extractor.parameters():\n",
    "#    param.requires_grad = False\n",
    "#for n, p in model.named_parameters():\n",
    "#    if not (n.startswith('det') or n.startswith('class_embed') or n.startswith('bbox_embed')):\n",
    "#        p.requires_grad = False\n",
    "\n",
    "train_params = list(filter(lambda p: p[1].requires_grad, model.named_parameters()))\n",
    "nb_train = sum([int(torch.numel(p[1])) for p in train_params])\n",
    "nb_tot = sum([int(torch.numel(p)) for p in model.parameters()])\n",
    "print(f\"Training {nb_train} params out of {nb_tot}\")\n",
    "\n",
    "#optimizer = torch.optim.SGD(\n",
    "#    params=[p[1] for p in train_params],\n",
    "#    lr=0.005,\n",
    "#    momentum=0.9,\n",
    "#    weight_decay=0.0005\n",
    "#)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=[p[1] for p in train_params],\n",
    "    lr=1e-3,\n",
    "    betas=(0.9,0.999),\n",
    "    weight_decay=1e-4,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "#lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#    optimizer,\n",
    "#    max_lr=1e-3,\n",
    "#    steps_per_epoch=len(train_dataloader),\n",
    "#    epochs=100)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "#    optimizer=optimizer,\n",
    "#    start_factor=1.,\n",
    "#    end_factor=0.1,\n",
    "#    total_iters=10000\n",
    "#)\n",
    "\n",
    "lr_scheduler = CosineLRScheduler(\n",
    "    optimizer,\n",
    "    t_initial=150,\n",
    "    t_mul=1.,\n",
    "    lr_min=1e-7,\n",
    "    decay_rate=0.1,\n",
    "    warmup_lr_init=1e-6,\n",
    "    warmup_t=0,\n",
    "    cycle_limit=1,\n",
    "    t_in_epochs=True,\n",
    "    noise_range_t=None,\n",
    "    noise_pct=0.67,\n",
    "    noise_std=1.,\n",
    "    noise_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7aa85d72-d0bb-4725-8083-1a9a438170be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unnorm_bounding_boxes(inpt):\n",
    "    bounding_boxes = inpt.as_subclass(torch.Tensor)\n",
    "    in_dtype = bounding_boxes.dtype\n",
    "    bounding_boxes = bounding_boxes.clone() if bounding_boxes.is_floating_point() else bounding_boxes.float()\n",
    "    whwh = torch.Tensor(inpt.canvas_size).repeat(2).flip(dims=(0,)).to(inpt.device) # canvas_size is H,W hence the flip to WHWH\n",
    "    out_boxes = bounding_boxes*whwh \n",
    "    return tv_tensors.wrap(out_boxes.to(in_dtype), like=inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a13c370-b3be-4342-8ab3-7d172f8b3cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<00:00, 25.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "valid_loss = 21.601595420837402\n",
      "valid_bbox_loss = 1.681334180831909\n",
      "valid_giou_loss = 1.8176218461990357\n",
      "valid_ce_loss = 9.559680948257446\n",
      "{'classes': tensor(0, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                            | 9/30 [00:01<00:03,  6.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m train_ce_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     71\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)):\n\u001b[1;32m     73\u001b[0m     image \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(dev)\n\u001b[1;32m     74\u001b[0m     targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m, in \u001b[0;36mPennFudanDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPNGImages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs[idx])\n\u001b[1;32m     21\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPedMasks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks[idx])\n\u001b[0;32m---> 22\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mread_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m mask \u001b[38;5;241m=\u001b[39m read_image(mask_path)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# instances are encoded as different colors\u001b[39;00m\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torchvision/io/image.py:298\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode, apply_exif_orientation)\u001b[0m\n\u001b[1;32m    296\u001b[0m     _log_api_usage_once(read_image)\n\u001b[1;32m    297\u001b[0m data \u001b[38;5;241m=\u001b[39m read_file(path)\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_exif_orientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapply_exif_orientation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torchvision/io/image.py:269\u001b[0m, in \u001b[0;36mdecode_image\u001b[0;34m(input, mode, apply_exif_orientation)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    268\u001b[0m     _log_api_usage_once(decode_image)\n\u001b[0;32m--> 269\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_exif_orientation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[0;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "#torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "start_epoch = 0\n",
    "for epoch in range(start_epoch, 150):\n",
    "    time_ep = time.time()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_bbox_loss = 0\n",
    "    valid_giou_loss = 0\n",
    "    valid_ce_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        map_metric = MeanAveragePrecision(\n",
    "            box_format='cxcywh', # make sure your dataset outputs target in xywh format\n",
    "            backend='faster_coco_eval'\n",
    "        )\n",
    "        for batch in tqdm(val_dataloader, total=len(val_dataloader)):\n",
    "            image = batch[\"image\"].to(dev)\n",
    "            targets = batch['target']\n",
    "            pred_logits, pred_boxes = model.forward(image)\n",
    "            tgt_cls = [tgt[\"labels\"].to(dev) for tgt in targets]\n",
    "            #tgt_boxes = [normalize_bbox(tgt[\"boxes\"]).to(dev) for tgt in targets]\n",
    "            tgt_boxes = [tgt[\"boxes\"].to(dev) for tgt in targets]\n",
    "            matches = hungarian_matching(\n",
    "                pred_logits,\n",
    "                pred_boxes,\n",
    "                tgt_cls,\n",
    "                tgt_boxes\n",
    "            )\n",
    "            losses = eval_losses(\n",
    "                pred_logits,\n",
    "                pred_boxes,\n",
    "                tgt_cls,\n",
    "                tgt_boxes,\n",
    "                matches\n",
    "            )\n",
    "\n",
    "            loss = losses['combined_loss']\n",
    "            valid_loss += loss.detach().item()\n",
    "            valid_bbox_loss += losses[\"loss_bbox\"].detach().item()\n",
    "            valid_giou_loss += losses[\"loss_giou\"].detach().item()\n",
    "            valid_ce_loss += losses[\"loss_ce\"].detach().item()\n",
    "            b,c,h,w = image.shape\n",
    "            preds = post_process(\n",
    "                pred_logits.to(\"cpu\"),\n",
    "                pred_boxes.to(\"cpu\"),\n",
    "                (h,w)\n",
    "            )\n",
    "            for t in batch['target']:\n",
    "                t['boxes'] = unnorm_bounding_boxes(t['boxes'])\n",
    "            map_metric.update(preds, batch['target'])\n",
    "        valid_loss /= len(val_dataloader)\n",
    "        valid_bbox_loss /= len(val_dataloader)\n",
    "        valid_giou_loss /= len(val_dataloader)\n",
    "        valid_ce_loss /= len(val_dataloader)\n",
    "        mapmetrics = map_metric.compute()\n",
    "        print(f\"{epoch = }\")\n",
    "        print(f\"{valid_loss = }\")\n",
    "        print(f\"{valid_bbox_loss = }\")\n",
    "        print(f\"{valid_giou_loss = }\")\n",
    "        print(f\"{valid_ce_loss = }\")\n",
    "        print(pformat(mapmetrics))\n",
    "        map_metric.reset()\n",
    "        \n",
    "    train_loss = 0\n",
    "    train_bbox_loss = 0\n",
    "    train_giou_loss = 0\n",
    "    train_ce_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "        image = batch[\"image\"].to(dev)\n",
    "        targets = batch['target']\n",
    "        pred_logits, pred_boxes = model.forward(image)\n",
    "        tgt_cls = [tgt[\"labels\"].to(dev) for tgt in targets]\n",
    "        #tgt_boxes = [normalize_bbox(tgt[\"boxes\"]).to(dev) for tgt in targets]\n",
    "        tgt_boxes = [tgt[\"boxes\"].to(dev) for tgt in targets]\n",
    "        matches = hungarian_matching(\n",
    "            pred_logits,\n",
    "            pred_boxes,\n",
    "            tgt_cls,\n",
    "            tgt_boxes\n",
    "        )\n",
    "        losses = eval_losses(\n",
    "            pred_logits,\n",
    "            pred_boxes,\n",
    "            tgt_cls,\n",
    "            tgt_boxes,\n",
    "            matches\n",
    "        )\n",
    "        loss = losses['combined_loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #lr_scheduler.step()\n",
    "        train_loss += loss.detach().item()\n",
    "        train_bbox_loss += losses[\"loss_bbox\"].detach().item()\n",
    "        train_giou_loss += losses[\"loss_giou\"].detach().item()\n",
    "        train_ce_loss += losses[\"loss_ce\"].detach().item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_bbox_loss /= len(train_dataloader)\n",
    "    train_giou_loss /= len(train_dataloader)\n",
    "    train_ce_loss /= len(train_dataloader)\n",
    "    print(f\"{epoch = }\")\n",
    "    #print(f\"lr = {lr_scheduler.get_last_lr()[0]}\"),\n",
    "    print(f\"{train_loss = }\")\n",
    "    print(f\"{train_bbox_loss = }\")\n",
    "    print(f\"{train_giou_loss = }\")\n",
    "    print(f\"{train_ce_loss = }\")\n",
    "    lr_scheduler.step(epoch)\n",
    "    time_ep = time.time() - time_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0e07e-5732-48ef-8b02-6d2967a998b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8bd61-96a6-4021-9df3-b9067a5481f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_38",
   "language": "python",
   "name": "dl_toolbox_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
