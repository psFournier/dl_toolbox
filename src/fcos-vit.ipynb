{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9374da9c-ecba-49c1-97bc-30bf4e820eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import gc \n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Subset, RandomSampler\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torchvision\n",
    "from torchvision.ops import box_convert, generalized_box_iou\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelP6P7, ExtraFPNBlock\n",
    "from torchvision.ops.misc import Conv2dNormActivation\n",
    "import timm\n",
    "from timm.layers import resample_abs_pos_embed \n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pformat\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "from dl_toolbox.transforms import NormalizeBB\n",
    "from dl_toolbox.utils import list_of_dicts_to_dict_of_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70a27c-fad7-44ad-9724-4fe987a941a2",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5eb7083-dd77-43bf-8fe8-d08a5f6f2323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Scale(nn.Module):\n",
    "\n",
    "    def __init__(self, init_value=1.0):\n",
    "        super(Scale, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([init_value]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.scale\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, n_classes, n_share_convs=4, n_feat_levels=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        tower = []\n",
    "        for _ in range(n_share_convs):\n",
    "            tower.append(\n",
    "                nn.Conv2d(in_channels,\n",
    "                          in_channels,\n",
    "                          kernel_size=3,\n",
    "                          stride=1,\n",
    "                          padding=1,\n",
    "                          bias=True))\n",
    "            tower.append(nn.GroupNorm(32, in_channels))\n",
    "            tower.append(nn.ReLU())\n",
    "        self.shared_layers = nn.Sequential(*tower)\n",
    "\n",
    "        self.cls_logits = nn.Conv2d(in_channels,\n",
    "                                    n_classes,\n",
    "                                    kernel_size=3,\n",
    "                                    stride=1,\n",
    "                                    padding=1)\n",
    "        self.bbox_pred = nn.Conv2d(in_channels,\n",
    "                                   4,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1)\n",
    "        self.ctrness = nn.Conv2d(in_channels,\n",
    "                                 1,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1,\n",
    "                                 padding=1)\n",
    "\n",
    "        self.scales = nn.ModuleList([Scale(init_value=1.0) for _ in range(n_feat_levels)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_logits = []\n",
    "        bbox_preds = []\n",
    "        cness_preds = []\n",
    "        for l, features in enumerate(x):\n",
    "            features = self.shared_layers(features)\n",
    "            cls_logits.append(self.cls_logits(features).flatten(-2))\n",
    "            cness_preds.append(self.ctrness(features).flatten(-2))\n",
    "            reg = self.bbox_pred(features)\n",
    "            reg = self.scales[l](reg)\n",
    "            bbox_preds.append(nn.functional.relu(reg).flatten(-2))\n",
    "        all_logits = torch.cat(cls_logits, dim=-1).permute(0,2,1) # BxNumAnchorsxC\n",
    "        all_box_regs = torch.cat(bbox_preds, dim=-1).permute(0,2,1) # BxNumAnchorsx4\n",
    "        all_cness = torch.cat(cness_preds, dim=-1).permute(0,2,1) # BxNumAnchorsx1\n",
    "        return all_logits, all_box_regs, all_cness\n",
    "\n",
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    \"\"\" LayerNorm for channels of '2D' spatial NCHW tensors \"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6, affine=True):\n",
    "        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.layer_norm(\n",
    "            x.permute(0, 2, 3, 1), self.normalized_shape, self.weight, self.bias, self.eps).permute(0, 3, 1, 2)\n",
    "\n",
    "class SimpleFeaturePyramidNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Module that adds a Simple FPN from on top of a set of feature maps. This is based on\n",
    "    `\"Exploring Plain Vision Transformer Backbones for Object Detection\" <https://arxiv.org/abs/2203.16527>`_.\n",
    "\n",
    "    Unlike regular FPN, Simple FPN expects a single feature map,\n",
    "    on which the Simple FPN will be added.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of channels for the input feature map that\n",
    "            is passed to the module\n",
    "        out_channels (int): number of channels of the Simple FPN representation\n",
    "        extra_blocks (ExtraFPNBlock or None): if provided, extra operations will\n",
    "            be performed. It is expected to take the fpn features, the original\n",
    "            features and the names of the original features as input, and returns\n",
    "            a new list of feature maps and their corresponding names\n",
    "        norm_layer (callable, optional): Module specifying the normalization layer to use. Default: LayerNorm\n",
    "\n",
    "    Examples::\n",
    "    \n",
    "        >>> vitdet = ViTDet(256, 10)\n",
    "        >>> x = torch.rand(2, 3, 224, 224)\n",
    "        >>> feat_dict = vitdet.forward_feat(x)\n",
    "        >>> features = list(feat_dict.values())\n",
    "        >>> print(f'{[f.shape for f in features] = }')\n",
    "        >>> box_cls, box_regression, centerness = vitdet.head(features)\n",
    "        >>> print(f'{box_cls.shape = }')\n",
    "        >>> assert sum([f.shape[2]*f.shape[3] for f in features])==box_cls.shape[1]\n",
    "\n",
    "        DOES NOT WORK BELOW\n",
    "        >>> m = torchvision.ops.SimpleFeaturePyramidNetwork(10, 5)\n",
    "        >>> # get some dummy data\n",
    "        >>> x = torch.rand(1, 10, 64, 64)\n",
    "        >>> # compute the Simple FPN on top of x\n",
    "        >>> output = m(x)\n",
    "        >>> print([(k, v.shape) for k, v in output.items()])\n",
    "        >>> # returns\n",
    "        >>>   [('feat0', torch.Size([1, 5, 64, 64])),\n",
    "        >>>    ('feat2', torch.Size([1, 5, 16, 16])),\n",
    "        >>>    ('feat3', torch.Size([1, 5, 8, 8]))]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        extra_blocks: Optional[ExtraFPNBlock] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for block_index in range(0,4):\n",
    "            layers = []\n",
    "            current_in_channels = in_channels\n",
    "            if block_index == 0:\n",
    "                layers.extend([\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels,\n",
    "                        in_channels // 2,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                    norm_layer(in_channels // 2),\n",
    "                    nn.GELU(),\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels // 2,\n",
    "                        in_channels // 4,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                ])\n",
    "                current_in_channels = in_channels // 4\n",
    "            elif block_index == 1:\n",
    "                layers.append(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels,\n",
    "                        in_channels // 2,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                )\n",
    "                current_in_channels = in_channels // 2\n",
    "            elif block_index == 2:\n",
    "                # nothing to do for this scale\n",
    "                pass\n",
    "            elif block_index == 3:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "            layers.extend([\n",
    "                Conv2dNormActivation(\n",
    "                    current_in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=1,\n",
    "                    padding=0,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=None\n",
    "                ),\n",
    "                Conv2dNormActivation(\n",
    "                    out_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=None\n",
    "                )\n",
    "            ])\n",
    "            self.blocks.append(nn.Sequential(*layers))\n",
    "\n",
    "        if extra_blocks is not None:\n",
    "            if not isinstance(extra_blocks, ExtraFPNBlock):\n",
    "                raise TypeError(f\"extra_blocks should be of type ExtraFPNBlock not {type(extra_blocks)}\")\n",
    "        self.extra_blocks = extra_blocks\n",
    "\n",
    "    def forward(self, x: Tensor) -> Dict[str, Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the Simple FPN for a feature map.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): input feature map.\n",
    "\n",
    "        Returns:\n",
    "            results (list[Tensor]): feature maps after FPN layers.\n",
    "                They are ordered from highest resolution first.\n",
    "        \"\"\"\n",
    "        results = [block(x) for block in self.blocks]\n",
    "        names = [f\"{i}\" for i in range(len(self.blocks))]\n",
    "\n",
    "        if self.extra_blocks is not None:\n",
    "            results, names = self.extra_blocks(results, [x], names)\n",
    "\n",
    "        # make it back an OrderedDict\n",
    "        out = OrderedDict([(k, v) for k, v in zip(names, results)])\n",
    "\n",
    "        return out\n",
    "    \n",
    "class ViTDet(nn.Module):\n",
    "    \n",
    "    def __init__(self, out_channels, num_classes):\n",
    "        super(ViTDet, self).__init__()\n",
    "        #self.backbone = timm.create_model('samvit_base_patch16.sa1b', pretrained=True)\n",
    "        self.backbone = timm.create_model(\n",
    "            'vit_tiny_patch16_224',\n",
    "            pretrained=True,\n",
    "            dynamic_img_size=True #Deals with inputs of other size than pretraining\n",
    "        )\n",
    "        self.sfpn = SimpleFeaturePyramidNetwork(\n",
    "            in_channels=192,\n",
    "            out_channels=out_channels,\n",
    "            #extra_blocks=LastLevelP6P7(out_channels,out_channels),\n",
    "            norm_layer=LayerNorm2d\n",
    "        )\n",
    "        self.head = Head(out_channels, num_classes, n_feat_levels=4) # 6=4+2extrablocks\n",
    "        \n",
    "    def forward_feat(self, x):\n",
    "        intermediates = self.backbone.forward_intermediates(x, indices=1, norm=False, intermediates_only=True)\n",
    "        features = self.sfpn(intermediates[0])\n",
    "        return features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat_dict = self.forward_feat(x)\n",
    "        features = list(feat_dict.values())\n",
    "        box_cls, box_regression, centerness = self.head(features)\n",
    "        return box_cls, box_regression, centerness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7c09e-9929-4dc5-8f68-d4201a623ed4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3393812-b4bc-4456-90ba-d765d7be142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = read_image(img_path)\n",
    "        mask = read_image(mask_path)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(masks)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = tv_tensors.Image(img)\n",
    "        h, w = T.functional.get_size(img)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(h,w))\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return {'image': img, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "def collate(batch):\n",
    "    batch = list_of_dicts_to_dict_of_lists(batch)\n",
    "    batch['image'] = torch.stack(batch['image'])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae2350-24d5-45dc-a239-aa1223c4fb69",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58abad9-3d22-4433-802b-9242a825c794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LossEvaluator(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(LossEvaluator, self).__init__()\n",
    "        self.centerness_loss_func = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "        self.num_classes = num_classes\n",
    "                \n",
    "    def __call__(self, cls_logits, reg_preds, cness_preds, cls_tgts, reg_tgts):\n",
    "        pos_inds_b, pos_inds_loc = torch.nonzero(cls_tgts > 0, as_tuple=True)\n",
    "        num_pos = len(pos_inds_b)\n",
    "        reg_preds = reg_preds[pos_inds_b, pos_inds_loc, :]\n",
    "        reg_tgts = reg_tgts[pos_inds_b, pos_inds_loc, :]\n",
    "        cness_preds = cness_preds[pos_inds_b, pos_inds_loc, :].squeeze(-1)\n",
    "        cness_tgts = self._compute_centerness_targets(reg_tgts)\n",
    "        cls_loss = self._get_cls_loss(cls_logits, cls_tgts, max(num_pos, 1.))\n",
    "        reg_loss, centerness_loss = 0,0\n",
    "        if num_pos > 0:\n",
    "            reg_loss = self._get_reg_loss(\n",
    "                reg_preds, reg_tgts, cness_tgts)\n",
    "            centerness_loss = self._get_centerness_loss(\n",
    "                cness_preds, cness_tgts, num_pos)\n",
    "        losses = {}\n",
    "        losses[\"cls_loss\"] = cls_loss\n",
    "        losses[\"reg_loss\"] = reg_loss\n",
    "        losses[\"centerness_loss\"] = centerness_loss\n",
    "        losses[\"combined_loss\"] = cls_loss + reg_loss + centerness_loss\n",
    "        return losses\n",
    "    \n",
    "    def _compute_centerness_targets(self, reg_tgts):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            reg_tgts: l, t, r, b values to regress, shape BxNumAx4\n",
    "        Returns:\n",
    "            A tensor of shape BxNumA giving how centered each anchor is for the bbox it must regress\n",
    "        \"\"\"\n",
    "        if len(reg_tgts) == 0:\n",
    "            return reg_tgts.new_zeros(len(reg_tgts))\n",
    "        left_right = reg_tgts[..., [0, 2]]\n",
    "        top_bottom = reg_tgts[..., [1, 3]]\n",
    "        centerness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\\n",
    "                    (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0])\n",
    "        return torch.sqrt(centerness)\n",
    "\n",
    "    def _get_cls_loss(self, cls_preds, cls_targets, num_pos_samples):\n",
    "        \"\"\"\n",
    "        cls_targets takes values in 0...C, 0 only when there is no obj to be detected for the anchor\n",
    "        \"\"\"\n",
    "        onehot = nn.functional.one_hot(cls_targets.long(), self.num_classes+1)[...,1:].float()\n",
    "        cls_loss = torchvision.ops.sigmoid_focal_loss(cls_preds, onehot)\n",
    "        return cls_loss.sum() / num_pos_samples\n",
    "\n",
    "    def _get_reg_loss(self, reg_preds, reg_targets, centerness_targets):\n",
    "        ltrb_preds = reg_preds.reshape(-1, 4)\n",
    "        ltrb_tgts = reg_targets.reshape(-1, 4)\n",
    "        xyxy_preds = torch.cat([-ltrb_preds[:,:2], ltrb_preds[:,2:]], dim=1) \n",
    "        xyxy_tgts = torch.cat([-ltrb_tgts[:,:2], ltrb_tgts[:,2:]], dim=1)\n",
    "        reg_losses = torchvision.ops.distance_box_iou_loss(xyxy_preds, xyxy_tgts, reduction='none')\n",
    "        sum_centerness_targets = centerness_targets.sum()\n",
    "        reg_loss = (reg_losses * centerness_targets).sum() / sum_centerness_targets\n",
    "        return reg_loss\n",
    "\n",
    "    def _get_centerness_loss(self, centerness_preds, centerness_targets,\n",
    "                             num_pos_samples):\n",
    "        centerness_loss = self.centerness_loss_func(centerness_preds,\n",
    "                                                    centerness_targets)\n",
    "        return centerness_loss / num_pos_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c47b3-dd85-422f-b1f4-79c2af239f3b",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96751d23-bad3-438e-aa34-478b6a6faafe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INF = 100000000\n",
    "\n",
    "def get_fm_anchors(h, w, s):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        h, w: height, width of the feat map\n",
    "        s: stride of the featmap = size reduction factor relative to image\n",
    "    Returns:\n",
    "        Tensor NumAnchorsInFeatMap x 2, ordered by column \n",
    "        TODO: check why: DONE: it corresponds to how locs are computed in \n",
    "        https://github.com/tianzhi0549/FCOS/blob/master/fcos_core/modeling/rpn/fcos/fcos.py\n",
    "        When flattening feat maps, we see first the line at H(=y) fixed and W(=x) moving\n",
    "        \n",
    "    \"\"\"\n",
    "    locs_x = [s / 2 + x * s for x in range(w)]\n",
    "    locs_y = [s / 2 + y * s for y in range(h)]\n",
    "    locs = [(x, y) for y in locs_y for x in locs_x] # order !\n",
    "    return torch.tensor(locs)\n",
    "\n",
    "def get_all_anchors_bb_sizes(fm_sizes, fm_strides, bb_sizes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fm_sizes: seq of feature_maps sizes\n",
    "        fm_strides: seq of corresponding strides\n",
    "        bb_sizes: seq of bbox sizes feature maps are associated with, len = len(fm) + 1\n",
    "    Returns:\n",
    "        anchors: list of num_featmaps elem, where each elem indicates the tensor of anchors of size Nx2 in the original image corresponding to each location in the feature map at this level\n",
    "        anchors_bb_sizes: sizes of the bbox each anchor is authorized/supposed to detect\n",
    "    \"\"\"\n",
    "    bb_sizes = [-1] + bb_sizes + [INF]\n",
    "    anchors, anchors_bb_sizes = [], []\n",
    "    for l, ((h,w), s) in enumerate(zip(fm_sizes, fm_strides)):\n",
    "        fm_anchors = get_fm_anchors(h, w, s)\n",
    "        sizes = torch.tensor([bb_sizes[l], bb_sizes[l+1]], dtype=torch.float32)\n",
    "        sizes = sizes.repeat(len(fm_anchors)).view(len(fm_anchors), 2)\n",
    "        anchors.append(fm_anchors)\n",
    "        anchors_bb_sizes.append(sizes)\n",
    "    return torch.cat(anchors, 0), torch.cat(anchors_bb_sizes, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e372b006-de62-412d-af86-c779553f2a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_reg_targets(anchors, bbox):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        anchors: Lx2, anchors coordinates\n",
    "        bbox: tensor of bbox Tx4, format should be xywh\n",
    "    Returns:\n",
    "        reg_tgt: l,t,r,b values to regress for each pair (anchor, bbox)\n",
    "        anchor_in_box: whether anchor is in bbox for each pair (anchor, bbox)\n",
    "    \"\"\"\n",
    "    xs, ys = anchors[:, 0], anchors[:, 1] # L & L, x & y reversed ?? x means position on x-axis\n",
    "    l = xs[:, None] - bbox[:, 0][None] # Lx1 - 1xT -> LxT\n",
    "    t = ys[:, None] - bbox[:, 1][None]\n",
    "    r = bbox[:, 2][None] + bbox[:, 0][None] - xs[:, None]\n",
    "    b = bbox[:, 3][None] + bbox[:, 1][None] - ys[:, None]  \n",
    "    #print(xs[0], ys[0], l[0], t[0], r[0], b[0])\n",
    "    return torch.stack([l, t, r, b], dim=2) # LxTx4\n",
    "\n",
    "def apply_distance_constraints(reg_targets, anchor_sizes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        reg_targets: LxTx4\n",
    "        anchor_bb_sizes: Lx2\n",
    "    Returns:\n",
    "        A LxT tensor where value at (anchor, bbox) is true if the max value to regress at this anchor for this bbox is inside the bounds associated to this anchor\n",
    "        If other values to regress than the max are negatives, it is dealt with anchor_in_boxes.\n",
    "    \"\"\"\n",
    "    max_reg_targets, _ = reg_targets.max(dim=2) # LxT\n",
    "    min_reg_targets, _ = reg_targets.min(dim=2) # LxT\n",
    "    dist_constraints = torch.stack([\n",
    "        min_reg_targets > 0,\n",
    "        max_reg_targets >= anchor_sizes[:, None, 0],\n",
    "        max_reg_targets <= anchor_sizes[:, None, 1]\n",
    "    ])\n",
    "    return torch.all(dist_constraints, dim=0)\n",
    "\n",
    "def anchor_bbox_area(bbox, anchors, fits_to_feature_level):\n",
    "    \"\"\"\n",
    "    Args: bbox is XYWH\n",
    "    Returns: \n",
    "        Tensor LxT where value at (anchor, bbox) is the area of bbox if anchor is in bbox and anchor is associated with bbox of that size\n",
    "        Else INF.\n",
    "    \"\"\"\n",
    "    #bbox_areas = _calc_bbox_area(bbox_targets) # T\n",
    "    bbox_areas = bbox[:, 2] * bbox[:, 3] # T\n",
    "    # area of each target bbox repeated for each loc with inf where the the loc is not \n",
    "    # in the target bbox or if the loc is not at the right level for this bbox size\n",
    "    anchor_bbox_area = bbox_areas[None].repeat(len(anchors), 1) # LxT\n",
    "    anchor_bbox_area[~fits_to_feature_level] = INF\n",
    "    return anchor_bbox_area\n",
    "\n",
    "def associate_targets_to_anchors(targets_batch, anchors, anchors_bb_sizes):\n",
    "    \"\"\"\n",
    "    Associate one target cls/bbox to regress ONLY to each anchor: among the bboxes that contain the anchor and have the right size, pick that of min area.\n",
    "    If no tgt exists for an anchor, the tgt class is 0.\n",
    "    inputs:\n",
    "        targets_batch: list of dict of tv_tensors {'labels':, 'boxes':}; boxes should be in XYWH format\n",
    "        anchors: \n",
    "        anchor_bb_sizes:\n",
    "    outputs:\n",
    "        all class targets: BxNumAnchors\n",
    "        all bbox targets: BxNumAnchorsx4\n",
    "    \"\"\"\n",
    "    all_reg_targets, all_cls_targets = [], []\n",
    "    for targets in targets_batch:\n",
    "        bbox_targets = targets['boxes'] # Tx4, format XYWH\n",
    "        cls_targets = targets['labels'] # T\n",
    "        reg_targets = calculate_reg_targets(\n",
    "            anchors, bbox_targets) # LxTx4, LxT\n",
    "        fits_to_feature_level = apply_distance_constraints(\n",
    "            reg_targets, anchors_bb_sizes) # LxT\n",
    "        locations_to_gt_area = anchor_bbox_area(\n",
    "            bbox_targets, anchors, fits_to_feature_level)\n",
    "        # Core of the anchor/target association\n",
    "        if cls_targets.shape[0]>0:\n",
    "            loc_min_area, loc_min_idxs = locations_to_gt_area.min(dim=1) #L,idx in [0,T-1],T must be>0\n",
    "            reg_targets = reg_targets[range(len(anchors)), loc_min_idxs] # Lx4\n",
    "            cls_targets = cls_targets[loc_min_idxs] # L\n",
    "            cls_targets[loc_min_area == INF] = 0 # 0 is no-obj category\n",
    "        else:\n",
    "            cls_targets = cls_targets.new_zeros((len(anchors),))\n",
    "            reg_targets = reg_targets.new_zeros((len(anchors),4))\n",
    "        all_cls_targets.append(cls_targets)\n",
    "        all_reg_targets.append(reg_targets)\n",
    "    # BxL & BxLx4\n",
    "    return torch.stack(all_cls_targets), torch.stack(all_reg_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfde0d8-2db1-4c7a-8fa9-b86c9c25c907",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Post-processing predictions to boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea21012-a1cc-4b43-b168-ade9619b1f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_nms_thresh=0.3\n",
    "pre_nms_top_n=100000\n",
    "nms_thresh=0.45\n",
    "fpn_post_nms_top_n=50\n",
    "min_size=0\n",
    "\n",
    "def post_process(logits, ltrb, cness, input_size):\n",
    "    probas = logits.sigmoid() # LxC\n",
    "    high_probas = probas > pre_nms_thresh # LxC\n",
    "    # Indices on L and C axis of high prob pairs anchor/class\n",
    "    high_prob_anchors_idx, high_prob_cls = high_probas.nonzero(as_tuple=True) # dim l <= L*C\n",
    "    high_prob_cls += 1 # 0 is for no object\n",
    "    high_prob_ltrb = ltrb[high_prob_anchors_idx] # lx4\n",
    "    high_prob_anchors = anchors[high_prob_anchors_idx] # lx2\n",
    "    # Tensor shape l with values from logits*cness such that logits > pre_nms_thresh \n",
    "    cness_modulated_probas = probas * cness.sigmoid() # LxC\n",
    "    high_prob_scores = cness_modulated_probas[high_probas] # l\n",
    "    # si l est trop longue\n",
    "    if high_probas.sum().item() > pre_nms_top_n:\n",
    "        # Filter the pre_nms_top_n most probable pairs \n",
    "        high_prob_scores, top_k_indices = high_prob_scores.topk(\n",
    "            pre_nms_top_n, sorted=False) \n",
    "        high_prob_cls = high_prob_cls[top_k_indices]\n",
    "        high_prob_ltrb = high_prob_ltrb[top_k_indices]\n",
    "        high_prob_anchors = high_prob_anchors[top_k_indices]\n",
    "\n",
    "    # Rewrites bbox (x0,y0,x1,y1) from reg targets (l,t,r,b) following eq (1) in paper\n",
    "    high_prob_boxes = torch.stack([\n",
    "        high_prob_anchors[:, 0] - high_prob_ltrb[:, 0],\n",
    "        high_prob_anchors[:, 1] - high_prob_ltrb[:, 1],\n",
    "        high_prob_anchors[:, 0] + high_prob_ltrb[:, 2],\n",
    "        high_prob_anchors[:, 1] + high_prob_ltrb[:, 3],\n",
    "    ], dim=1)\n",
    "\n",
    "    high_prob_boxes = torchvision.ops.clip_boxes_to_image(high_prob_boxes, input_size)\n",
    "    big_enough_box_idxs = torchvision.ops.remove_small_boxes(high_prob_boxes, min_size)\n",
    "    boxes = high_prob_boxes[big_enough_box_idxs]\n",
    "    # Why not do that on scores and classes too ? \n",
    "    classes = high_prob_cls[big_enough_box_idxs]\n",
    "    scores = high_prob_scores[big_enough_box_idxs]\n",
    "    #high_prob_scores = torch.sqrt(high_prob_scores) # WHY SQRT ? REmOVED\n",
    "    # NMS expects boxes to be in xyxy format\n",
    "    nms_idxs = torchvision.ops.nms(boxes, scores, nms_thresh)\n",
    "    boxes = boxes[nms_idxs]\n",
    "    scores = scores[nms_idxs]\n",
    "    classes = classes[nms_idxs]\n",
    "    if len(nms_idxs) > fpn_post_nms_top_n:\n",
    "        image_thresh, _ = torch.kthvalue(\n",
    "            scores.cpu(),\n",
    "            len(nms_idxs) - fpn_post_nms_top_n + 1)\n",
    "        keep = scores >= image_thresh.item()\n",
    "        #keep = torch.nonzero(keep).squeeze(1)\n",
    "        boxes, scores, classes = boxes[keep], scores[keep], classes[keep]\n",
    "    # Then back to xywh boxes for preds and metric computation\n",
    "    boxes[:, 2] -= boxes[:, 0]\n",
    "    boxes[:, 3] -= boxes[:, 1]\n",
    "    \n",
    "    # Isn't this cond auto valid from the beginning filter ?\n",
    "    #keep = scores >= pre_nms_thresh\n",
    "    #boxes, scores, classes = boxes[keep], scores[keep], classes[keep]\n",
    "    return boxes, scores, classes \n",
    "\n",
    "def post_process_batch(\n",
    "    cls_preds, # B x L x C \n",
    "    reg_preds, # B x L x 4\n",
    "    cness_preds, # B x L x 1\n",
    "    input_size\n",
    "): \n",
    "    preds = []\n",
    "    for logits, ltrb, cness in zip(cls_preds, reg_preds, cness_preds):\n",
    "        boxes, scores, classes = post_process(logits, ltrb, cness, input_size)\n",
    "        preds.append({'boxes': boxes, 'scores': scores, 'labels': classes})\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10d1a2-e538-4b4a-9874-da06c893f786",
   "metadata": {},
   "source": [
    "### Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96fb4d70-c871-4804-b6de-d2dd2cbae880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472246b-884d-43ed-9619-785a40b174ec",
   "metadata": {},
   "source": [
    "### Instanciations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf81ff54-3815-491f-8636-a79822706505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf = T.Compose(\n",
    "    [\n",
    "        T.ToDtype(torch.float, scale=True),\n",
    "        T.Resize(size=480, max_size=640),\n",
    "        T.RandomCrop(size=(640,640), pad_if_needed=True, fill=0),\n",
    "        T.ConvertBoundingBoxFormat(format='XYWH'),\n",
    "        T.SanitizeBoundingBoxes(),\n",
    "        #T.ToPureTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "dataset_test = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_set = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "val_set = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    dataset=train_set,\n",
    "    sampler=RandomSampler(\n",
    "        train_set,\n",
    "        replacement=True,\n",
    "        num_samples=100*2\n",
    "    ),\n",
    "    drop_last=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    dataset=val_set,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "262b6a09-8eaa-496d-a60b-5aa802902cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 10758498 params out of 10758498\n"
     ]
    }
   ],
   "source": [
    "# Freeze params here if needed\n",
    "    \n",
    "#for param in model.feature_extractor.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "anchors, anchor_sizes = get_all_anchors_bb_sizes(\n",
    "    fm_sizes=[(160,160),(80,80),(40,40),(20,20)], # 640/16 * [4,2,1,0.5]\n",
    "    fm_strides=[4, 8, 16, 32],\n",
    "    bb_sizes=[128, 256, 512]\n",
    ")\n",
    "\n",
    "model = ViTDet(num_classes=1, out_channels=256)\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#dev = torch.device(\"cpu\")\n",
    "model.to(dev)\n",
    "eval_losses = LossEvaluator(\n",
    "    num_classes=1\n",
    ")\n",
    "\n",
    "train_params = list(filter(lambda p: p[1].requires_grad, model.named_parameters()))\n",
    "nb_train = sum([int(torch.numel(p[1])) for p in train_params])\n",
    "nb_tot = sum([int(torch.numel(p)) for p in model.parameters()])\n",
    "print(f\"Training {nb_train} params out of {nb_tot}\")\n",
    "\n",
    "#optimizer = torch.optim.SGD(\n",
    "#    params=[p[1] for p in train_params],\n",
    "#    lr=0.005,\n",
    "#    momentum=0.9,\n",
    "#    weight_decay=0.0005\n",
    "#)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-3,\n",
    "    betas=(0.9,0.999),\n",
    "    weight_decay=5e-2,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=100)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "#    optimizer=optimizer,\n",
    "#    start_factor=1.,\n",
    "#    end_factor=0.1,\n",
    "#    total_iters=20\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655752a3-bf41-4efa-a4ca-c99069bb9435",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a13c370-b3be-4342-8ab3-7d172f8b3cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [02:07<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "valid_loss = 4.571698150634766\n",
      "valid_cls_loss = 2.847186561822891\n",
      "valid_reg_loss = 1.035345914363861\n",
      "valid_cen_loss = 0.6891656732559204\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "lr = 4.263124248945712e-05\n",
      "train_loss = 1.925172724723816\n",
      "train_cls_loss = 0.25651481792330744\n",
      "train_reg_loss = 1.0360094320774078\n",
      "train_cen_loss = 0.6326484853029251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1\n",
      "valid_loss = 1.8794851756095887\n",
      "valid_cls_loss = 0.2243960040807724\n",
      "valid_reg_loss = 1.0351521921157838\n",
      "valid_cen_loss = 0.6199369716644287\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1\n",
      "lr = 5.0496122303502204e-05\n",
      "train_loss = 1.8235097312927246\n",
      "train_cls_loss = 0.17545499101281167\n",
      "train_reg_loss = 1.035438940525055\n",
      "train_cen_loss = 0.6126158010959625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 17.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2\n",
      "valid_loss = 1.8315304684638978\n",
      "valid_cls_loss = 0.18818149268627166\n",
      "valid_reg_loss = 1.0329590463638305\n",
      "valid_cen_loss = 0.6103899216651917\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2\n",
      "lr = 6.350841275071359e-05\n",
      "train_loss = 1.7543774116039277\n",
      "train_cls_loss = 0.12793579868972302\n",
      "train_reg_loss = 1.023951560854912\n",
      "train_cen_loss = 0.6024900585412979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 17.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3\n",
      "valid_loss = 1.7496209692955018\n",
      "valid_cls_loss = 0.17222889468073846\n",
      "valid_reg_loss = 0.9525289130210877\n",
      "valid_cen_loss = 0.6248631536960602\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3\n",
      "lr = 8.15254534498001e-05\n",
      "train_loss = 1.454387345314026\n",
      "train_cls_loss = 0.13858818411827087\n",
      "train_reg_loss = 0.701485076546669\n",
      "train_cen_loss = 0.6143140828609467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4\n",
      "valid_loss = 1.4371985626220702\n",
      "valid_cls_loss = 0.19637031227350235\n",
      "valid_reg_loss = 0.6270934653282165\n",
      "valid_cen_loss = 0.6137347793579102\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1165),\n",
      " 'map_50': tensor(0.4101),\n",
      " 'map_75': tensor(0.0236),\n",
      " 'map_large': tensor(0.1352),\n",
      " 'map_medium': tensor(0.0070),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1008),\n",
      " 'mar_10': tensor(0.2038),\n",
      " 'mar_100': tensor(0.2226),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.2602),\n",
      " 'mar_medium': tensor(0.0133),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4\n",
      "lr = 0.00010434971438817134\n",
      "train_loss = 1.3314146542549132\n",
      "train_cls_loss = 0.14372816152870654\n",
      "train_reg_loss = 0.5811896812915802\n",
      "train_cen_loss = 0.606496816277504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5\n",
      "valid_loss = 1.3767595672607422\n",
      "valid_cls_loss = 0.19601856648921967\n",
      "valid_reg_loss = 0.5675552064180374\n",
      "valid_cen_loss = 0.6131857991218567\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1889),\n",
      " 'map_50': tensor(0.5159),\n",
      " 'map_75': tensor(0.0912),\n",
      " 'map_large': tensor(0.2210),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1391),\n",
      " 'mar_10': tensor(0.2609),\n",
      " 'mar_100': tensor(0.2759),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3248),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5\n",
      "lr = 0.00013173096154427957\n",
      "train_loss = 1.2579604482650757\n",
      "train_cls_loss = 0.11972391538321972\n",
      "train_reg_loss = 0.5404035338759422\n",
      "train_cen_loss = 0.5978329914808274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6\n",
      "valid_loss = 1.3872157740592956\n",
      "valid_cls_loss = 0.22290661588311195\n",
      "valid_reg_loss = 0.5534544396400451\n",
      "valid_cen_loss = 0.610854719877243\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1463),\n",
      " 'map_50': tensor(0.5051),\n",
      " 'map_75': tensor(0.0186),\n",
      " 'map_large': tensor(0.1713),\n",
      " 'map_medium': tensor(0.0010),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1180),\n",
      " 'mar_10': tensor(0.2481),\n",
      " 'mar_100': tensor(0.2624),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3062),\n",
      " 'mar_medium': tensor(0.0200),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6\n",
      "lr = 0.0001633690003309099\n",
      "train_loss = 1.2394319820404052\n",
      "train_cls_loss = 0.12574273340404032\n",
      "train_reg_loss = 0.5138686603307724\n",
      "train_cen_loss = 0.5998205834627152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7\n",
      "valid_loss = 1.3253236246109008\n",
      "valid_cls_loss = 0.18801102370023728\n",
      "valid_reg_loss = 0.5298266327381134\n",
      "valid_cen_loss = 0.6074859631061554\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.2286),\n",
      " 'map_50': tensor(0.7047),\n",
      " 'map_75': tensor(0.0809),\n",
      " 'map_large': tensor(0.2677),\n",
      " 'map_medium': tensor(0.0016),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1406),\n",
      " 'mar_10': tensor(0.3180),\n",
      " 'mar_100': tensor(0.3429),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.4009),\n",
      " 'mar_medium': tensor(0.0200),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7\n",
      "lr = 0.0001989169667816524\n",
      "train_loss = 1.2166020810604095\n",
      "train_cls_loss = 0.13603218600153924\n",
      "train_reg_loss = 0.4741384518146515\n",
      "train_cen_loss = 0.6064314448833465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8\n",
      "valid_loss = 1.3678788661956787\n",
      "valid_cls_loss = 0.22902177706360816\n",
      "valid_reg_loss = 0.5340778928995132\n",
      "valid_cen_loss = 0.604779200553894\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.0843),\n",
      " 'map_50': tensor(0.4126),\n",
      " 'map_75': tensor(0.0027),\n",
      " 'map_large': tensor(0.0985),\n",
      " 'map_medium': tensor(0.0035),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0699),\n",
      " 'mar_10': tensor(0.1940),\n",
      " 'mar_100': tensor(0.2211),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.2593),\n",
      " 'mar_medium': tensor(0.0067),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8\n",
      "lr = 0.00023798513039759433\n",
      "train_loss = 1.204167561531067\n",
      "train_cls_loss = 0.13247684858739375\n",
      "train_reg_loss = 0.46172657907009124\n",
      "train_cen_loss = 0.6099641400575638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9\n",
      "valid_loss = 1.261417977809906\n",
      "valid_cls_loss = 0.18202126979827882\n",
      "valid_reg_loss = 0.472168071269989\n",
      "valid_cen_loss = 0.6072286331653595\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.2269),\n",
      " 'map_50': tensor(0.6699),\n",
      " 'map_75': tensor(0.0985),\n",
      " 'map_large': tensor(0.2653),\n",
      " 'map_medium': tensor(0.0046),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1391),\n",
      " 'mar_10': tensor(0.3150),\n",
      " 'mar_100': tensor(0.3353),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3929),\n",
      " 'mar_medium': tensor(0.0133),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9\n",
      "lr = 0.0002801451669616888\n",
      "train_loss = 1.1779203915596008\n",
      "train_cls_loss = 0.14407153613865376\n",
      "train_reg_loss = 0.43503872126340865\n",
      "train_cen_loss = 0.5988101321458816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10\n",
      "valid_loss = 1.311760549545288\n",
      "valid_cls_loss = 0.22556384280323982\n",
      "valid_reg_loss = 0.4690006226301193\n",
      "valid_cen_loss = 0.6171960818767548\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1865),\n",
      " 'map_50': tensor(0.5375),\n",
      " 'map_75': tensor(0.0647),\n",
      " 'map_large': tensor(0.2194),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1429),\n",
      " 'mar_10': tensor(0.2782),\n",
      " 'mar_100': tensor(0.2782),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3274),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10\n",
      "lr = 0.00032493485447575006\n",
      "train_loss = 1.2066954779624939\n",
      "train_cls_loss = 0.16726480424404144\n",
      "train_reg_loss = 0.42979271441698075\n",
      "train_cen_loss = 0.6096379625797271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 11\n",
      "valid_loss = 1.388966965675354\n",
      "valid_cls_loss = 0.30320895597338676\n",
      "valid_reg_loss = 0.4742782151699066\n",
      "valid_cen_loss = 0.6114797937870026\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1358),\n",
      " 'map_50': tensor(0.5594),\n",
      " 'map_75': tensor(0.0124),\n",
      " 'map_large': tensor(0.1585),\n",
      " 'map_medium': tensor(0.0124),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0917),\n",
      " 'mar_10': tensor(0.2353),\n",
      " 'mar_100': tensor(0.2519),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.2920),\n",
      " 'mar_medium': tensor(0.0333),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 11\n",
      "lr = 0.00037186314073612604\n",
      "train_loss = 1.160288050174713\n",
      "train_cls_loss = 0.16007100246846676\n",
      "train_reg_loss = 0.39415094286203384\n",
      "train_cen_loss = 0.6060661113262177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 19.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 12\n",
      "valid_loss = 1.2807581639289856\n",
      "valid_cls_loss = 0.20598782539367677\n",
      "valid_reg_loss = 0.4566451281309128\n",
      "valid_cen_loss = 0.6181251978874207\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1713),\n",
      " 'map_50': tensor(0.5360),\n",
      " 'map_75': tensor(0.0301),\n",
      " 'map_large': tensor(0.2010),\n",
      " 'map_medium': tensor(0.0035),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1098),\n",
      " 'mar_10': tensor(0.3165),\n",
      " 'mar_100': tensor(0.3241),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3805),\n",
      " 'mar_medium': tensor(0.0067),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 12\n",
      "lr = 0.00042041552698962625\n",
      "train_loss = 1.1168885552883148\n",
      "train_cls_loss = 0.1482484246790409\n",
      "train_reg_loss = 0.3666773736476898\n",
      "train_cen_loss = 0.6019627505540848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 13\n",
      "valid_loss = 1.2695294427871704\n",
      "valid_cls_loss = 0.21752222150564193\n",
      "valid_reg_loss = 0.42929411828517916\n",
      "valid_cen_loss = 0.6227131068706513\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1642),\n",
      " 'map_50': tensor(0.5741),\n",
      " 'map_75': tensor(0.0250),\n",
      " 'map_large': tensor(0.1907),\n",
      " 'map_medium': tensor(0.0307),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1150),\n",
      " 'mar_10': tensor(0.2902),\n",
      " 'mar_100': tensor(0.3060),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3549),\n",
      " 'mar_medium': tensor(0.0400),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 13\n",
      "lr = 0.00047005970864593945\n",
      "train_loss = 1.1103915822505952\n",
      "train_cls_loss = 0.15001648642122745\n",
      "train_reg_loss = 0.3558663922548294\n",
      "train_cen_loss = 0.6045087039470672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 14\n",
      "valid_loss = 1.1778260695934295\n",
      "valid_cls_loss = 0.19317560374736786\n",
      "valid_reg_loss = 0.3762330350279808\n",
      "valid_cen_loss = 0.6084174370765686\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.2508),\n",
      " 'map_50': tensor(0.6897),\n",
      " 'map_75': tensor(0.0816),\n",
      " 'map_large': tensor(0.2943),\n",
      " 'map_medium': tensor(0.0002),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1609),\n",
      " 'mar_10': tensor(0.3361),\n",
      " 'mar_100': tensor(0.3526),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.4142),\n",
      " 'mar_medium': tensor(0.0067),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████████████████████████████████████████████████▎                                                      | 54/100 [00:11<00:09,  4.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     83\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 84\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m train_cls_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     86\u001b[0m train_reg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "start_epoch = 0\n",
    "for epoch in range(start_epoch, 100):\n",
    "    time_ep = time.time()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_cls_loss = 0\n",
    "    valid_reg_loss = 0\n",
    "    valid_cen_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        map_metric = MeanAveragePrecision(\n",
    "            box_format='xywh', # make sure your dataset outputs target in xywh format\n",
    "            backend='faster_coco_eval'\n",
    "        )\n",
    "        for batch in tqdm(val_dataloader, total=len(val_dataloader)):\n",
    "            cls_tgts, reg_tgts = associate_targets_to_anchors(\n",
    "                batch['target'],\n",
    "                anchors,\n",
    "                anchor_sizes\n",
    "            )\n",
    "            image = batch[\"image\"].to(dev)\n",
    "            cls_logits, bbox_reg, centerness = model(image)\n",
    "            losses = eval_losses(\n",
    "                cls_logits,\n",
    "                bbox_reg,\n",
    "                centerness,\n",
    "                cls_tgts.to(dev),\n",
    "                reg_tgts.to(dev)\n",
    "            )\n",
    "            loss = losses['combined_loss']\n",
    "            valid_loss += loss.detach().item()\n",
    "            valid_cls_loss += losses[\"cls_loss\"].detach().item()\n",
    "            valid_reg_loss += losses[\"reg_loss\"].detach().item()\n",
    "            valid_cen_loss += losses[\"centerness_loss\"].detach().item()\n",
    "            b,c,h,w = image.shape\n",
    "            preds = post_process_batch(\n",
    "                cls_logits.to(\"cpu\"),\n",
    "                bbox_reg.to(\"cpu\"),\n",
    "                centerness.to(\"cpu\"),\n",
    "                (h,w)\n",
    "            )\n",
    "            map_metric.update(preds, batch['target'])\n",
    "        valid_loss /= len(val_dataloader)\n",
    "        valid_cls_loss /= len(val_dataloader)\n",
    "        valid_reg_loss /= len(val_dataloader)\n",
    "        valid_cen_loss /= len(val_dataloader)\n",
    "        mapmetrics = map_metric.compute()\n",
    "        print(f\"{epoch = }\")\n",
    "        print(f\"{valid_loss = }\")\n",
    "        print(f\"{valid_cls_loss = }\")\n",
    "        print(f\"{valid_reg_loss = }\")\n",
    "        print(f\"{valid_cen_loss = }\")\n",
    "        print(pformat(mapmetrics))\n",
    "        map_metric.reset()\n",
    "    train_loss = 0\n",
    "    train_cls_loss = 0\n",
    "    train_reg_loss = 0\n",
    "    train_cen_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "        image = batch[\"image\"].to(dev)               \n",
    "        optimizer.zero_grad()\n",
    "        cls_logits, bbox_reg, centerness = model(image)\n",
    "        cls_tgts, reg_tgts = associate_targets_to_anchors(\n",
    "            batch['target'],\n",
    "            anchors,\n",
    "            anchor_sizes\n",
    "        ) # BxNumAnchors, BxNumAnchorsx4    \n",
    "        losses = eval_losses(\n",
    "            cls_logits,\n",
    "            bbox_reg,\n",
    "            centerness,\n",
    "            cls_tgts.to(dev),\n",
    "            reg_tgts.to(dev)\n",
    "        )\n",
    "        loss = losses['combined_loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        train_loss += loss.detach().item()\n",
    "        train_cls_loss += losses[\"cls_loss\"].detach().item()\n",
    "        train_reg_loss += losses[\"reg_loss\"].detach().item()\n",
    "        train_cen_loss += losses[\"centerness_loss\"].detach().item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_cls_loss /= len(train_dataloader)\n",
    "    train_reg_loss /= len(train_dataloader)\n",
    "    train_cen_loss /= len(train_dataloader)\n",
    "    print(f\"{epoch = }\")\n",
    "    print(f\"lr = {lr_scheduler.get_last_lr()[0]}\"),\n",
    "    print(f\"{train_loss = }\")\n",
    "    print(f\"{train_cls_loss = }\")\n",
    "    print(f\"{train_reg_loss = }\")\n",
    "    print(f\"{train_cen_loss = }\")\n",
    "    \n",
    "    time_ep = time.time() - time_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f633afe-0349-4224-9d70-2d7c6e034893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_38",
   "language": "python",
   "name": "dl_toolbox_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
