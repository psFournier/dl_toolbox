{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9374da9c-ecba-49c1-97bc-30bf4e820eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import gc \n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Subset, RandomSampler\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torchvision\n",
    "from torchvision.ops import box_convert, generalized_box_iou\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelP6P7, ExtraFPNBlock\n",
    "from torchvision.ops.misc import Conv2dNormActivation\n",
    "import timm\n",
    "from timm.layers import resample_abs_pos_embed \n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pformat\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "from dl_toolbox.transforms import NormalizeBB\n",
    "from dl_toolbox.utils import list_of_dicts_to_dict_of_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70a27c-fad7-44ad-9724-4fe987a941a2",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5eb7083-dd77-43bf-8fe8-d08a5f6f2323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Scale(nn.Module):\n",
    "\n",
    "    def __init__(self, init_value=1.0):\n",
    "        super(Scale, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([init_value]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.scale\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, n_classes, n_share_convs=4, n_feat_levels=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        tower = []\n",
    "        for _ in range(n_share_convs):\n",
    "            tower.append(\n",
    "                nn.Conv2d(in_channels,\n",
    "                          in_channels,\n",
    "                          kernel_size=3,\n",
    "                          stride=1,\n",
    "                          padding=1,\n",
    "                          bias=True))\n",
    "            tower.append(nn.GroupNorm(32, in_channels))\n",
    "            tower.append(nn.ReLU())\n",
    "        self.shared_layers = nn.Sequential(*tower)\n",
    "\n",
    "        self.cls_logits = nn.Conv2d(in_channels,\n",
    "                                    n_classes,\n",
    "                                    kernel_size=3,\n",
    "                                    stride=1,\n",
    "                                    padding=1)\n",
    "        self.bbox_pred = nn.Conv2d(in_channels,\n",
    "                                   4,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1)\n",
    "        self.ctrness = nn.Conv2d(in_channels,\n",
    "                                 1,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1,\n",
    "                                 padding=1)\n",
    "\n",
    "        self.scales = nn.ModuleList([Scale(init_value=1.0) for _ in range(n_feat_levels)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_logits = []\n",
    "        bbox_preds = []\n",
    "        cness_preds = []\n",
    "        for l, features in enumerate(x):\n",
    "            features = self.shared_layers(features)\n",
    "            cls_logits.append(self.cls_logits(features).flatten(-2))\n",
    "            cness_preds.append(self.ctrness(features).flatten(-2))\n",
    "            reg = self.bbox_pred(features)\n",
    "            reg = self.scales[l](reg)\n",
    "            bbox_preds.append(nn.functional.relu(reg).flatten(-2))\n",
    "        all_logits = torch.cat(cls_logits, dim=-1).permute(0,2,1) # BxNumAnchorsxC\n",
    "        all_box_regs = torch.cat(bbox_preds, dim=-1).permute(0,2,1) # BxNumAnchorsx4\n",
    "        all_cness = torch.cat(cness_preds, dim=-1).permute(0,2,1) # BxNumAnchorsx1\n",
    "        return all_logits, all_box_regs, all_cness\n",
    "\n",
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    \"\"\" LayerNorm for channels of '2D' spatial NCHW tensors \"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6, affine=True):\n",
    "        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.layer_norm(\n",
    "            x.permute(0, 2, 3, 1), self.normalized_shape, self.weight, self.bias, self.eps).permute(0, 3, 1, 2)\n",
    "\n",
    "class SimpleFeaturePyramidNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Module that adds a Simple FPN from on top of a set of feature maps. This is based on\n",
    "    `\"Exploring Plain Vision Transformer Backbones for Object Detection\" <https://arxiv.org/abs/2203.16527>`_.\n",
    "\n",
    "    Unlike regular FPN, Simple FPN expects a single feature map,\n",
    "    on which the Simple FPN will be added.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of channels for the input feature map that\n",
    "            is passed to the module\n",
    "        out_channels (int): number of channels of the Simple FPN representation\n",
    "        extra_blocks (ExtraFPNBlock or None): if provided, extra operations will\n",
    "            be performed. It is expected to take the fpn features, the original\n",
    "            features and the names of the original features as input, and returns\n",
    "            a new list of feature maps and their corresponding names\n",
    "        norm_layer (callable, optional): Module specifying the normalization layer to use. Default: LayerNorm\n",
    "\n",
    "    Examples::\n",
    "    \n",
    "        >>> vitdet = ViTDet(256, 10)\n",
    "        >>> x = torch.rand(2, 3, 224, 224)\n",
    "        >>> feat_dict = vitdet.forward_feat(x)\n",
    "        >>> features = list(feat_dict.values())\n",
    "        >>> print(f'{[f.shape for f in features] = }')\n",
    "        >>> box_cls, box_regression, centerness = vitdet.head(features)\n",
    "        >>> print(f'{box_cls.shape = }')\n",
    "        >>> assert sum([f.shape[2]*f.shape[3] for f in features])==box_cls.shape[1]\n",
    "\n",
    "        DOES NOT WORK BELOW\n",
    "        >>> m = torchvision.ops.SimpleFeaturePyramidNetwork(10, 5)\n",
    "        >>> # get some dummy data\n",
    "        >>> x = torch.rand(1, 10, 64, 64)\n",
    "        >>> # compute the Simple FPN on top of x\n",
    "        >>> output = m(x)\n",
    "        >>> print([(k, v.shape) for k, v in output.items()])\n",
    "        >>> # returns\n",
    "        >>>   [('feat0', torch.Size([1, 5, 64, 64])),\n",
    "        >>>    ('feat2', torch.Size([1, 5, 16, 16])),\n",
    "        >>>    ('feat3', torch.Size([1, 5, 8, 8]))]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        extra_blocks: Optional[ExtraFPNBlock] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for block_index in range(0,4):\n",
    "            layers = []\n",
    "            current_in_channels = in_channels\n",
    "            if block_index == 0:\n",
    "                layers.extend([\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels,\n",
    "                        in_channels // 2,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                    norm_layer(in_channels // 2),\n",
    "                    nn.GELU(),\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels // 2,\n",
    "                        in_channels // 4,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                ])\n",
    "                current_in_channels = in_channels // 4\n",
    "            elif block_index == 1:\n",
    "                layers.append(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels,\n",
    "                        in_channels // 2,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                    ),\n",
    "                )\n",
    "                current_in_channels = in_channels // 2\n",
    "            elif block_index == 2:\n",
    "                # nothing to do for this scale\n",
    "                pass\n",
    "            elif block_index == 3:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "            layers.extend([\n",
    "                Conv2dNormActivation(\n",
    "                    current_in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=1,\n",
    "                    padding=0,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=None\n",
    "                ),\n",
    "                Conv2dNormActivation(\n",
    "                    out_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=None\n",
    "                )\n",
    "            ])\n",
    "            self.blocks.append(nn.Sequential(*layers))\n",
    "\n",
    "        if extra_blocks is not None:\n",
    "            if not isinstance(extra_blocks, ExtraFPNBlock):\n",
    "                raise TypeError(f\"extra_blocks should be of type ExtraFPNBlock not {type(extra_blocks)}\")\n",
    "        self.extra_blocks = extra_blocks\n",
    "\n",
    "    def forward(self, x: Tensor) -> Dict[str, Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the Simple FPN for a feature map.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): input feature map.\n",
    "\n",
    "        Returns:\n",
    "            results (list[Tensor]): feature maps after FPN layers.\n",
    "                They are ordered from highest resolution first.\n",
    "        \"\"\"\n",
    "        results = [block(x) for block in self.blocks]\n",
    "        names = [f\"{i}\" for i in range(len(self.blocks))]\n",
    "\n",
    "        if self.extra_blocks is not None:\n",
    "            results, names = self.extra_blocks(results, [x], names)\n",
    "\n",
    "        # make it back an OrderedDict\n",
    "        out = OrderedDict([(k, v) for k, v in zip(names, results)])\n",
    "\n",
    "        return out\n",
    "    \n",
    "class ViTDet(nn.Module):\n",
    "    \n",
    "    def __init__(self, out_channels, num_classes):\n",
    "        super(ViTDet, self).__init__()\n",
    "        #self.backbone = timm.create_model('samvit_base_patch16.sa1b', pretrained=True)\n",
    "        self.backbone = timm.create_model(\n",
    "            'vit_tiny_patch16_224',\n",
    "            pretrained=True,\n",
    "            dynamic_img_size=True #Deals with inputs of other size than pretraining\n",
    "        )\n",
    "        self.sfpn = SimpleFeaturePyramidNetwork(\n",
    "            in_channels=192,\n",
    "            out_channels=out_channels,\n",
    "            #extra_blocks=LastLevelP6P7(out_channels,out_channels),\n",
    "            norm_layer=LayerNorm2d\n",
    "        )\n",
    "        self.head = Head(out_channels, num_classes, n_feat_levels=6) # 6=4+2extrablocks\n",
    "        \n",
    "    def forward_feat(self, x):\n",
    "        intermediates = self.backbone.forward_intermediates(x, indices=1, norm=False, intermediates_only=True)\n",
    "        features = self.sfpn(intermediates[0])\n",
    "        return features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat_dict = self.forward_feat(x)\n",
    "        features = list(feat_dict.values())\n",
    "        box_cls, box_regression, centerness = self.head(features)\n",
    "        return box_cls, box_regression, centerness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7c09e-9929-4dc5-8f68-d4201a623ed4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3393812-b4bc-4456-90ba-d765d7be142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = read_image(img_path)\n",
    "        mask = read_image(mask_path)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(masks)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = tv_tensors.Image(img)\n",
    "        h, w = T.functional.get_size(img)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(h,w))\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return {'image': img, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "def collate(batch):\n",
    "    batch = list_of_dicts_to_dict_of_lists(batch)\n",
    "    batch['image'] = torch.stack(batch['image'])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae2350-24d5-45dc-a239-aa1223c4fb69",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58abad9-3d22-4433-802b-9242a825c794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LossEvaluator(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(LossEvaluator, self).__init__()\n",
    "        self.centerness_loss_func = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "        self.num_classes = num_classes\n",
    "                \n",
    "    def __call__(self, cls_logits, reg_preds, cness_preds, cls_tgts, reg_tgts):\n",
    "        pos_inds_b, pos_inds_loc = torch.nonzero(cls_tgts > 0, as_tuple=True)\n",
    "        num_pos = len(pos_inds_b)\n",
    "        reg_preds = reg_preds[pos_inds_b, pos_inds_loc, :]\n",
    "        reg_tgts = reg_tgts[pos_inds_b, pos_inds_loc, :]\n",
    "        cness_preds = cness_preds[pos_inds_b, pos_inds_loc, :].squeeze(-1)\n",
    "        cness_tgts = self._compute_centerness_targets(reg_tgts)\n",
    "        cls_loss = self._get_cls_loss(cls_logits, cls_tgts, max(num_pos, 1.))\n",
    "        reg_loss, centerness_loss = 0,0\n",
    "        if num_pos > 0:\n",
    "            reg_loss = self._get_reg_loss(\n",
    "                reg_preds, reg_tgts, cness_tgts)\n",
    "            centerness_loss = self._get_centerness_loss(\n",
    "                cness_preds, cness_tgts, num_pos)\n",
    "        losses = {}\n",
    "        losses[\"cls_loss\"] = cls_loss\n",
    "        losses[\"reg_loss\"] = reg_loss\n",
    "        losses[\"centerness_loss\"] = centerness_loss\n",
    "        losses[\"combined_loss\"] = cls_loss + reg_loss + centerness_loss\n",
    "        return losses\n",
    "    \n",
    "    def _compute_centerness_targets(self, reg_tgts):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            reg_tgts: l, t, r, b values to regress, shape BxNumAx4\n",
    "        Returns:\n",
    "            A tensor of shape BxNumA giving how centered each anchor is for the bbox it must regress\n",
    "        \"\"\"\n",
    "        if len(reg_tgts) == 0:\n",
    "            return reg_tgts.new_zeros(len(reg_tgts))\n",
    "        left_right = reg_tgts[..., [0, 2]]\n",
    "        top_bottom = reg_tgts[..., [1, 3]]\n",
    "        centerness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\\n",
    "                    (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0])\n",
    "        return torch.sqrt(centerness)\n",
    "\n",
    "    def _get_cls_loss(self, cls_preds, cls_targets, num_pos_samples):\n",
    "        \"\"\"\n",
    "        cls_targets takes values in 0...C, 0 only when there is no obj to be detected for the anchor\n",
    "        \"\"\"\n",
    "        onehot = nn.functional.one_hot(cls_targets.long(), self.num_classes+1)[...,1:].float()\n",
    "        cls_loss = torchvision.ops.sigmoid_focal_loss(cls_preds, onehot)\n",
    "        return cls_loss.sum() / num_pos_samples\n",
    "\n",
    "    def _get_reg_loss(self, reg_preds, reg_targets, centerness_targets):\n",
    "        ltrb_preds = reg_preds.reshape(-1, 4)\n",
    "        ltrb_tgts = reg_targets.reshape(-1, 4)\n",
    "        xyxy_preds = torch.cat([-ltrb_preds[:,:2], ltrb_preds[:,2:]], dim=1) \n",
    "        xyxy_tgts = torch.cat([-ltrb_tgts[:,:2], ltrb_tgts[:,2:]], dim=1)\n",
    "        reg_losses = torchvision.ops.distance_box_iou_loss(xyxy_preds, xyxy_tgts, reduction='none')\n",
    "        sum_centerness_targets = centerness_targets.sum()\n",
    "        reg_loss = (reg_losses * centerness_targets).sum() / sum_centerness_targets\n",
    "        return reg_loss\n",
    "\n",
    "    def _get_centerness_loss(self, centerness_preds, centerness_targets,\n",
    "                             num_pos_samples):\n",
    "        centerness_loss = self.centerness_loss_func(centerness_preds,\n",
    "                                                    centerness_targets)\n",
    "        return centerness_loss / num_pos_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c47b3-dd85-422f-b1f4-79c2af239f3b",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96751d23-bad3-438e-aa34-478b6a6faafe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INF = 100000000\n",
    "\n",
    "def get_fm_anchors(h, w, s):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        h, w: height, width of the feat map\n",
    "        s: stride of the featmap = size reduction factor relative to image\n",
    "    Returns:\n",
    "        Tensor NumAnchorsInFeatMap x 2, ordered by column \n",
    "        TODO: check why: DONE: it corresponds to how locs are computed in \n",
    "        https://github.com/tianzhi0549/FCOS/blob/master/fcos_core/modeling/rpn/fcos/fcos.py\n",
    "        When flattening feat maps, we see first the line at H(=y) fixed and W(=x) moving\n",
    "        \n",
    "    \"\"\"\n",
    "    locs_x = [s / 2 + x * s for x in range(w)]\n",
    "    locs_y = [s / 2 + y * s for y in range(h)]\n",
    "    locs = [(x, y) for y in locs_y for x in locs_x] # order !\n",
    "    return torch.tensor(locs)\n",
    "\n",
    "def get_all_anchors_bb_sizes(fm_sizes, fm_strides, bb_sizes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fm_sizes: seq of feature_maps sizes\n",
    "        fm_strides: seq of corresponding strides\n",
    "        bb_sizes: seq of bbox sizes feature maps are associated with, len = len(fm) + 1\n",
    "    Returns:\n",
    "        anchors: list of num_featmaps elem, where each elem indicates the tensor of anchors of size Nx2 in the original image corresponding to each location in the feature map at this level\n",
    "        anchors_bb_sizes: sizes of the bbox each anchor is authorized/supposed to detect\n",
    "    \"\"\"\n",
    "    bb_sizes = [-1] + bb_sizes + [INF]\n",
    "    anchors, anchors_bb_sizes = [], []\n",
    "    for l, ((h,w), s) in enumerate(zip(fm_sizes, fm_strides)):\n",
    "        fm_anchors = get_fm_anchors(h, w, s)\n",
    "        sizes = torch.tensor([bb_sizes[l], bb_sizes[l+1]], dtype=torch.float32)\n",
    "        sizes = sizes.repeat(len(fm_anchors)).view(len(fm_anchors), 2)\n",
    "        anchors.append(fm_anchors)\n",
    "        anchors_bb_sizes.append(sizes)\n",
    "    return torch.cat(anchors, 0), torch.cat(anchors_bb_sizes, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e372b006-de62-412d-af86-c779553f2a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_reg_targets(anchors, bbox):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        anchors: Lx2, anchors coordinates\n",
    "        bbox: tensor of bbox Tx4, format should be xywh\n",
    "    Returns:\n",
    "        reg_tgt: l,t,r,b values to regress for each pair (anchor, bbox)\n",
    "        anchor_in_box: whether anchor is in bbox for each pair (anchor, bbox)\n",
    "    \"\"\"\n",
    "    xs, ys = anchors[:, 0], anchors[:, 1] # L & L, x & y reversed ?? x means position on x-axis\n",
    "    l = xs[:, None] - bbox[:, 0][None] # Lx1 - 1xT -> LxT\n",
    "    t = ys[:, None] - bbox[:, 1][None]\n",
    "    r = bbox[:, 2][None] + bbox[:, 0][None] - xs[:, None]\n",
    "    b = bbox[:, 3][None] + bbox[:, 1][None] - ys[:, None]  \n",
    "    #print(xs[0], ys[0], l[0], t[0], r[0], b[0])\n",
    "    return torch.stack([l, t, r, b], dim=2) # LxTx4\n",
    "\n",
    "def apply_distance_constraints(reg_targets, anchor_sizes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        reg_targets: LxTx4\n",
    "        anchor_bb_sizes: Lx2\n",
    "    Returns:\n",
    "        A LxT tensor where value at (anchor, bbox) is true if the max value to regress at this anchor for this bbox is inside the bounds associated to this anchor\n",
    "        If other values to regress than the max are negatives, it is dealt with anchor_in_boxes.\n",
    "    \"\"\"\n",
    "    max_reg_targets, _ = reg_targets.max(dim=2) # LxT\n",
    "    min_reg_targets, _ = reg_targets.min(dim=2) # LxT\n",
    "    dist_constraints = torch.stack([\n",
    "        min_reg_targets > 0,\n",
    "        max_reg_targets >= anchor_sizes[:, None, 0],\n",
    "        max_reg_targets <= anchor_sizes[:, None, 1]\n",
    "    ])\n",
    "    return torch.all(dist_constraints, dim=0)\n",
    "\n",
    "def anchor_bbox_area(bbox, anchors, fits_to_feature_level):\n",
    "    \"\"\"\n",
    "    Args: bbox is XYWH\n",
    "    Returns: \n",
    "        Tensor LxT where value at (anchor, bbox) is the area of bbox if anchor is in bbox and anchor is associated with bbox of that size\n",
    "        Else INF.\n",
    "    \"\"\"\n",
    "    #bbox_areas = _calc_bbox_area(bbox_targets) # T\n",
    "    bbox_areas = bbox[:, 2] * bbox[:, 3] # T\n",
    "    # area of each target bbox repeated for each loc with inf where the the loc is not \n",
    "    # in the target bbox or if the loc is not at the right level for this bbox size\n",
    "    anchor_bbox_area = bbox_areas[None].repeat(len(anchors), 1) # LxT\n",
    "    anchor_bbox_area[~fits_to_feature_level] = INF\n",
    "    return anchor_bbox_area\n",
    "\n",
    "def associate_targets_to_anchors(targets_batch, anchors, anchors_bb_sizes):\n",
    "    \"\"\"\n",
    "    Associate one target cls/bbox to regress ONLY to each anchor: among the bboxes that contain the anchor and have the right size, pick that of min area.\n",
    "    If no tgt exists for an anchor, the tgt class is 0.\n",
    "    inputs:\n",
    "        targets_batch: list of dict of tv_tensors {'labels':, 'boxes':}; boxes should be in XYWH format\n",
    "        anchors: \n",
    "        anchor_bb_sizes:\n",
    "    outputs:\n",
    "        all class targets: BxNumAnchors\n",
    "        all bbox targets: BxNumAnchorsx4\n",
    "    \"\"\"\n",
    "    all_reg_targets, all_cls_targets = [], []\n",
    "    for targets in targets_batch:\n",
    "        bbox_targets = targets['boxes'] # Tx4, format XYWH\n",
    "        cls_targets = targets['labels'] # T\n",
    "        reg_targets = calculate_reg_targets(\n",
    "            anchors, bbox_targets) # LxTx4, LxT\n",
    "        fits_to_feature_level = apply_distance_constraints(\n",
    "            reg_targets, anchors_bb_sizes) # LxT\n",
    "        locations_to_gt_area = anchor_bbox_area(\n",
    "            bbox_targets, anchors, fits_to_feature_level)\n",
    "        # Core of the anchor/target association\n",
    "        if cls_targets.shape[0]>0:\n",
    "            loc_min_area, loc_min_idxs = locations_to_gt_area.min(dim=1) #L,idx in [0,T-1],T must be>0\n",
    "            reg_targets = reg_targets[range(len(anchors)), loc_min_idxs] # Lx4\n",
    "            cls_targets = cls_targets[loc_min_idxs] # L\n",
    "            cls_targets[loc_min_area == INF] = 0 # 0 is no-obj category\n",
    "        else:\n",
    "            cls_targets = cls_targets.new_zeros((len(anchors),))\n",
    "            reg_targets = reg_targets.new_zeros((len(anchors),4))\n",
    "        all_cls_targets.append(cls_targets)\n",
    "        all_reg_targets.append(reg_targets)\n",
    "    # BxL & BxLx4\n",
    "    return torch.stack(all_cls_targets), torch.stack(all_reg_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfde0d8-2db1-4c7a-8fa9-b86c9c25c907",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Post-processing predictions to boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea21012-a1cc-4b43-b168-ade9619b1f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_nms_thresh=0.3\n",
    "pre_nms_top_n=100000\n",
    "nms_thresh=0.45\n",
    "fpn_post_nms_top_n=50\n",
    "min_size=0\n",
    "\n",
    "def post_process(logits, ltrb, cness, input_size):\n",
    "    probas = logits.sigmoid() # LxC\n",
    "    high_probas = probas > pre_nms_thresh # LxC\n",
    "    # Indices on L and C axis of high prob pairs anchor/class\n",
    "    high_prob_anchors_idx, high_prob_cls = high_probas.nonzero(as_tuple=True) # dim l <= L*C\n",
    "    high_prob_cls += 1 # 0 is for no object\n",
    "    high_prob_ltrb = ltrb[high_prob_anchors_idx] # lx4\n",
    "    high_prob_anchors = anchors[high_prob_anchors_idx] # lx2\n",
    "    # Tensor shape l with values from logits*cness such that logits > pre_nms_thresh \n",
    "    cness_modulated_probas = probas * cness.sigmoid() # LxC\n",
    "    high_prob_scores = cness_modulated_probas[high_probas] # l\n",
    "    # si l est trop longue\n",
    "    if high_probas.sum().item() > pre_nms_top_n:\n",
    "        # Filter the pre_nms_top_n most probable pairs \n",
    "        high_prob_scores, top_k_indices = high_prob_scores.topk(\n",
    "            pre_nms_top_n, sorted=False) \n",
    "        high_prob_cls = high_prob_cls[top_k_indices]\n",
    "        high_prob_ltrb = high_prob_ltrb[top_k_indices]\n",
    "        high_prob_anchors = high_prob_anchors[top_k_indices]\n",
    "\n",
    "    # Rewrites bbox (x0,y0,x1,y1) from reg targets (l,t,r,b) following eq (1) in paper\n",
    "    high_prob_boxes = torch.stack([\n",
    "        high_prob_anchors[:, 0] - high_prob_ltrb[:, 0],\n",
    "        high_prob_anchors[:, 1] - high_prob_ltrb[:, 1],\n",
    "        high_prob_anchors[:, 0] + high_prob_ltrb[:, 2],\n",
    "        high_prob_anchors[:, 1] + high_prob_ltrb[:, 3],\n",
    "    ], dim=1)\n",
    "\n",
    "    high_prob_boxes = torchvision.ops.clip_boxes_to_image(high_prob_boxes, input_size)\n",
    "    big_enough_box_idxs = torchvision.ops.remove_small_boxes(high_prob_boxes, min_size)\n",
    "    boxes = high_prob_boxes[big_enough_box_idxs]\n",
    "    # Why not do that on scores and classes too ? \n",
    "    classes = high_prob_cls[big_enough_box_idxs]\n",
    "    scores = high_prob_scores[big_enough_box_idxs]\n",
    "    #high_prob_scores = torch.sqrt(high_prob_scores) # WHY SQRT ? REmOVED\n",
    "    # NMS expects boxes to be in xyxy format\n",
    "    nms_idxs = torchvision.ops.nms(boxes, scores, nms_thresh)\n",
    "    boxes = boxes[nms_idxs]\n",
    "    scores = scores[nms_idxs]\n",
    "    classes = classes[nms_idxs]\n",
    "    if len(nms_idxs) > fpn_post_nms_top_n:\n",
    "        image_thresh, _ = torch.kthvalue(\n",
    "            scores.cpu(),\n",
    "            len(nms_idxs) - fpn_post_nms_top_n + 1)\n",
    "        keep = scores >= image_thresh.item()\n",
    "        #keep = torch.nonzero(keep).squeeze(1)\n",
    "        boxes, scores, classes = boxes[keep], scores[keep], classes[keep]\n",
    "    # Then back to xywh boxes for preds and metric computation\n",
    "    boxes[:, 2] -= boxes[:, 0]\n",
    "    boxes[:, 3] -= boxes[:, 1]\n",
    "    \n",
    "    # Isn't this cond auto valid from the beginning filter ?\n",
    "    #keep = scores >= pre_nms_thresh\n",
    "    #boxes, scores, classes = boxes[keep], scores[keep], classes[keep]\n",
    "    return boxes, scores, classes \n",
    "\n",
    "def post_process_batch(\n",
    "    cls_preds, # B x L x C \n",
    "    reg_preds, # B x L x 4\n",
    "    cness_preds, # B x L x 1\n",
    "    input_size\n",
    "): \n",
    "    preds = []\n",
    "    for logits, ltrb, cness in zip(cls_preds, reg_preds, cness_preds):\n",
    "        boxes, scores, classes = post_process(logits, ltrb, cness, input_size)\n",
    "        preds.append({'boxes': boxes, 'scores': scores, 'labels': classes})\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10d1a2-e538-4b4a-9874-da06c893f786",
   "metadata": {},
   "source": [
    "### Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96fb4d70-c871-4804-b6de-d2dd2cbae880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472246b-884d-43ed-9619-785a40b174ec",
   "metadata": {},
   "source": [
    "### Instanciations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf81ff54-3815-491f-8636-a79822706505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf = T.Compose(\n",
    "    [\n",
    "        T.ToDtype(torch.float, scale=True),\n",
    "        T.Resize(size=480, max_size=640),\n",
    "        T.RandomCrop(size=(640,640), pad_if_needed=True, fill=0),\n",
    "        T.ConvertBoundingBoxFormat(format='XYWH'),\n",
    "        T.SanitizeBoundingBoxes(),\n",
    "        #T.ToPureTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "dataset_test = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_set = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "val_set = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    dataset=train_set,\n",
    "    sampler=RandomSampler(\n",
    "        train_set,\n",
    "        replacement=True,\n",
    "        num_samples=100*2\n",
    "    ),\n",
    "    drop_last=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    dataset=val_set,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "262b6a09-8eaa-496d-a60b-5aa802902cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 10758500 params out of 10758500\n"
     ]
    }
   ],
   "source": [
    "# Freeze params here if needed\n",
    "    \n",
    "#for param in model.feature_extractor.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "anchors, anchor_sizes = get_all_anchors_bb_sizes(\n",
    "    fm_sizes=[(160,160),(80,80),(40,40),(20,20)], # 640/16 * [4,2,1,0.5]\n",
    "    fm_strides=[4, 8, 16, 32],\n",
    "    bb_sizes=[128, 256, 512]\n",
    ")\n",
    "\n",
    "model = ViTDet(num_classes=1, out_channels=256)\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#dev = torch.device(\"cpu\")\n",
    "model.to(dev)\n",
    "eval_losses = LossEvaluator(\n",
    "    num_classes=1\n",
    ")\n",
    "\n",
    "train_params = list(filter(lambda p: p[1].requires_grad, model.named_parameters()))\n",
    "nb_train = sum([int(torch.numel(p[1])) for p in train_params])\n",
    "nb_tot = sum([int(torch.numel(p)) for p in model.parameters()])\n",
    "print(f\"Training {nb_train} params out of {nb_tot}\")\n",
    "\n",
    "#optimizer = torch.optim.SGD(\n",
    "#    params=[p[1] for p in train_params],\n",
    "#    lr=0.005,\n",
    "#    momentum=0.9,\n",
    "#    weight_decay=0.0005\n",
    "#)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-3,\n",
    "    betas=(0.9,0.999),\n",
    "    weight_decay=5e-2,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=100)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "#    optimizer=optimizer,\n",
    "#    start_factor=1.,\n",
    "#    end_factor=0.1,\n",
    "#    total_iters=20\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655752a3-bf41-4efa-a4ca-c99069bb9435",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a13c370-b3be-4342-8ab3-7d172f8b3cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [02:07<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "valid_loss = 4.570200605392456\n",
      "valid_cls_loss = 2.8458824336528776\n",
      "valid_reg_loss = 1.0354415011405944\n",
      "valid_cen_loss = 0.6888766968250275\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "lr = 4.2630366137540415e-05\n",
      "train_loss = 1.9044084018468856\n",
      "train_cls_loss = 0.24045929174870254\n",
      "train_reg_loss = 1.0354000735282898\n",
      "train_cen_loss = 0.6285490369796753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1\n",
      "valid_loss = 1.854781937599182\n",
      "valid_cls_loss = 0.20799286738038064\n",
      "valid_reg_loss = 1.0343887543678283\n",
      "valid_cen_loss = 0.6124003183841705\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1\n",
      "lr = 5.0492636108421976e-05\n",
      "train_loss = 1.7847205948829652\n",
      "train_cls_loss = 0.14654838796705008\n",
      "train_reg_loss = 1.0321072179079056\n",
      "train_cen_loss = 0.6060649824142456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2\n",
      "valid_loss = 1.8423284029960632\n",
      "valid_cls_loss = 0.2214021673798561\n",
      "valid_reg_loss = 1.0153265726566314\n",
      "valid_cen_loss = 0.6055996692180634\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.),\n",
      " 'map_50': tensor(0.),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(0.),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.),\n",
      " 'mar_10': tensor(0.),\n",
      " 'mar_100': tensor(0.),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2\n",
      "lr = 6.350064054307949e-05\n",
      "train_loss = 1.5335031539201736\n",
      "train_cls_loss = 0.18311305411159992\n",
      "train_reg_loss = 0.7330014505982398\n",
      "train_cen_loss = 0.6173886474967003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3\n",
      "valid_loss = 1.4369921970367432\n",
      "valid_cls_loss = 0.18674474343657493\n",
      "valid_reg_loss = 0.629165325164795\n",
      "valid_cen_loss = 0.6210821223258972\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1111),\n",
      " 'map_50': tensor(0.4222),\n",
      " 'map_75': tensor(0.0170),\n",
      " 'map_large': tensor(0.1290),\n",
      " 'map_medium': tensor(0.0069),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0880),\n",
      " 'mar_10': tensor(0.1895),\n",
      " 'mar_100': tensor(0.2045),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.2398),\n",
      " 'mar_medium': tensor(0.0067),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3\n",
      "lr = 8.151181354825528e-05\n",
      "train_loss = 1.3073997706174851\n",
      "train_cls_loss = 0.13184211790561676\n",
      "train_reg_loss = 0.5682296781241893\n",
      "train_cen_loss = 0.6073279771208763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4\n",
      "valid_loss = 1.3980400013923644\n",
      "valid_cls_loss = 0.19601939246058464\n",
      "valid_reg_loss = 0.599433256983757\n",
      "valid_cen_loss = 0.6025873494148254\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.0908),\n",
      " 'map_50': tensor(0.3412),\n",
      " 'map_75': tensor(0.0137),\n",
      " 'map_large': tensor(0.1057),\n",
      " 'map_medium': tensor(0.0014),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0842),\n",
      " 'mar_10': tensor(0.2000),\n",
      " 'mar_100': tensor(0.2218),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.2602),\n",
      " 'mar_medium': tensor(0.0067),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4\n",
      "lr = 0.00010432875520907355\n",
      "train_loss = 1.2790304714441298\n",
      "train_cls_loss = 0.14032711043953897\n",
      "train_reg_loss = 0.5337618283927441\n",
      "train_cen_loss = 0.6049415370821953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5\n",
      "valid_loss = 1.371737916469574\n",
      "valid_cls_loss = 0.1915132987499237\n",
      "valid_reg_loss = 0.5705960464477539\n",
      "valid_cen_loss = 0.6096285724639893\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1339),\n",
      " 'map_50': tensor(0.4680),\n",
      " 'map_75': tensor(0.0382),\n",
      " 'map_large': tensor(0.1567),\n",
      " 'map_medium': tensor(0.0035),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1143),\n",
      " 'mar_10': tensor(0.2632),\n",
      " 'mar_100': tensor(0.2805),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3292),\n",
      " 'mar_medium': tensor(0.0067),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5\n",
      "lr = 0.00013170139506425542\n",
      "train_loss = 1.2585374504327773\n",
      "train_cls_loss = 0.1443712931498885\n",
      "train_reg_loss = 0.5098312239348889\n",
      "train_cen_loss = 0.6043349322676659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6\n",
      "valid_loss = 1.4220083141326905\n",
      "valid_cls_loss = 0.2524347487092018\n",
      "valid_reg_loss = 0.557370069026947\n",
      "valid_cen_loss = 0.612203494310379\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1097),\n",
      " 'map_50': tensor(0.3778),\n",
      " 'map_75': tensor(0.0264),\n",
      " 'map_large': tensor(0.1271),\n",
      " 'map_medium': tensor(0.0208),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0962),\n",
      " 'mar_10': tensor(0.2323),\n",
      " 'mar_100': tensor(0.2526),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.2947),\n",
      " 'mar_medium': tensor(0.0200),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6\n",
      "lr = 0.00016332973284309213\n",
      "train_loss = 1.2193678605556488\n",
      "train_cls_loss = 0.14599513560533522\n",
      "train_reg_loss = 0.4681146916747093\n",
      "train_cen_loss = 0.6052580305933952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7\n",
      "valid_loss = 1.357437915802002\n",
      "valid_cls_loss = 0.21538873702287675\n",
      "valid_reg_loss = 0.5026441860198975\n",
      "valid_cen_loss = 0.6394049954414368\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1743),\n",
      " 'map_50': tensor(0.5805),\n",
      " 'map_75': tensor(0.0420),\n",
      " 'map_large': tensor(0.2045),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1301),\n",
      " 'mar_10': tensor(0.2707),\n",
      " 'mar_100': tensor(0.2842),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3345),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7\n",
      "lr = 0.00019886712642608328\n",
      "train_loss = 1.2042622303962707\n",
      "train_cls_loss = 0.14624780662357806\n",
      "train_reg_loss = 0.45364793211221693\n",
      "train_cen_loss = 0.6043664935231209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 19.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8\n",
      "valid_loss = 1.2647386813163757\n",
      "valid_cls_loss = 0.17915082812309266\n",
      "valid_reg_loss = 0.4746346354484558\n",
      "valid_cen_loss = 0.6109532189369201\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.2202),\n",
      " 'map_50': tensor(0.6670),\n",
      " 'map_75': tensor(0.0494),\n",
      " 'map_large': tensor(0.2582),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1376),\n",
      " 'mar_10': tensor(0.2962),\n",
      " 'mar_100': tensor(0.3098),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3646),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:23<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8\n",
      "lr = 0.0002379240909936571\n",
      "train_loss = 1.1725226759910583\n",
      "train_cls_loss = 0.15809993527829647\n",
      "train_reg_loss = 0.406094693094492\n",
      "train_cen_loss = 0.6083280456066131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9\n",
      "valid_loss = 1.4392381954193114\n",
      "valid_cls_loss = 0.3346230810880661\n",
      "valid_reg_loss = 0.4794835954904556\n",
      "valid_cen_loss = 0.6251315200328826\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1100),\n",
      " 'map_50': tensor(0.4318),\n",
      " 'map_75': tensor(0.0082),\n",
      " 'map_large': tensor(0.1287),\n",
      " 'map_medium': tensor(0.0139),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0744),\n",
      " 'mar_10': tensor(0.2466),\n",
      " 'mar_100': tensor(0.2820),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3301),\n",
      " 'mar_medium': tensor(0.0133),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9\n",
      "lr = 0.00028007256772484015\n",
      "train_loss = 1.164393764436245\n",
      "train_cls_loss = 0.16415862292051314\n",
      "train_reg_loss = 0.3898171383887529\n",
      "train_cen_loss = 0.6104180061817169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10\n",
      "valid_loss = 1.2388693177700043\n",
      "valid_cls_loss = 0.20933797508478164\n",
      "valid_reg_loss = 0.41216541051864625\n",
      "valid_cen_loss = 0.6173659265041351\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.2050),\n",
      " 'map_50': tensor(0.6110),\n",
      " 'map_75': tensor(0.0411),\n",
      " 'map_large': tensor(0.2383),\n",
      " 'map_medium': tensor(0.0075),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1256),\n",
      " 'mar_10': tensor(0.3105),\n",
      " 'mar_100': tensor(0.3429),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3894),\n",
      " 'mar_medium': tensor(0.1067),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10\n",
      "lr = 0.0003248506152615381\n",
      "train_loss = 1.12825496673584\n",
      "train_cls_loss = 0.15638189639896155\n",
      "train_reg_loss = 0.36502049267292025\n",
      "train_cen_loss = 0.6068525767326355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 11\n",
      "valid_loss = 1.2656659615039825\n",
      "valid_cls_loss = 0.20387523338198663\n",
      "valid_reg_loss = 0.43049712300300597\n",
      "valid_cen_loss = 0.6312936055660248\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1794),\n",
      " 'map_50': tensor(0.5961),\n",
      " 'map_75': tensor(0.0219),\n",
      " 'map_large': tensor(0.2101),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1188),\n",
      " 'mar_10': tensor(0.2669),\n",
      " 'mar_100': tensor(0.2759),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3248),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 11\n",
      "lr = 0.000371767472520644\n",
      "train_loss = 1.154264419078827\n",
      "train_cls_loss = 0.16648796536028385\n",
      "train_reg_loss = 0.3791844702512026\n",
      "train_cen_loss = 0.6085919818282127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 12\n",
      "valid_loss = 1.222894834280014\n",
      "valid_cls_loss = 0.21986298963427545\n",
      "valid_reg_loss = 0.39270633697509766\n",
      "valid_cen_loss = 0.6103255105018616\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1784),\n",
      " 'map_50': tensor(0.5769),\n",
      " 'map_75': tensor(0.0354),\n",
      " 'map_large': tensor(0.2080),\n",
      " 'map_medium': tensor(0.0030),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1429),\n",
      " 'mar_10': tensor(0.2782),\n",
      " 'mar_100': tensor(0.2842),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3319),\n",
      " 'mar_medium': tensor(0.0200),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 12\n",
      "lr = 0.0004203089373662653\n",
      "train_loss = 1.1203000739216804\n",
      "train_cls_loss = 0.16403166510164738\n",
      "train_reg_loss = 0.3535011406987906\n",
      "train_cen_loss = 0.6027672651410103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 13\n",
      "valid_loss = 1.2773105907440185\n",
      "valid_cls_loss = 0.23733104169368743\n",
      "valid_reg_loss = 0.41834857285022736\n",
      "valid_cen_loss = 0.6216309845447541\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1762),\n",
      " 'map_50': tensor(0.4917),\n",
      " 'map_75': tensor(0.0694),\n",
      " 'map_large': tensor(0.2068),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1293),\n",
      " 'mar_10': tensor(0.2684),\n",
      " 'mar_100': tensor(0.2684),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3159),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 13\n",
      "lr = 0.00046994300219258336\n",
      "train_loss = 1.1018867698311805\n",
      "train_cls_loss = 0.15381716903299092\n",
      "train_reg_loss = 0.3465454034507275\n",
      "train_cen_loss = 0.6015241986513138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 14\n",
      "valid_loss = 1.203785685300827\n",
      "valid_cls_loss = 0.22158458828926086\n",
      "valid_reg_loss = 0.37382513731718064\n",
      "valid_cen_loss = 0.6083759546279908\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.2309),\n",
      " 'map_50': tensor(0.6970),\n",
      " 'map_75': tensor(0.0948),\n",
      " 'map_large': tensor(0.2692),\n",
      " 'map_medium': tensor(0.0056),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1556),\n",
      " 'mar_10': tensor(0.3338),\n",
      " 'mar_100': tensor(0.3361),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3903),\n",
      " 'mar_medium': tensor(0.0400),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 14\n",
      "lr = 0.0005201256846521496\n",
      "train_loss = 1.1687019044160842\n",
      "train_cls_loss = 0.18597741633653642\n",
      "train_reg_loss = 0.37066698968410494\n",
      "train_cen_loss = 0.6120574977993966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 15\n",
      "valid_loss = 1.3016180539131164\n",
      "valid_cls_loss = 0.23655320584774017\n",
      "valid_reg_loss = 0.44011237561702726\n",
      "valid_cen_loss = 0.6249524736404419\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1275),\n",
      " 'map_50': tensor(0.5329),\n",
      " 'map_75': tensor(0.0186),\n",
      " 'map_large': tensor(0.1489),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0977),\n",
      " 'mar_10': tensor(0.2368),\n",
      " 'mar_100': tensor(0.2459),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.2894),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 15\n",
      "lr = 0.0005703069896256616\n",
      "train_loss = 1.175437308549881\n",
      "train_cls_loss = 0.19342098891735077\n",
      "train_reg_loss = 0.3704371852427721\n",
      "train_cen_loss = 0.6115791317820549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 16\n",
      "valid_loss = 1.2631787300109862\n",
      "valid_cls_loss = 0.2617907884716988\n",
      "valid_reg_loss = 0.387419151365757\n",
      "valid_cen_loss = 0.6139687895774841\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.2115),\n",
      " 'map_50': tensor(0.6076),\n",
      " 'map_75': tensor(0.0678),\n",
      " 'map_large': tensor(0.2477),\n",
      " 'map_medium': tensor(0.0035),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1421),\n",
      " 'mar_10': tensor(0.3188),\n",
      " 'mar_100': tensor(0.3368),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3956),\n",
      " 'mar_medium': tensor(0.0067),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 16\n",
      "lr = 0.0006199369370908696\n",
      "train_loss = 1.1323073941469193\n",
      "train_cls_loss = 0.17337786361575128\n",
      "train_reg_loss = 0.35285217367112637\n",
      "train_cen_loss = 0.6060773584246636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 17\n",
      "valid_loss = 1.4026880550384522\n",
      "valid_cls_loss = 0.3599850100278854\n",
      "valid_reg_loss = 0.41493745923042297\n",
      "valid_cen_loss = 0.6277655935287476\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1205),\n",
      " 'map_50': tensor(0.3519),\n",
      " 'map_75': tensor(0.0265),\n",
      " 'map_large': tensor(0.1409),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1173),\n",
      " 'mar_10': tensor(0.1865),\n",
      " 'mar_100': tensor(0.1865),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.2195),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 17\n",
      "lr = 0.0006684715898260222\n",
      "train_loss = 1.1739817944169044\n",
      "train_cls_loss = 0.1875375123694539\n",
      "train_reg_loss = 0.3708539319038391\n",
      "train_cen_loss = 0.6155903500318527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 18\n",
      "valid_loss = 1.3662177777290345\n",
      "valid_cls_loss = 0.26877386778593065\n",
      "valid_reg_loss = 0.4843842768669128\n",
      "valid_cen_loss = 0.6130596363544464\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.0420),\n",
      " 'map_50': tensor(0.1906),\n",
      " 'map_75': tensor(0.0048),\n",
      " 'map_large': tensor(0.0487),\n",
      " 'map_medium': tensor(0.0031),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0368),\n",
      " 'mar_10': tensor(0.1549),\n",
      " 'mar_100': tensor(0.2639),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3088),\n",
      " 'mar_medium': tensor(0.0133),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 18\n",
      "lr = 0.0007153790148850699\n",
      "train_loss = 1.1864407977461815\n",
      "train_cls_loss = 0.19927356019616127\n",
      "train_reg_loss = 0.37217360705137253\n",
      "train_cen_loss = 0.6149936318397522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 19\n",
      "valid_loss = 1.322247052192688\n",
      "valid_cls_loss = 0.2395534636080265\n",
      "valid_reg_loss = 0.4614692097902298\n",
      "valid_cen_loss = 0.6212243723869324\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1016),\n",
      " 'map_50': tensor(0.3990),\n",
      " 'map_75': tensor(0.0251),\n",
      " 'map_large': tensor(0.1187),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0684),\n",
      " 'mar_10': tensor(0.2143),\n",
      " 'mar_100': tensor(0.2263),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.2664),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:23<00:00,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 19\n",
      "lr = 0.0007601451135077022\n",
      "train_loss = 1.1900277319550514\n",
      "train_cls_loss = 0.1989364355430007\n",
      "train_reg_loss = 0.3751520444452763\n",
      "train_cen_loss = 0.6159392476081849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 20\n",
      "valid_loss = 1.2473041605949402\n",
      "valid_cls_loss = 0.2229857811331749\n",
      "valid_reg_loss = 0.4003553596138954\n",
      "valid_cen_loss = 0.6239630174636841\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1346),\n",
      " 'map_50': tensor(0.4757),\n",
      " 'map_75': tensor(0.0344),\n",
      " 'map_large': tensor(0.1580),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1105),\n",
      " 'mar_10': tensor(0.2639),\n",
      " 'mar_100': tensor(0.2669),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3142),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 20\n",
      "lr = 0.0008022792555692243\n",
      "train_loss = 1.1851224845647812\n",
      "train_cls_loss = 0.2029639868065715\n",
      "train_reg_loss = 0.3662799184024334\n",
      "train_cen_loss = 0.6158785739541054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 21\n",
      "valid_loss = 1.2465760231018066\n",
      "valid_cls_loss = 0.22509042471647261\n",
      "valid_reg_loss = 0.3987590569257736\n",
      "valid_cen_loss = 0.622726548910141\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1802),\n",
      " 'map_50': tensor(0.5688),\n",
      " 'map_75': tensor(0.0178),\n",
      " 'map_large': tensor(0.2114),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1301),\n",
      " 'mar_10': tensor(0.2857),\n",
      " 'mar_100': tensor(0.2857),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3363),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:23<00:00,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 21\n",
      "lr = 0.0008413196568174989\n",
      "train_loss = 1.146890150308609\n",
      "train_cls_loss = 0.18357272535562516\n",
      "train_reg_loss = 0.3543390186131001\n",
      "train_cen_loss = 0.608978407382965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 22\n",
      "valid_loss = 1.2484378838539123\n",
      "valid_cls_loss = 0.22434668600559235\n",
      "valid_reg_loss = 0.40831359326839445\n",
      "valid_cen_loss = 0.6157776093482972\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1725),\n",
      " 'map_50': tensor(0.5799),\n",
      " 'map_75': tensor(0.0570),\n",
      " 'map_large': tensor(0.1999),\n",
      " 'map_medium': tensor(0.0289),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1278),\n",
      " 'mar_10': tensor(0.2955),\n",
      " 'mar_100': tensor(0.3301),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3788),\n",
      " 'mar_medium': tensor(0.0733),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 22\n",
      "lr = 0.000876838439963195\n",
      "train_loss = 1.1490809699892999\n",
      "train_cls_loss = 0.19133191250264645\n",
      "train_reg_loss = 0.3488973243534565\n",
      "train_cen_loss = 0.6088517278432846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 19.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 23\n",
      "valid_loss = 1.3054478943347931\n",
      "valid_cls_loss = 0.28085193783044815\n",
      "valid_reg_loss = 0.405687637925148\n",
      "valid_cen_loss = 0.618908314704895\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1814),\n",
      " 'map_50': tensor(0.5689),\n",
      " 'map_75': tensor(0.0438),\n",
      " 'map_large': tensor(0.2129),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1383),\n",
      " 'mar_10': tensor(0.2880),\n",
      " 'mar_100': tensor(0.2880),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3389),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 23\n",
      "lr = 0.0009084463241545082\n",
      "train_loss = 1.1713784298300742\n",
      "train_cls_loss = 0.19602549709379674\n",
      "train_reg_loss = 0.36224113203585145\n",
      "train_cen_loss = 0.6131118005514145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 24\n",
      "valid_loss = 1.2728550934791565\n",
      "valid_cls_loss = 0.23452383518218994\n",
      "valid_reg_loss = 0.4199833446741104\n",
      "valid_cen_loss = 0.6183479130268097\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1452),\n",
      " 'map_50': tensor(0.5166),\n",
      " 'map_75': tensor(0.0295),\n",
      " 'map_large': tensor(0.1692),\n",
      " 'map_medium': tensor(0.0347),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0925),\n",
      " 'mar_10': tensor(0.2617),\n",
      " 'mar_100': tensor(0.2662),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3088),\n",
      " 'mar_medium': tensor(0.0333),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 24\n",
      "lr = 0.000935796891440371\n",
      "train_loss = 1.1316157013177872\n",
      "train_cls_loss = 0.17992167301476003\n",
      "train_reg_loss = 0.34536085702478886\n",
      "train_cen_loss = 0.6063331723213196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 19.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 25\n",
      "valid_loss = 1.2676978123188019\n",
      "valid_cls_loss = 0.25144035190343855\n",
      "valid_reg_loss = 0.3998811602592468\n",
      "valid_cen_loss = 0.6163762927055358\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1698),\n",
      " 'map_50': tensor(0.4989),\n",
      " 'map_75': tensor(0.0502),\n",
      " 'map_large': tensor(0.1984),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1226),\n",
      " 'mar_10': tensor(0.2586),\n",
      " 'mar_100': tensor(0.2586),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3044),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 25\n",
      "lr = 0.00095859038346232\n",
      "train_loss = 1.160029198229313\n",
      "train_cls_loss = 0.20018100436776876\n",
      "train_reg_loss = 0.35150728046894075\n",
      "train_cen_loss = 0.6083409079909324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 26\n",
      "valid_loss = 1.3124344229698182\n",
      "valid_cls_loss = 0.2520655199885368\n",
      "valid_reg_loss = 0.43549617767333987\n",
      "valid_cen_loss = 0.6248727262020111\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1281),\n",
      " 'map_50': tensor(0.4953),\n",
      " 'map_75': tensor(0.0195),\n",
      " 'map_large': tensor(0.1486),\n",
      " 'map_medium': tensor(0.0347),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0955),\n",
      " 'mar_10': tensor(0.2346),\n",
      " 'mar_100': tensor(0.2827),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3283),\n",
      " 'mar_medium': tensor(0.0333),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 26\n",
      "lr = 0.00097657698676381\n",
      "train_loss = 1.1583123609423638\n",
      "train_cls_loss = 0.1928509099036455\n",
      "train_reg_loss = 0.3528684414923191\n",
      "train_cen_loss = 0.6125930088758469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 27\n",
      "valid_loss = 1.2545703196525573\n",
      "valid_cls_loss = 0.23381512701511384\n",
      "valid_reg_loss = 0.40385332524776457\n",
      "valid_cen_loss = 0.6169018650054932\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1398),\n",
      " 'map_50': tensor(0.4076),\n",
      " 'map_75': tensor(0.0451),\n",
      " 'map_large': tensor(0.1635),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0940),\n",
      " 'mar_10': tensor(0.2707),\n",
      " 'mar_100': tensor(0.2805),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3301),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 27\n",
      "lr = 0.0009895595707104512\n",
      "train_loss = 1.1896401980519296\n",
      "train_cls_loss = 0.20388034153729678\n",
      "train_reg_loss = 0.3693798165768385\n",
      "train_cen_loss = 0.6163800397515297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 28\n",
      "valid_loss = 1.259595640897751\n",
      "valid_cls_loss = 0.264798903465271\n",
      "valid_reg_loss = 0.38172967225313187\n",
      "valid_cen_loss = 0.6130670654773712\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1538),\n",
      " 'map_50': tensor(0.4571),\n",
      " 'map_75': tensor(0.0560),\n",
      " 'map_large': tensor(0.1799),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1383),\n",
      " 'mar_10': tensor(0.2744),\n",
      " 'mar_100': tensor(0.2744),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3230),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 28\n",
      "lr = 0.0009973958480139476\n",
      "train_loss = 1.1495850956439972\n",
      "train_cls_loss = 0.1901218181848526\n",
      "train_reg_loss = 0.3533931948244572\n",
      "train_cen_loss = 0.6060700818896294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 29\n",
      "valid_loss = 1.2970997309684753\n",
      "valid_cls_loss = 0.2869164535403252\n",
      "valid_reg_loss = 0.39221232026815417\n",
      "valid_cen_loss = 0.6179709577560425\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1393),\n",
      " 'map_50': tensor(0.5015),\n",
      " 'map_75': tensor(0.0237),\n",
      " 'map_large': tensor(0.1634),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1293),\n",
      " 'mar_10': tensor(0.2602),\n",
      " 'mar_100': tensor(0.2602),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3062),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 29\n",
      "lr = 0.0009999999874112692\n",
      "train_loss = 1.170163624584675\n",
      "train_cls_loss = 0.19934747703373432\n",
      "train_reg_loss = 0.3611428602039814\n",
      "train_cen_loss = 0.6096732807159424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 30\n",
      "valid_loss = 1.2712799942493438\n",
      "valid_cls_loss = 0.26139652222394943\n",
      "valid_reg_loss = 0.39491682916879656\n",
      "valid_cen_loss = 0.6149666488170624\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1320),\n",
      " 'map_50': tensor(0.4253),\n",
      " 'map_75': tensor(0.0582),\n",
      " 'map_large': tensor(0.1537),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1241),\n",
      " 'mar_10': tensor(0.2414),\n",
      " 'mar_100': tensor(0.2429),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.2858),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 30\n",
      "lr = 0.000999491488906453\n",
      "train_loss = 1.1497522240877152\n",
      "train_cls_loss = 0.18749628886580466\n",
      "train_reg_loss = 0.353060025498271\n",
      "train_cen_loss = 0.609195912182331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 31\n",
      "valid_loss = 1.2801474118232727\n",
      "valid_cls_loss = 0.245744781345129\n",
      "valid_reg_loss = 0.3960122525691986\n",
      "valid_cen_loss = 0.6383903765678406\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1471),\n",
      " 'map_50': tensor(0.4885),\n",
      " 'map_75': tensor(0.0377),\n",
      " 'map_large': tensor(0.1727),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1083),\n",
      " 'mar_10': tensor(0.2752),\n",
      " 'mar_100': tensor(0.2835),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3336),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 31\n",
      "lr = 0.0009979770850443537\n",
      "train_loss = 1.1344149175286293\n",
      "train_cls_loss = 0.18629400011152028\n",
      "train_reg_loss = 0.34183319501578807\n",
      "train_cen_loss = 0.6062877237796783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 32\n",
      "valid_loss = 1.2583451437950135\n",
      "valid_cls_loss = 0.24717615216970443\n",
      "valid_reg_loss = 0.3909767085313797\n",
      "valid_cen_loss = 0.6201922905445099\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1807),\n",
      " 'map_50': tensor(0.5523),\n",
      " 'map_75': tensor(0.0764),\n",
      " 'map_large': tensor(0.2120),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1406),\n",
      " 'mar_10': tensor(0.2962),\n",
      " 'mar_100': tensor(0.2962),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3487),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 32\n",
      "lr = 0.0009954598256328086\n",
      "train_loss = 1.1419506496191025\n",
      "train_cls_loss = 0.1887270049750805\n",
      "train_reg_loss = 0.3428398948162794\n",
      "train_cen_loss = 0.6103837513923644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 33\n",
      "valid_loss = 1.2270871078968049\n",
      "valid_cls_loss = 0.23295256227254868\n",
      "valid_reg_loss = 0.3850962468981743\n",
      "valid_cen_loss = 0.6090383040904999\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.2072),\n",
      " 'map_50': tensor(0.6407),\n",
      " 'map_75': tensor(0.0360),\n",
      " 'map_large': tensor(0.2398),\n",
      " 'map_medium': tensor(0.0373),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1165),\n",
      " 'mar_10': tensor(0.3233),\n",
      " 'mar_100': tensor(0.3398),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3867),\n",
      " 'mar_medium': tensor(0.1000),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 33\n",
      "lr = 0.0009919447800972691\n",
      "train_loss = 1.0950710347294808\n",
      "train_cls_loss = 0.17060224540531635\n",
      "train_reg_loss = 0.3234100418537855\n",
      "train_cen_loss = 0.601058748960495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 34\n",
      "valid_loss = 1.2320858108997346\n",
      "valid_cls_loss = 0.22694634988904\n",
      "valid_reg_loss = 0.37842759877443316\n",
      "valid_cen_loss = 0.6267118585109711\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1837),\n",
      " 'map_50': tensor(0.5857),\n",
      " 'map_75': tensor(0.0685),\n",
      " 'map_large': tensor(0.2136),\n",
      " 'map_medium': tensor(0.0139),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1383),\n",
      " 'mar_10': tensor(0.2797),\n",
      " 'mar_100': tensor(0.2820),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3301),\n",
      " 'mar_medium': tensor(0.0133),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 34\n",
      "lr = 0.0009874390272716525\n",
      "train_loss = 1.0930761283636092\n",
      "train_cls_loss = 0.16212396752089261\n",
      "train_reg_loss = 0.3274197454750538\n",
      "train_cen_loss = 0.6035324138402939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 19.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 35\n",
      "valid_loss = 1.2879620277881623\n",
      "valid_cls_loss = 0.2688609017431736\n",
      "valid_reg_loss = 0.3969916480779648\n",
      "valid_cen_loss = 0.622109466791153\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1535),\n",
      " 'map_50': tensor(0.5250),\n",
      " 'map_75': tensor(0.0382),\n",
      " 'map_large': tensor(0.1800),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1278),\n",
      " 'mar_10': tensor(0.2602),\n",
      " 'mar_100': tensor(0.2677),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3150),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 35\n",
      "lr = 0.000981951641142512\n",
      "train_loss = 1.1063109067082406\n",
      "train_cls_loss = 0.16918087657541037\n",
      "train_reg_loss = 0.3349624191969633\n",
      "train_cen_loss = 0.6021676057577133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 36\n",
      "valid_loss = 1.242058016061783\n",
      "valid_cls_loss = 0.23331218600273132\n",
      "valid_reg_loss = 0.3901923236250877\n",
      "valid_cen_loss = 0.6185535085201264\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1565),\n",
      " 'map_50': tensor(0.5379),\n",
      " 'map_75': tensor(0.0473),\n",
      " 'map_large': tensor(0.1832),\n",
      " 'map_medium': tensor(0.0069),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1165),\n",
      " 'mar_10': tensor(0.2504),\n",
      " 'mar_100': tensor(0.2519),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.2956),\n",
      " 'mar_medium': tensor(0.0067),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 36\n",
      "lr = 0.0009754936725752357\n",
      "train_loss = 1.0996855786442756\n",
      "train_cls_loss = 0.17079241905361414\n",
      "train_reg_loss = 0.32783258937299253\n",
      "train_cen_loss = 0.6010605725646019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 37\n",
      "valid_loss = 1.279657461643219\n",
      "valid_cls_loss = 0.2522018751502037\n",
      "valid_reg_loss = 0.41402914732694623\n",
      "valid_cen_loss = 0.6134264373779297\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1716),\n",
      " 'map_50': tensor(0.5474),\n",
      " 'map_75': tensor(0.0313),\n",
      " 'map_large': tensor(0.2011),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1068),\n",
      " 'mar_10': tensor(0.2880),\n",
      " 'mar_100': tensor(0.2932),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3451),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 37\n",
      "lr = 0.0009680781270590751\n",
      "train_loss = 1.0870029148459435\n",
      "train_cls_loss = 0.16827053312212228\n",
      "train_reg_loss = 0.3173671278357506\n",
      "train_cen_loss = 0.6013652580976486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 38\n",
      "valid_loss = 1.2208687269687652\n",
      "valid_cls_loss = 0.22705426931381226\n",
      "valid_reg_loss = 0.3767026153206825\n",
      "valid_cen_loss = 0.6171118378639221\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1946),\n",
      " 'map_50': tensor(0.6034),\n",
      " 'map_75': tensor(0.0779),\n",
      " 'map_large': tensor(0.2272),\n",
      " 'map_medium': tensor(0.0074),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1233),\n",
      " 'mar_10': tensor(0.3188),\n",
      " 'mar_100': tensor(0.3346),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3885),\n",
      " 'mar_medium': tensor(0.0400),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 38\n",
      "lr = 0.0009597199385158189\n",
      "train_loss = 1.102399517893791\n",
      "train_cls_loss = 0.1726223338767886\n",
      "train_reg_loss = 0.3266926968097687\n",
      "train_cen_loss = 0.6030844885110855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 39\n",
      "valid_loss = 1.268483978509903\n",
      "valid_cls_loss = 0.24402828261256218\n",
      "valid_reg_loss = 0.3930867999792099\n",
      "valid_cen_loss = 0.6313689064979553\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1473),\n",
      " 'map_50': tensor(0.4450),\n",
      " 'map_75': tensor(0.0312),\n",
      " 'map_large': tensor(0.1722),\n",
      " 'map_medium': tensor(0.0017),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1165),\n",
      " 'mar_10': tensor(0.2594),\n",
      " 'mar_100': tensor(0.2594),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3044),\n",
      " 'mar_medium': tensor(0.0067),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 39\n",
      "lr = 0.0009504359392248641\n",
      "train_loss = 1.0832774192094803\n",
      "train_cls_loss = 0.16231244567781686\n",
      "train_reg_loss = 0.3205079750716686\n",
      "train_cen_loss = 0.6004569986462593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 40\n",
      "valid_loss = 1.194223676919937\n",
      "valid_cls_loss = 0.2058141940832138\n",
      "valid_reg_loss = 0.3695949837565422\n",
      "valid_cen_loss = 0.6188145053386688\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1822),\n",
      " 'map_50': tensor(0.5508),\n",
      " 'map_75': tensor(0.0620),\n",
      " 'map_large': tensor(0.2126),\n",
      " 'map_medium': tensor(0.0173),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1361),\n",
      " 'mar_10': tensor(0.3030),\n",
      " 'mar_100': tensor(0.3090),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3584),\n",
      " 'mar_medium': tensor(0.0400),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 40\n",
      "lr = 0.0009402448259252435\n",
      "train_loss = 1.0719247230887412\n",
      "train_cls_loss = 0.16138931274414062\n",
      "train_reg_loss = 0.3104084661602974\n",
      "train_cen_loss = 0.6001269400119782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 41\n",
      "valid_loss = 1.2620433783531189\n",
      "valid_cls_loss = 0.2425130194425583\n",
      "valid_reg_loss = 0.39901400834321976\n",
      "valid_cen_loss = 0.6205163431167603\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1376),\n",
      " 'map_50': tensor(0.4608),\n",
      " 'map_75': tensor(0.0175),\n",
      " 'map_large': tensor(0.1614),\n",
      " 'map_medium': tensor(0.0069),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1023),\n",
      " 'mar_10': tensor(0.2571),\n",
      " 'mar_100': tensor(0.2617),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3071),\n",
      " 'mar_medium': tensor(0.0067),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 41\n",
      "lr = 0.000929167122162882\n",
      "train_loss = 1.067229661345482\n",
      "train_cls_loss = 0.16379123136401177\n",
      "train_reg_loss = 0.303094819560647\n",
      "train_cen_loss = 0.6003436121344566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 42\n",
      "valid_loss = 1.2318152678012848\n",
      "valid_cls_loss = 0.23209682375192642\n",
      "valid_reg_loss = 0.38211481004953385\n",
      "valid_cen_loss = 0.6176036381721497\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.2068),\n",
      " 'map_50': tensor(0.6101),\n",
      " 'map_75': tensor(0.0767),\n",
      " 'map_large': tensor(0.2430),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1353),\n",
      " 'mar_10': tensor(0.3128),\n",
      " 'mar_100': tensor(0.3135),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3690),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 42\n",
      "lr = 0.0009172251369589075\n",
      "train_loss = 1.0382198038697243\n",
      "train_cls_loss = 0.14650294199585914\n",
      "train_reg_loss = 0.299331029728055\n",
      "train_cen_loss = 0.5923858338594437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 43\n",
      "valid_loss = 1.2795903539657594\n",
      "valid_cls_loss = 0.2664282463490963\n",
      "valid_reg_loss = 0.396060619354248\n",
      "valid_cen_loss = 0.617101491689682\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1695),\n",
      " 'map_50': tensor(0.5480),\n",
      " 'map_75': tensor(0.0377),\n",
      " 'map_large': tensor(0.1991),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1218),\n",
      " 'mar_10': tensor(0.2752),\n",
      " 'mar_100': tensor(0.2850),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3354),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 43\n",
      "lr = 0.0009044429198822497\n",
      "train_loss = 1.0181884226202964\n",
      "train_cls_loss = 0.13752731174230576\n",
      "train_reg_loss = 0.2871160626411438\n",
      "train_cen_loss = 0.5935450506210327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 19.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 44\n",
      "valid_loss = 1.2096754479408265\n",
      "valid_cls_loss = 0.21106788352131844\n",
      "valid_reg_loss = 0.3876765584945679\n",
      "valid_cen_loss = 0.6109310114383697\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1379),\n",
      " 'map_50': tensor(0.4305),\n",
      " 'map_75': tensor(0.0417),\n",
      " 'map_large': tensor(0.1607),\n",
      " 'map_medium': tensor(0.0004),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1083),\n",
      " 'mar_10': tensor(0.2662),\n",
      " 'mar_100': tensor(0.2850),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3345),\n",
      " 'mar_medium': tensor(0.0067),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 44\n",
      "lr = 0.0008908462126170102\n",
      "train_loss = 1.0038299307227134\n",
      "train_cls_loss = 0.13381182530894875\n",
      "train_reg_loss = 0.27873248398303985\n",
      "train_cen_loss = 0.5912856194376945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 45\n",
      "valid_loss = 1.2544016063213348\n",
      "valid_cls_loss = 0.2562096467614174\n",
      "valid_reg_loss = 0.3842126655578613\n",
      "valid_cen_loss = 0.6139792966842651\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1828),\n",
      " 'map_50': tensor(0.5879),\n",
      " 'map_75': tensor(0.0588),\n",
      " 'map_large': tensor(0.2140),\n",
      " 'map_medium': tensor(0.0054),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1248),\n",
      " 'mar_10': tensor(0.2962),\n",
      " 'mar_100': tensor(0.2970),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3460),\n",
      " 'mar_medium': tensor(0.0267),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 45\n",
      "lr = 0.0008764623971221354\n",
      "train_loss = 1.0204938662052154\n",
      "train_cls_loss = 0.14265036525204777\n",
      "train_reg_loss = 0.2872580546885729\n",
      "train_cen_loss = 0.5905854451656342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 46\n",
      "valid_loss = 1.2473805522918702\n",
      "valid_cls_loss = 0.24451598301529884\n",
      "valid_reg_loss = 0.3854181391000748\n",
      "valid_cen_loss = 0.61744642496109\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.2011),\n",
      " 'map_50': tensor(0.6432),\n",
      " 'map_75': tensor(0.0555),\n",
      " 'map_large': tensor(0.2350),\n",
      " 'map_medium': tensor(0.0035),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1278),\n",
      " 'mar_10': tensor(0.3180),\n",
      " 'mar_100': tensor(0.3353),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3938),\n",
      " 'mar_medium': tensor(0.0067),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 46\n",
      "lr = 0.000861320440487796\n",
      "train_loss = 0.9749607568979264\n",
      "train_cls_loss = 0.1207557594962418\n",
      "train_reg_loss = 0.2670153622329235\n",
      "train_cen_loss = 0.5871896344423294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 47\n",
      "valid_loss = 1.3762449383735658\n",
      "valid_cls_loss = 0.3703518095612526\n",
      "valid_reg_loss = 0.3888108482956886\n",
      "valid_cen_loss = 0.6170822763442994\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1437),\n",
      " 'map_50': tensor(0.4296),\n",
      " 'map_75': tensor(0.0274),\n",
      " 'map_large': tensor(0.1684),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1248),\n",
      " 'mar_10': tensor(0.2278),\n",
      " 'mar_100': tensor(0.2278),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.2681),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 47\n",
      "lr = 0.0008454508365995225\n",
      "train_loss = 0.9805250644683838\n",
      "train_cls_loss = 0.1303615831024945\n",
      "train_reg_loss = 0.26209300860762597\n",
      "train_cen_loss = 0.5880704766511917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 48\n",
      "valid_loss = 1.258397102355957\n",
      "valid_cls_loss = 0.24214180037379265\n",
      "valid_reg_loss = 0.3992984837293625\n",
      "valid_cen_loss = 0.6169568228721619\n",
      "{'classes': tensor(1, dtype=torch.int32),\n",
      " 'map': tensor(0.1619),\n",
      " 'map_50': tensor(0.5524),\n",
      " 'map_75': tensor(0.0365),\n",
      " 'map_large': tensor(0.1901),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.1038),\n",
      " 'mar_10': tensor(0.2857),\n",
      " 'mar_100': tensor(0.2932),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3451),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 48\n",
      "lr = 0.0008288855447275769\n",
      "train_loss = 0.9598958098888397\n",
      "train_cls_loss = 0.11956895435228944\n",
      "train_reg_loss = 0.25585189159959554\n",
      "train_cen_loss = 0.5844749611616135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|███████████████████████████████████████████████████████████████████████████████▏                                        | 33/50 [00:01<00:00, 18.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m\n\u001b[1;32m     25\u001b[0m image \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(dev)\n\u001b[1;32m     26\u001b[0m cls_logits, bbox_reg, centerness \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[1;32m     27\u001b[0m losses \u001b[38;5;241m=\u001b[39m eval_losses(\n\u001b[1;32m     28\u001b[0m     cls_logits,\n\u001b[1;32m     29\u001b[0m     bbox_reg,\n\u001b[1;32m     30\u001b[0m     centerness,\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mcls_tgts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     32\u001b[0m     reg_tgts\u001b[38;5;241m.\u001b[39mto(dev)\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m valid_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#gc.collect()\n",
    "#torch.cuda.empty_cache()\n",
    "#gc.collect()\n",
    "\n",
    "start_epoch = 0\n",
    "for epoch in range(start_epoch, 100):\n",
    "    time_ep = time.time()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_cls_loss = 0\n",
    "    valid_reg_loss = 0\n",
    "    valid_cen_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        map_metric = MeanAveragePrecision(\n",
    "            box_format='xywh', # make sure your dataset outputs target in xywh format\n",
    "            backend='faster_coco_eval'\n",
    "        )\n",
    "        for batch in tqdm(val_dataloader, total=len(val_dataloader)):\n",
    "            cls_tgts, reg_tgts = associate_targets_to_anchors(\n",
    "                batch['target'],\n",
    "                anchors,\n",
    "                anchor_sizes\n",
    "            )\n",
    "            image = batch[\"image\"].to(dev)\n",
    "            cls_logits, bbox_reg, centerness = model(image)\n",
    "            losses = eval_losses(\n",
    "                cls_logits,\n",
    "                bbox_reg,\n",
    "                centerness,\n",
    "                cls_tgts.to(dev),\n",
    "                reg_tgts.to(dev)\n",
    "            )\n",
    "            loss = losses['combined_loss']\n",
    "            valid_loss += loss.detach().item()\n",
    "            valid_cls_loss += losses[\"cls_loss\"].detach().item()\n",
    "            valid_reg_loss += losses[\"reg_loss\"].detach().item()\n",
    "            valid_cen_loss += losses[\"centerness_loss\"].detach().item()\n",
    "            b,c,h,w = image.shape\n",
    "            preds = post_process_batch(\n",
    "                cls_logits.to(\"cpu\"),\n",
    "                bbox_reg.to(\"cpu\"),\n",
    "                centerness.to(\"cpu\"),\n",
    "                (h,w)\n",
    "            )\n",
    "            map_metric.update(preds, batch['target'])\n",
    "        valid_loss /= len(val_dataloader)\n",
    "        valid_cls_loss /= len(val_dataloader)\n",
    "        valid_reg_loss /= len(val_dataloader)\n",
    "        valid_cen_loss /= len(val_dataloader)\n",
    "        mapmetrics = map_metric.compute()\n",
    "        print(f\"{epoch = }\")\n",
    "        print(f\"{valid_loss = }\")\n",
    "        print(f\"{valid_cls_loss = }\")\n",
    "        print(f\"{valid_reg_loss = }\")\n",
    "        print(f\"{valid_cen_loss = }\")\n",
    "        print(pformat(mapmetrics))\n",
    "        map_metric.reset()\n",
    "    train_loss = 0\n",
    "    train_cls_loss = 0\n",
    "    train_reg_loss = 0\n",
    "    train_cen_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "        image = batch[\"image\"].to(dev)               \n",
    "        optimizer.zero_grad()\n",
    "        cls_logits, bbox_reg, centerness = model(image)\n",
    "        cls_tgts, reg_tgts = associate_targets_to_anchors(\n",
    "            batch['target'],\n",
    "            anchors,\n",
    "            anchor_sizes\n",
    "        ) # BxNumAnchors, BxNumAnchorsx4    \n",
    "        losses = eval_losses(\n",
    "            cls_logits,\n",
    "            bbox_reg,\n",
    "            centerness,\n",
    "            cls_tgts.to(dev),\n",
    "            reg_tgts.to(dev)\n",
    "        )\n",
    "        loss = losses['combined_loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        train_loss += loss.detach().item()\n",
    "        train_cls_loss += losses[\"cls_loss\"].detach().item()\n",
    "        train_reg_loss += losses[\"reg_loss\"].detach().item()\n",
    "        train_cen_loss += losses[\"centerness_loss\"].detach().item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_cls_loss /= len(train_dataloader)\n",
    "    train_reg_loss /= len(train_dataloader)\n",
    "    train_cen_loss /= len(train_dataloader)\n",
    "    print(f\"{epoch = }\")\n",
    "    print(f\"lr = {lr_scheduler.get_last_lr()[0]}\"),\n",
    "    print(f\"{train_loss = }\")\n",
    "    print(f\"{train_cls_loss = }\")\n",
    "    print(f\"{train_reg_loss = }\")\n",
    "    print(f\"{train_cen_loss = }\")\n",
    "    \n",
    "    time_ep = time.time() - time_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f633afe-0349-4224-9d70-2d7c6e034893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_38",
   "language": "python",
   "name": "dl_toolbox_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
