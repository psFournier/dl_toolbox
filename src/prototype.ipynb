{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5357bdfe-eb44-475a-8556-f462960974dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import random\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd7de2b-733a-4831-b3c9-326b8a9f4359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1694728-0db8-473a-a9ca-2f6a91610b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import List, Tuple\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class FewShotDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Abstract class for all datasets used in a context of Few-Shot Learning.\n",
    "    The tools we use in few-shot learning, especially TaskSampler, expect an\n",
    "    implementation of FewShotDataset.\n",
    "    Compared to PyTorch's Dataset, FewShotDataset forces a method get_labels.\n",
    "    This exposes the list of all items labels and therefore allows to sample\n",
    "    items depending on their label.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, item: int) -> Tuple[Tensor, int]:\n",
    "        raise NotImplementedError(\n",
    "            \"All PyTorch datasets, including few-shot datasets, need a __getitem__ method.\"\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self) -> int:\n",
    "        raise NotImplementedError(\n",
    "            \"All PyTorch datasets, including few-shot datasets, need a __len__ method.\"\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_labels(self) -> List[int]:\n",
    "        raise NotImplementedError(\n",
    "            \"Implementations of FewShotDataset need a get_labels method.\"\n",
    "        )\n",
    "\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Optional, Set, Tuple, Union\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#from .default_configs import DEFAULT_IMAGE_FORMATS, default_transform\n",
    "#from .few_shot_dataset import FewShotDataset\n",
    "\n",
    "import torchvision.transforms.v2 as T\n",
    "IMAGENET_NORMALIZATION = {\"mean\": [0.485, 0.456, 0.406], \"std\": [0.229, 0.224, 0.225]}\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def default_transform(image_size: int, training: bool) -> Callable:\n",
    "    \"\"\"\n",
    "    Create a composition of torchvision transformations, with some randomization if we are\n",
    "        building a training set.\n",
    "    Args:\n",
    "        image_size: size of dataset images\n",
    "        training: whether this is a training set or not\n",
    "\n",
    "    Returns:\n",
    "        compositions of torchvision transformations\n",
    "    \"\"\"\n",
    "    return (\n",
    "        T.Compose(\n",
    "            [\n",
    "                T.RandomResizedCrop(image_size),\n",
    "                T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(**IMAGENET_NORMALIZATION),\n",
    "            ]\n",
    "        )\n",
    "        if training\n",
    "        else T.Compose(\n",
    "            [\n",
    "                T.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n",
    "                T.CenterCrop(image_size),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(**IMAGENET_NORMALIZATION),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class EasySet(FewShotDataset):\n",
    "    \"\"\"\n",
    "    A ready-to-use dataset. Will work for any dataset where the images are\n",
    "    grouped in directories by class. It expects a JSON file defining the\n",
    "    classes and where to find them. It must have the following shape:\n",
    "        {\n",
    "            \"class_names\": [\n",
    "                \"class_1\",\n",
    "                \"class_2\"\n",
    "            ],\n",
    "            \"class_roots\": [\n",
    "                \"path/to/class_1_folder\",\n",
    "                \"path/to/class_2_folder\"\n",
    "            ]\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        specs_file: Union[Path, str],\n",
    "        image_size: int = 84,\n",
    "        transform: Optional[Callable] = None,\n",
    "        training: bool = False,\n",
    "        supported_formats: Optional[Set[str]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            specs_file: path to the JSON file\n",
    "            image_size: images returned by the dataset will be square images of the given size\n",
    "            transform: torchvision transforms to be applied to images. If none is provided,\n",
    "                we use some standard transformations including ImageNet normalization.\n",
    "                These default transformations depend on the \"training\" argument.\n",
    "            training: preprocessing is slightly different for a training set, adding a random\n",
    "                cropping and a random horizontal flip. Only used if transforms = None.\n",
    "            supported_formats: set of allowed file format. When listing data instances, EasySet\n",
    "                will only consider these files. If none is provided, we use the default set of\n",
    "                image formats.\n",
    "        \"\"\"\n",
    "        specs = self.load_specs(Path(specs_file))\n",
    "\n",
    "        self.images, self.labels = self.list_data_instances(\n",
    "            specs[\"class_roots\"], supported_formats=supported_formats\n",
    "        )\n",
    "\n",
    "        self.class_names = specs[\"class_names\"]\n",
    "        \n",
    "        self.transform = (\n",
    "            transform if transform else default_transform(image_size, training)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def load_specs(specs_file: Path) -> dict:\n",
    "        \"\"\"\n",
    "        Load specs from a JSON file.\n",
    "        Args:\n",
    "            specs_file: path to the JSON file\n",
    "\n",
    "        Returns:\n",
    "            dictionary contained in the JSON file\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if specs_file is not a JSON, or if it is a JSON and the content is not\n",
    "                of the expected shape.\n",
    "        \"\"\"\n",
    "\n",
    "        if specs_file.suffix != \".json\":\n",
    "            raise ValueError(\"EasySet requires specs in a JSON file.\")\n",
    "\n",
    "        with open(specs_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            specs = json.load(file)\n",
    "\n",
    "        if \"class_names\" not in specs.keys() or \"class_roots\" not in specs.keys():\n",
    "            raise ValueError(\n",
    "                \"EasySet requires specs in a JSON file with the keys class_names and class_roots.\"\n",
    "            )\n",
    "\n",
    "        if len(specs[\"class_names\"]) != len(specs[\"class_roots\"]):\n",
    "            raise ValueError(\n",
    "                \"Number of class names does not match the number of class root directories.\"\n",
    "            )\n",
    "\n",
    "        return specs\n",
    "\n",
    "    @staticmethod\n",
    "    def list_data_instances(\n",
    "        class_roots: List[str], supported_formats: Optional[Set[str]] = None\n",
    "    ) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        Explore the directories specified in class_roots to find all data instances.\n",
    "        Args:\n",
    "            class_roots: each element is the path to the directory containing the elements\n",
    "                of one class\n",
    "            supported_formats: set of allowed file format. When listing data instances, EasySet\n",
    "                will only consider these files. If none is provided, we use the default set of\n",
    "                image formats.\n",
    "\n",
    "        Returns:\n",
    "            list of paths to the images, and a list of same length containing the integer label\n",
    "                of each image\n",
    "        \"\"\"\n",
    "        #if supported_formats is None:\n",
    "        #    supported_formats = DEFAULT_IMAGE_FORMATS\n",
    "\n",
    "        images = []\n",
    "        labels = []\n",
    "        for class_id, class_root in enumerate(class_roots):\n",
    "            class_root = class_root[1:] # added to remove . at the start\n",
    "            class_images = [\n",
    "                str(image_path)\n",
    "                for image_path in sorted(Path(class_root).glob(\"*\"))\n",
    "                if image_path.is_file()\n",
    "                & (image_path.suffix.lower() in supported_formats)\n",
    "            ]\n",
    "\n",
    "            images += class_images\n",
    "            labels += len(class_images) * [class_id]\n",
    "\n",
    "        if len(images) == 0:\n",
    "            warnings.warn(\n",
    "                UserWarning(\n",
    "                    \"No images found in the specified directories. The dataset will be empty.\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def __getitem__(self, item: int):\n",
    "        \"\"\"\n",
    "        Get a data sample from its integer id.\n",
    "        Args:\n",
    "            item: sample's integer id\n",
    "\n",
    "        Returns:\n",
    "            data sample in the form of a tuple (image, label), where label is an integer.\n",
    "            The type of the image object depends of the output type of self.transform. By default\n",
    "            it's a torch.Tensor, however you are free to define any function as self.transform, and\n",
    "            therefore any type for the output image. For instance, if self.transform = lambda x: x,\n",
    "            then the output image will be of type PIL.Image.Image.\n",
    "        \"\"\"\n",
    "        # Some images of ILSVRC2015 are grayscale, so we convert everything to RGB for consistence.\n",
    "        # If you want to work on grayscale images, use torch.transforms.Grayscale in your\n",
    "        # transformation pipeline.\n",
    "        img = self.transform(Image.open(self.images[item]).convert(\"RGB\"))\n",
    "        label = self.labels[item]\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_labels(self) -> List[int]:\n",
    "        return self.labels\n",
    "\n",
    "    def number_of_classes(self):\n",
    "        return len(self.class_names)\n",
    "    \n",
    "from pathlib import Path\n",
    "\n",
    "#from .easy_set import EasySet\n",
    "\n",
    "CUB_SPECS_DIR = Path(\"/data/CUB\")\n",
    "\n",
    "\n",
    "class CUB(EasySet):\n",
    "    def __init__(self, split: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Build the CUB dataset for the specific split.\n",
    "        Args:\n",
    "            split: one of the available split (typically train, val, test).\n",
    "        Raises:\n",
    "            ValueError: if the specified split cannot be associated with a JSON spec file\n",
    "                from CUB's specs directory\n",
    "        \"\"\"\n",
    "        specs_file = CUB_SPECS_DIR / f\"{split}.json\"\n",
    "        if not specs_file.is_file():\n",
    "            raise ValueError(\n",
    "                f\"Could not find specs file {specs_file.name} in {CUB_SPECS_DIR}\"\n",
    "            )\n",
    "        super().__init__(specs_file=specs_file, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d267275-e415-41db-bf7a-21bfcb48d8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv38/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "n_workers = 4\n",
    "\n",
    "train_set = CUB(split=\"train\", training=True, supported_formats={\".bmp\", \".png\", \".jpeg\", \".jpg\"})\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "918e1ef0-8675-48dd-a870-3cca76b535ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "model = timm.create_model(\n",
    "    'resnet18',\n",
    "    pretrained=True,\n",
    "    num_classes=len(set(train_set.get_labels()))\n",
    ").to(DEVICE)\n",
    "\n",
    "#model = resnet12(\n",
    "#    use_fc=True,\n",
    "#    num_classes=len(set(train_set.get_labels())),\n",
    "#).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdbc92e0-ef6b-4d58-a03e-52d983332e19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "#from easyfsl.methods.utils import compute_prototypes\n",
    "\n",
    "def compute_prototypes(support_features: Tensor, support_labels: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute class prototypes from support features and labels\n",
    "    Args:\n",
    "        support_features: for each instance in the support set, its feature vector\n",
    "        support_labels: for each instance in the support set, its label\n",
    "\n",
    "    Returns:\n",
    "        for each label of the support set, the average feature vector of instances with this label\n",
    "    \"\"\"\n",
    "\n",
    "    n_way = len(torch.unique(support_labels))\n",
    "    # Prototype i is the mean of all instances of features corresponding to labels == i\n",
    "    return torch.cat(\n",
    "        [\n",
    "            support_features[torch.nonzero(support_labels == label)].mean(0)\n",
    "            for label in range(n_way)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "class FewShotClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract class providing methods usable by all few-shot classification algorithms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: Optional[nn.Module] = None,\n",
    "        use_softmax: bool = False,\n",
    "        feature_centering: Optional[Tensor] = None,\n",
    "        feature_normalization: Optional[float] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Few-Shot Classifier\n",
    "        Args:\n",
    "            backbone: the feature extractor used by the method. Must output a tensor of the\n",
    "                appropriate shape (depending on the method).\n",
    "                If None is passed, the backbone will be initialized as nn.Identity().\n",
    "            use_softmax: whether to return predictions as soft probabilities\n",
    "            feature_centering: a features vector on which to center all computed features.\n",
    "                If None is passed, no centering is performed.\n",
    "            feature_normalization: a value by which to normalize all computed features after centering.\n",
    "                It is used as the p argument in torch.nn.functional.normalize().\n",
    "                If None is passed, no normalization is performed.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone if backbone is not None else nn.Identity()\n",
    "        self.use_softmax = use_softmax\n",
    "\n",
    "        self.prototypes = torch.tensor(())\n",
    "        self.support_features = torch.tensor(())\n",
    "        self.support_labels = torch.tensor(())\n",
    "\n",
    "        self.feature_centering = (\n",
    "            feature_centering if feature_centering is not None else torch.tensor(0)\n",
    "        )\n",
    "        self.feature_normalization = feature_normalization\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(\n",
    "        self,\n",
    "        query_images: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Predict classification labels.\n",
    "        Args:\n",
    "            query_images: images of the query set of shape (n_query, **image_shape)\n",
    "        Returns:\n",
    "            a prediction of classification scores for query images of shape (n_query, n_classes)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"All few-shot algorithms must implement a forward method.\"\n",
    "        )\n",
    "\n",
    "    def process_support_set(\n",
    "        self,\n",
    "        support_images: Tensor,\n",
    "        support_labels: Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Harness information from the support set, so that query labels can later be predicted using a forward call.\n",
    "        The default behaviour shared by most few-shot classifiers is to compute prototypes and store the support set.\n",
    "        Args:\n",
    "            support_images: images of the support set of shape (n_support, **image_shape)\n",
    "            support_labels: labels of support set images of shape (n_support, )\n",
    "        \"\"\"\n",
    "        self.compute_prototypes_and_store_support_set(support_images, support_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_transductive() -> bool:\n",
    "        raise NotImplementedError(\n",
    "            \"All few-shot algorithms must implement a is_transductive method.\"\n",
    "        )\n",
    "\n",
    "    def compute_features(self, images: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute features from images and perform centering and normalization.\n",
    "        Args:\n",
    "            images: images of shape (n_images, **image_shape)\n",
    "        Returns:\n",
    "            features of shape (n_images, feature_dimension)\n",
    "        \"\"\"\n",
    "        original_features = self.backbone(images)\n",
    "        centered_features = original_features - self.feature_centering\n",
    "        if self.feature_normalization is not None:\n",
    "            return nn.functional.normalize(\n",
    "                centered_features, p=self.feature_normalization, dim=1\n",
    "            )\n",
    "        return centered_features\n",
    "\n",
    "    def softmax_if_specified(self, output: Tensor, temperature: float = 1.0) -> Tensor:\n",
    "        \"\"\"\n",
    "        If the option is chosen when the classifier is initialized, we perform a softmax on the\n",
    "        output in order to return soft probabilities.\n",
    "        Args:\n",
    "            output: output of the forward method of shape (n_query, n_classes)\n",
    "            temperature: temperature of the softmax\n",
    "        Returns:\n",
    "            output as it was, or output as soft probabilities, of shape (n_query, n_classes)\n",
    "        \"\"\"\n",
    "        return (temperature * output).softmax(-1) if self.use_softmax else output\n",
    "\n",
    "    def l2_distance_to_prototypes(self, samples: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute prediction logits from their euclidean distance to support set prototypes.\n",
    "        Args:\n",
    "            samples: features of the items to classify of shape (n_samples, feature_dimension)\n",
    "        Returns:\n",
    "            prediction logits of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        return -torch.cdist(samples, self.prototypes)\n",
    "\n",
    "    def cosine_distance_to_prototypes(self, samples) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute prediction logits from their cosine distance to support set prototypes.\n",
    "        Args:\n",
    "            samples: features of the items to classify of shape (n_samples, feature_dimension)\n",
    "        Returns:\n",
    "            prediction logits of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        return (\n",
    "            nn.functional.normalize(samples, dim=1)\n",
    "            @ nn.functional.normalize(self.prototypes, dim=1).T\n",
    "        )\n",
    "\n",
    "    def compute_prototypes_and_store_support_set(\n",
    "        self,\n",
    "        support_images: Tensor,\n",
    "        support_labels: Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extract support features, compute prototypes, and store support labels, features, and prototypes.\n",
    "        Args:\n",
    "            support_images: images of the support set of shape (n_support, **image_shape)\n",
    "            support_labels: labels of support set images of shape (n_support, )\n",
    "        \"\"\"\n",
    "        self.support_labels = support_labels\n",
    "        self.support_features = self.compute_features(support_images)\n",
    "        self._raise_error_if_features_are_multi_dimensional(self.support_features)\n",
    "        self.prototypes = compute_prototypes(self.support_features, support_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def _raise_error_if_features_are_multi_dimensional(features: Tensor):\n",
    "        if len(features.shape) != 2:\n",
    "            raise ValueError(\n",
    "                \"Illegal backbone or feature shape. \"\n",
    "                \"Expected output for an image is a 1-dim tensor.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79be1f02-0a47-4f3b-a962-9fc110a5d7ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrototypicalNetworks(FewShotClassifier):\n",
    "    \"\"\"\n",
    "    Jake Snell, Kevin Swersky, and Richard S. Zemel.\n",
    "    \"Prototypical networks for few-shot learning.\" (2017)\n",
    "    https://arxiv.org/abs/1703.05175\n",
    "\n",
    "    Prototypical networks extract feature vectors for both support and query images. Then it\n",
    "    computes the mean of support features for each class (called prototypes), and predict\n",
    "    classification scores for query images based on their euclidean distance to the prototypes.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query_images: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Overrides forward method of FewShotClassifier.\n",
    "        Predict query labels based on their distance to class prototypes in the feature space.\n",
    "        Classification scores are the negative of euclidean distances.\n",
    "        \"\"\"\n",
    "        # Extract the features of query images\n",
    "        query_features = self.compute_features(query_images)\n",
    "        self._raise_error_if_features_are_multi_dimensional(query_features)\n",
    "\n",
    "        # Compute the euclidean distance from queries to prototypes\n",
    "        scores = self.l2_distance_to_prototypes(query_features)\n",
    "\n",
    "        return self.softmax_if_specified(scores)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_transductive() -> bool:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec92b974-bbb6-4faa-bf31-f539e0492987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "few_shot_classifier = PrototypicalNetworks(model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3e1cdc-4175-494b-b2a1-d3887421674b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "from typing import Dict, Iterator, List, Tuple, Union\n",
    "\n",
    "class TaskSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Samples batches in the shape of few-shot classification tasks. At each iteration, it will sample\n",
    "    n_way classes, and then sample support and query images from these classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: FewShotDataset,\n",
    "        n_way: int,\n",
    "        n_shot: int,\n",
    "        n_query: int,\n",
    "        n_tasks: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: dataset from which to sample classification tasks. Must have implement get_labels() from\n",
    "                FewShotDataset.\n",
    "            n_way: number of classes in one task\n",
    "            n_shot: number of support images for each class in one task\n",
    "            n_query: number of query images for each class in one task\n",
    "            n_tasks: number of tasks to sample\n",
    "        \"\"\"\n",
    "        super().__init__(data_source=None)\n",
    "        self.n_way = n_way\n",
    "        self.n_shot = n_shot\n",
    "        self.n_query = n_query\n",
    "        self.n_tasks = n_tasks\n",
    "\n",
    "        self.items_per_label: Dict[int, List[int]] = {}\n",
    "        for item, label in enumerate(dataset.get_labels()):\n",
    "            if label in self.items_per_label:\n",
    "                self.items_per_label[label].append(item)\n",
    "            else:\n",
    "                self.items_per_label[label] = [item]\n",
    "\n",
    "        self._check_dataset_size_fits_sampler_parameters()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.n_tasks\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        \"\"\"\n",
    "        Sample n_way labels uniformly at random,\n",
    "        and then sample n_shot + n_query items for each label, also uniformly at random.\n",
    "        Yields:\n",
    "            a list of indices of length (n_way * (n_shot + n_query))\n",
    "        \"\"\"\n",
    "        for _ in range(self.n_tasks):\n",
    "            yield torch.cat(\n",
    "                [\n",
    "                    torch.tensor(\n",
    "                        random.sample(\n",
    "                            self.items_per_label[label], self.n_shot + self.n_query\n",
    "                        )\n",
    "                    )\n",
    "                    for label in random.sample(\n",
    "                        sorted(self.items_per_label.keys()), self.n_way\n",
    "                    )\n",
    "                ]\n",
    "            ).tolist()\n",
    "\n",
    "    def episodic_collate_fn(\n",
    "        self, input_data: List[Tuple[Tensor, Union[Tensor, int]]]\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, List[int]]:\n",
    "        \"\"\"\n",
    "        Collate function to be used as argument for the collate_fn parameter of episodic\n",
    "            data loaders.\n",
    "        Args:\n",
    "            input_data: each element is a tuple containing:\n",
    "                - an image as a torch Tensor of shape (n_channels, height, width)\n",
    "                - the label of this image as an int or a 0-dim tensor\n",
    "        Returns:\n",
    "            tuple(Tensor, Tensor, Tensor, Tensor, list[int]): respectively:\n",
    "                - support images of shape (n_way * n_shot, n_channels, height, width),\n",
    "                - their labels of shape (n_way * n_shot),\n",
    "                - query images of shape (n_way * n_query, n_channels, height, width)\n",
    "                - their labels of shape (n_way * n_query),\n",
    "                - the dataset class ids of the class sampled in the episode\n",
    "        \"\"\"\n",
    "        input_data_with_int_labels = self._cast_input_data_to_tensor_int_tuple(\n",
    "            input_data\n",
    "        )\n",
    "        true_class_ids = list({x[1] for x in input_data_with_int_labels})\n",
    "        all_images = torch.cat([x[0].unsqueeze(0) for x in input_data_with_int_labels])\n",
    "        all_images = all_images.reshape(\n",
    "            (self.n_way, self.n_shot + self.n_query, *all_images.shape[1:])\n",
    "        )\n",
    "        all_labels = torch.tensor(\n",
    "            [true_class_ids.index(x[1]) for x in input_data_with_int_labels]\n",
    "        ).reshape((self.n_way, self.n_shot + self.n_query))\n",
    "        support_images = all_images[:, : self.n_shot].reshape(\n",
    "            (-1, *all_images.shape[2:])\n",
    "        )\n",
    "        query_images = all_images[:, self.n_shot :].reshape((-1, *all_images.shape[2:]))\n",
    "        support_labels = all_labels[:, : self.n_shot].flatten()\n",
    "        query_labels = all_labels[:, self.n_shot :].flatten()\n",
    "        return (\n",
    "            support_images,\n",
    "            support_labels,\n",
    "            query_images,\n",
    "            query_labels,\n",
    "            true_class_ids,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _cast_input_data_to_tensor_int_tuple(\n",
    "        input_data: List[Tuple[Tensor, Union[Tensor, int]]]\n",
    "    ) -> List[Tuple[Tensor, int]]:\n",
    "        \"\"\"\n",
    "        Check the type of the input for the episodic_collate_fn method, and cast it to the right type if possible.\n",
    "        Args:\n",
    "            input_data: each element is a tuple containing:\n",
    "                - an image as a torch Tensor of shape (n_channels, height, width)\n",
    "                - the label of this image as an int or a 0-dim tensor\n",
    "        Returns:\n",
    "            the input data with the labels cast to int\n",
    "        Raises:\n",
    "            TypeError : Wrong type of input images or labels\n",
    "            ValueError: Input label is not a 0-dim tensor\n",
    "        \"\"\"\n",
    "        for image, label in input_data:\n",
    "            if not isinstance(image, Tensor):\n",
    "                raise TypeError(\n",
    "                    f\"Illegal type of input instance: {type(image)}. \"\n",
    "                    + GENERIC_TYPING_ERROR_MESSAGE\n",
    "                )\n",
    "            if not isinstance(label, int):\n",
    "                if not isinstance(label, Tensor):\n",
    "                    raise TypeError(\n",
    "                        f\"Illegal type of input label: {type(label)}. \"\n",
    "                        + GENERIC_TYPING_ERROR_MESSAGE\n",
    "                    )\n",
    "                if label.dtype not in {\n",
    "                    torch.uint8,\n",
    "                    torch.int8,\n",
    "                    torch.int16,\n",
    "                    torch.int32,\n",
    "                    torch.int64,\n",
    "                }:\n",
    "                    raise TypeError(\n",
    "                        f\"Illegal dtype of input label tensor: {label.dtype}. \"\n",
    "                        + GENERIC_TYPING_ERROR_MESSAGE\n",
    "                    )\n",
    "                if label.ndim != 0:\n",
    "                    raise ValueError(\n",
    "                        f\"Illegal shape for input label tensor: {label.shape}. \"\n",
    "                        + GENERIC_TYPING_ERROR_MESSAGE\n",
    "                    )\n",
    "\n",
    "        return [(image, int(label)) for (image, label) in input_data]\n",
    "\n",
    "    def _check_dataset_size_fits_sampler_parameters(self):\n",
    "        \"\"\"\n",
    "        Check that the dataset size is compatible with the sampler parameters\n",
    "        \"\"\"\n",
    "        self._check_dataset_has_enough_labels()\n",
    "        self._check_dataset_has_enough_items_per_label()\n",
    "\n",
    "    def _check_dataset_has_enough_labels(self):\n",
    "        if self.n_way > len(self.items_per_label):\n",
    "            raise ValueError(\n",
    "                f\"The number of labels in the dataset ({len(self.items_per_label)} \"\n",
    "                f\"must be greater or equal to n_way ({self.n_way}).\"\n",
    "            )\n",
    "\n",
    "    def _check_dataset_has_enough_items_per_label(self):\n",
    "        number_of_samples_per_label = [\n",
    "            len(items_for_label) for items_for_label in self.items_per_label.values()\n",
    "        ]\n",
    "        minimum_number_of_samples_per_label = min(number_of_samples_per_label)\n",
    "        label_with_minimum_number_of_samples = number_of_samples_per_label.index(\n",
    "            minimum_number_of_samples_per_label\n",
    "        )\n",
    "        if self.n_shot + self.n_query > minimum_number_of_samples_per_label:\n",
    "            raise ValueError(\n",
    "                f\"Label {label_with_minimum_number_of_samples} has only {minimum_number_of_samples_per_label} samples\"\n",
    "                f\"but all classes must have at least n_shot + n_query ({self.n_shot + self.n_query}) samples.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4029bc8a-74ec-49d0-94bf-ca1066148570",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv38/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#from easyfsl.methods import PrototypicalNetworks\n",
    "#from easyfsl.samplers import TaskSampler\n",
    "\n",
    "n_way = 5\n",
    "n_shot = 5\n",
    "n_query = 10\n",
    "n_validation_tasks = 500\n",
    "\n",
    "val_set = CUB(split=\"val\", training=False, supported_formats={\".bmp\", \".png\", \".jpeg\", \".jpg\"})\n",
    "val_sampler = TaskSampler(\n",
    "    val_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_validation_tasks\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_sampler=val_sampler,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=val_sampler.episodic_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19be9ddd-82fc-4f8d-9e63-f0792d4ed63c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Optimizer\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "LOSS_FUNCTION = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 200\n",
    "scheduler_milestones = [150, 180]\n",
    "scheduler_gamma = 0.1\n",
    "learning_rate = 1e-01\n",
    "tb_logs_dir = Path(\"/data/ouputs/cub\")\n",
    "\n",
    "train_optimizer = SGD(\n",
    "    model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4\n",
    ")\n",
    "train_scheduler = MultiStepLR(\n",
    "    train_optimizer,\n",
    "    milestones=scheduler_milestones,\n",
    "    gamma=scheduler_gamma,\n",
    ")\n",
    "\n",
    "tb_writer = SummaryWriter(log_dir=str(tb_logs_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85b0d64a-ecf6-4d42-a553-35c7978020cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_epoch(model_: nn.Module, data_loader: DataLoader, optimizer: Optimizer):\n",
    "    all_loss = []\n",
    "    model_.train()\n",
    "    with tqdm(data_loader, total=len(data_loader), desc=\"Training\") as tqdm_train:\n",
    "        for images, labels in tqdm_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = LOSS_FUNCTION(model_(images.to(DEVICE)), labels.to(DEVICE))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            all_loss.append(loss.item())\n",
    "\n",
    "            tqdm_train.set_postfix(loss=mean(all_loss))\n",
    "\n",
    "    return mean(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1e54125-8572-4677-8ae3-0df71c11a63d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_on_one_task(\n",
    "    model: FewShotClassifier,\n",
    "    support_images: Tensor,\n",
    "    support_labels: Tensor,\n",
    "    query_images: Tensor,\n",
    "    query_labels: Tensor,\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Returns the number of correct predictions of query labels, and the total number of\n",
    "    predictions.\n",
    "    \"\"\"\n",
    "    model.process_support_set(support_images, support_labels)\n",
    "    predictions = model(query_images).detach().data\n",
    "    number_of_correct_predictions = int(\n",
    "        (torch.max(predictions, 1)[1] == query_labels).sum().item()\n",
    "    )\n",
    "    return number_of_correct_predictions, len(query_labels)\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: FewShotClassifier,\n",
    "    data_loader: DataLoader,\n",
    "    device: str = \"cuda\",\n",
    "    use_tqdm: bool = True,\n",
    "    tqdm_prefix: Optional[str] = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on few-shot classification tasks\n",
    "    Args:\n",
    "        model: a few-shot classifier\n",
    "        data_loader: loads data in the shape of few-shot classification tasks*\n",
    "        device: where to cast data tensors.\n",
    "            Must be the same as the device hosting the model's parameters.\n",
    "        use_tqdm: whether to display the evaluation's progress bar\n",
    "        tqdm_prefix: prefix of the tqdm bar\n",
    "    Returns:\n",
    "        average classification accuracy\n",
    "    \"\"\"\n",
    "    # We'll count everything and compute the ratio at the end\n",
    "    total_predictions = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # eval mode affects the behaviour of some layers (such as batch normalization or dropout)\n",
    "    # no_grad() tells torch not to keep in memory the whole computational graph\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # We use a tqdm context to show a progress bar in the logs\n",
    "        with tqdm(\n",
    "            enumerate(data_loader),\n",
    "            total=len(data_loader),\n",
    "            disable=not use_tqdm,\n",
    "            desc=tqdm_prefix,\n",
    "        ) as tqdm_eval:\n",
    "            for _, (\n",
    "                support_images,\n",
    "                support_labels,\n",
    "                query_images,\n",
    "                query_labels,\n",
    "                _,\n",
    "            ) in tqdm_eval:\n",
    "                correct, total = evaluate_on_one_task(\n",
    "                    model,\n",
    "                    support_images.to(device),\n",
    "                    support_labels.to(device),\n",
    "                    query_images.to(device),\n",
    "                    query_labels.to(device),\n",
    "                )\n",
    "\n",
    "                total_predictions += total\n",
    "                correct_predictions += correct\n",
    "\n",
    "                # Log accuracy in real time\n",
    "                tqdm_eval.set_postfix(accuracy=correct_predictions / total_predictions)\n",
    "\n",
    "    return correct_predictions / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fa2f045-cf26-4a1a-920d-3e549dc70016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:05<00:00,  6.70it/s, loss=4.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:03<00:00,  8.62it/s, loss=3.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:03<00:00,  8.65it/s, loss=3.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:03<00:00,  8.60it/s, loss=2.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:04<00:00,  8.41it/s, loss=2.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:03<00:00,  8.51it/s, loss=2.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:04<00:00,  8.43it/s, loss=2.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:04<00:00,  8.27it/s, loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:04<00:00,  8.32it/s, loss=2.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:03<00:00,  8.67it/s, loss=1.98]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'set_use_fc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m average_loss \u001b[38;5;241m=\u001b[39m training_epoch(model, train_loader, train_optimizer)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m validation_frequency \u001b[38;5;241m==\u001b[39m validation_frequency \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# We use this very convenient method from EasyFSL's ResNet to specify\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# that the model shouldn't use its last fully connected layer during validation.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_use_fc\u001b[49m(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m     validation_accuracy \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m     17\u001b[0m         few_shot_classifier, val_loader, device\u001b[38;5;241m=\u001b[39mDEVICE, tqdm_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_use_fc(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/dl_toolbox/venv38/lib/python3.8/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'set_use_fc'"
     ]
    }
   ],
   "source": [
    "#from easyfsl.utils import evaluate\n",
    "\n",
    "\n",
    "best_state = model.state_dict()\n",
    "best_validation_accuracy = 0.0\n",
    "validation_frequency = 1\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    average_loss = training_epoch(model, train_loader, train_optimizer)\n",
    "\n",
    "    if epoch % validation_frequency == validation_frequency - 1:\n",
    "\n",
    "        # We use this very convenient method from EasyFSL's ResNet to specify\n",
    "        # that the model shouldn't use its last fully connected layer during validation.\n",
    "        model.set_use_fc(False)\n",
    "        validation_accuracy = evaluate(\n",
    "            few_shot_classifier, val_loader, device=DEVICE, tqdm_prefix=\"Validation\"\n",
    "        )\n",
    "        model.set_use_fc(True)\n",
    "\n",
    "        if validation_accuracy > best_validation_accuracy:\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "            best_state = copy.deepcopy(few_shot_classifier.state_dict())\n",
    "            # state_dict() returns a reference to the still evolving model's state so we deepcopy\n",
    "            # https://pytorch.org/tutorials/beginner/saving_loading_models\n",
    "            print(\"Ding ding ding! We found a new best model!\")\n",
    "\n",
    "        tb_writer.add_scalar(\"Val/acc\", validation_accuracy, epoch)\n",
    "\n",
    "    tb_writer.add_scalar(\"Train/loss\", average_loss, epoch)\n",
    "\n",
    "    # Warn the scheduler that we did an epoch\n",
    "    # so it knows when to decrease the learning rate\n",
    "    train_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24b82b-338c-4cff-9349-6457ada9db68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_38",
   "language": "python",
   "name": "dl_toolbox_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
