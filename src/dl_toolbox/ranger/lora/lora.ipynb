{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f96ad40-9784-4845-b82e-9c241f69a30b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "import dl_toolbox.datasets as datasets\n",
    "from torch.utils.data import Subset, RandomSampler\n",
    "import torch\n",
    "\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.Resize(size=(224, 224), antialias=True),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "NB_IMG = 45*700\n",
    "dataset = datasets.Resisc('/data/NWPU-RESISC45', transform, 'all45')\n",
    "trainset = Subset(dataset, indices=[i for i in range(NB_IMG) if 100<=i%700])\n",
    "valset = Subset(dataset, indices=[i for i in range(NB_IMG) if 100>i%700])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    sampler=RandomSampler(\n",
    "        trainset,\n",
    "        replacement=True,\n",
    "        num_samples=5000\n",
    "    ),\n",
    "    drop_last=True,\n",
    "    batch_size=4,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valset,\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eb213b9-b5bc-4865-9be9-2c417e7b51b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "def train(model, criterion, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    #optimizer.train()\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        data, target = batch['image'], batch['label']\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c22745-20a6-48e5-ab6e-d04c0814af52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, criterion, optimizer, device, test_loader):\n",
    "    model.eval()\n",
    "    #optimizer.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            data, target = batch['image'], batch['label']\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d86e6f5-7813-4f57-bca4-3f3235dc1d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "import minlora \n",
    "from functools import partial \n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "def get_lora_config(rank):\n",
    "    return {  # specify which layers to add lora to, by default only add to linear layers\n",
    "        nn.Linear: {\n",
    "            \"weight\": partial(minlora.LoRAParametrization.from_linear, rank=rank),\n",
    "        },\n",
    "        #nn.Conv2d: {\n",
    "        #    \"weight\": partial(minlora.LoRAParametrization.from_conv2d, rank=rank),\n",
    "        #},\n",
    "    }\n",
    "\n",
    "class test_lora(nn.Module):\n",
    "    def __init__(self, freeze, lora, rank):\n",
    "        super().__init__()\n",
    "        #self.model = timm.create_model(\n",
    "        #    'efficientnet_b0',\n",
    "        #    pretrained=True,\n",
    "        #    num_classes=45\n",
    "        #)\n",
    "        #self.feature_extractor = create_feature_extractor(\n",
    "        #    self.model,\n",
    "        #    {'bn2.act': 'features'}\n",
    "        #)\n",
    "        self.model = timm.create_model(\n",
    "            'vit_base_patch16_224',\n",
    "            pretrained=True,\n",
    "            global_pool='token',\n",
    "            num_classes=45\n",
    "        )       \n",
    "        self.feature_extractor = create_feature_extractor(\n",
    "            self.model,\n",
    "            {'norm': 'features'}\n",
    "        )\n",
    "        \n",
    "        print(get_graph_node_names(self.model))\n",
    "        if freeze:\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        if lora:\n",
    "            cfg = get_lora_config(rank)\n",
    "            minlora.add_lora(self.feature_extractor, lora_config=cfg)\n",
    "        #self.head = nn.Linear(self.encoder.num_features, 45)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "        #x = self.encoder.forward_features(x)\n",
    "        #x = x[:, self.encoder.num_prefix_tokens:].mean(dim=1)\n",
    "        #x = self.head(x)\n",
    "        #return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "182a7936-7788-4be4-b632-7d01f8f8ebd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['x', 'patch_embed.getattr', 'patch_embed.getitem', 'patch_embed.getitem_1', 'patch_embed.getitem_2', 'patch_embed.getitem_3', 'patch_embed.eq', 'patch_embed._assert', 'patch_embed.eq_1', 'patch_embed._assert_1', 'patch_embed.proj', 'patch_embed.flatten', 'patch_embed.transpose', 'patch_embed.norm', 'pos_embed', 'cls_token', 'getattr', 'getitem', 'expand', 'cat', 'add', 'pos_drop', 'patch_drop', 'norm_pre', 'blocks.0.norm1', 'blocks.0.attn.getattr', 'blocks.0.attn.getitem', 'blocks.0.attn.getitem_1', 'blocks.0.attn.getitem_2', 'blocks.0.attn.qkv', 'blocks.0.attn.reshape', 'blocks.0.attn.permute', 'blocks.0.attn.unbind', 'blocks.0.attn.getitem_3', 'blocks.0.attn.getitem_4', 'blocks.0.attn.getitem_5', 'blocks.0.attn.q_norm', 'blocks.0.attn.k_norm', 'blocks.0.attn.scaled_dot_product_attention', 'blocks.0.attn.transpose', 'blocks.0.attn.reshape_1', 'blocks.0.attn.proj', 'blocks.0.attn.proj_drop', 'blocks.0.ls1', 'blocks.0.drop_path1', 'blocks.0.add', 'blocks.0.norm2', 'blocks.0.mlp.fc1', 'blocks.0.mlp.act', 'blocks.0.mlp.drop1', 'blocks.0.mlp.norm', 'blocks.0.mlp.fc2', 'blocks.0.mlp.drop2', 'blocks.0.ls2', 'blocks.0.drop_path2', 'blocks.0.add_1', 'blocks.1.norm1', 'blocks.1.attn.getattr', 'blocks.1.attn.getitem', 'blocks.1.attn.getitem_1', 'blocks.1.attn.getitem_2', 'blocks.1.attn.qkv', 'blocks.1.attn.reshape', 'blocks.1.attn.permute', 'blocks.1.attn.unbind', 'blocks.1.attn.getitem_3', 'blocks.1.attn.getitem_4', 'blocks.1.attn.getitem_5', 'blocks.1.attn.q_norm', 'blocks.1.attn.k_norm', 'blocks.1.attn.scaled_dot_product_attention', 'blocks.1.attn.transpose', 'blocks.1.attn.reshape_1', 'blocks.1.attn.proj', 'blocks.1.attn.proj_drop', 'blocks.1.ls1', 'blocks.1.drop_path1', 'blocks.1.add', 'blocks.1.norm2', 'blocks.1.mlp.fc1', 'blocks.1.mlp.act', 'blocks.1.mlp.drop1', 'blocks.1.mlp.norm', 'blocks.1.mlp.fc2', 'blocks.1.mlp.drop2', 'blocks.1.ls2', 'blocks.1.drop_path2', 'blocks.1.add_1', 'blocks.2.norm1', 'blocks.2.attn.getattr', 'blocks.2.attn.getitem', 'blocks.2.attn.getitem_1', 'blocks.2.attn.getitem_2', 'blocks.2.attn.qkv', 'blocks.2.attn.reshape', 'blocks.2.attn.permute', 'blocks.2.attn.unbind', 'blocks.2.attn.getitem_3', 'blocks.2.attn.getitem_4', 'blocks.2.attn.getitem_5', 'blocks.2.attn.q_norm', 'blocks.2.attn.k_norm', 'blocks.2.attn.scaled_dot_product_attention', 'blocks.2.attn.transpose', 'blocks.2.attn.reshape_1', 'blocks.2.attn.proj', 'blocks.2.attn.proj_drop', 'blocks.2.ls1', 'blocks.2.drop_path1', 'blocks.2.add', 'blocks.2.norm2', 'blocks.2.mlp.fc1', 'blocks.2.mlp.act', 'blocks.2.mlp.drop1', 'blocks.2.mlp.norm', 'blocks.2.mlp.fc2', 'blocks.2.mlp.drop2', 'blocks.2.ls2', 'blocks.2.drop_path2', 'blocks.2.add_1', 'blocks.3.norm1', 'blocks.3.attn.getattr', 'blocks.3.attn.getitem', 'blocks.3.attn.getitem_1', 'blocks.3.attn.getitem_2', 'blocks.3.attn.qkv', 'blocks.3.attn.reshape', 'blocks.3.attn.permute', 'blocks.3.attn.unbind', 'blocks.3.attn.getitem_3', 'blocks.3.attn.getitem_4', 'blocks.3.attn.getitem_5', 'blocks.3.attn.q_norm', 'blocks.3.attn.k_norm', 'blocks.3.attn.scaled_dot_product_attention', 'blocks.3.attn.transpose', 'blocks.3.attn.reshape_1', 'blocks.3.attn.proj', 'blocks.3.attn.proj_drop', 'blocks.3.ls1', 'blocks.3.drop_path1', 'blocks.3.add', 'blocks.3.norm2', 'blocks.3.mlp.fc1', 'blocks.3.mlp.act', 'blocks.3.mlp.drop1', 'blocks.3.mlp.norm', 'blocks.3.mlp.fc2', 'blocks.3.mlp.drop2', 'blocks.3.ls2', 'blocks.3.drop_path2', 'blocks.3.add_1', 'blocks.4.norm1', 'blocks.4.attn.getattr', 'blocks.4.attn.getitem', 'blocks.4.attn.getitem_1', 'blocks.4.attn.getitem_2', 'blocks.4.attn.qkv', 'blocks.4.attn.reshape', 'blocks.4.attn.permute', 'blocks.4.attn.unbind', 'blocks.4.attn.getitem_3', 'blocks.4.attn.getitem_4', 'blocks.4.attn.getitem_5', 'blocks.4.attn.q_norm', 'blocks.4.attn.k_norm', 'blocks.4.attn.scaled_dot_product_attention', 'blocks.4.attn.transpose', 'blocks.4.attn.reshape_1', 'blocks.4.attn.proj', 'blocks.4.attn.proj_drop', 'blocks.4.ls1', 'blocks.4.drop_path1', 'blocks.4.add', 'blocks.4.norm2', 'blocks.4.mlp.fc1', 'blocks.4.mlp.act', 'blocks.4.mlp.drop1', 'blocks.4.mlp.norm', 'blocks.4.mlp.fc2', 'blocks.4.mlp.drop2', 'blocks.4.ls2', 'blocks.4.drop_path2', 'blocks.4.add_1', 'blocks.5.norm1', 'blocks.5.attn.getattr', 'blocks.5.attn.getitem', 'blocks.5.attn.getitem_1', 'blocks.5.attn.getitem_2', 'blocks.5.attn.qkv', 'blocks.5.attn.reshape', 'blocks.5.attn.permute', 'blocks.5.attn.unbind', 'blocks.5.attn.getitem_3', 'blocks.5.attn.getitem_4', 'blocks.5.attn.getitem_5', 'blocks.5.attn.q_norm', 'blocks.5.attn.k_norm', 'blocks.5.attn.scaled_dot_product_attention', 'blocks.5.attn.transpose', 'blocks.5.attn.reshape_1', 'blocks.5.attn.proj', 'blocks.5.attn.proj_drop', 'blocks.5.ls1', 'blocks.5.drop_path1', 'blocks.5.add', 'blocks.5.norm2', 'blocks.5.mlp.fc1', 'blocks.5.mlp.act', 'blocks.5.mlp.drop1', 'blocks.5.mlp.norm', 'blocks.5.mlp.fc2', 'blocks.5.mlp.drop2', 'blocks.5.ls2', 'blocks.5.drop_path2', 'blocks.5.add_1', 'blocks.6.norm1', 'blocks.6.attn.getattr', 'blocks.6.attn.getitem', 'blocks.6.attn.getitem_1', 'blocks.6.attn.getitem_2', 'blocks.6.attn.qkv', 'blocks.6.attn.reshape', 'blocks.6.attn.permute', 'blocks.6.attn.unbind', 'blocks.6.attn.getitem_3', 'blocks.6.attn.getitem_4', 'blocks.6.attn.getitem_5', 'blocks.6.attn.q_norm', 'blocks.6.attn.k_norm', 'blocks.6.attn.scaled_dot_product_attention', 'blocks.6.attn.transpose', 'blocks.6.attn.reshape_1', 'blocks.6.attn.proj', 'blocks.6.attn.proj_drop', 'blocks.6.ls1', 'blocks.6.drop_path1', 'blocks.6.add', 'blocks.6.norm2', 'blocks.6.mlp.fc1', 'blocks.6.mlp.act', 'blocks.6.mlp.drop1', 'blocks.6.mlp.norm', 'blocks.6.mlp.fc2', 'blocks.6.mlp.drop2', 'blocks.6.ls2', 'blocks.6.drop_path2', 'blocks.6.add_1', 'blocks.7.norm1', 'blocks.7.attn.getattr', 'blocks.7.attn.getitem', 'blocks.7.attn.getitem_1', 'blocks.7.attn.getitem_2', 'blocks.7.attn.qkv', 'blocks.7.attn.reshape', 'blocks.7.attn.permute', 'blocks.7.attn.unbind', 'blocks.7.attn.getitem_3', 'blocks.7.attn.getitem_4', 'blocks.7.attn.getitem_5', 'blocks.7.attn.q_norm', 'blocks.7.attn.k_norm', 'blocks.7.attn.scaled_dot_product_attention', 'blocks.7.attn.transpose', 'blocks.7.attn.reshape_1', 'blocks.7.attn.proj', 'blocks.7.attn.proj_drop', 'blocks.7.ls1', 'blocks.7.drop_path1', 'blocks.7.add', 'blocks.7.norm2', 'blocks.7.mlp.fc1', 'blocks.7.mlp.act', 'blocks.7.mlp.drop1', 'blocks.7.mlp.norm', 'blocks.7.mlp.fc2', 'blocks.7.mlp.drop2', 'blocks.7.ls2', 'blocks.7.drop_path2', 'blocks.7.add_1', 'blocks.8.norm1', 'blocks.8.attn.getattr', 'blocks.8.attn.getitem', 'blocks.8.attn.getitem_1', 'blocks.8.attn.getitem_2', 'blocks.8.attn.qkv', 'blocks.8.attn.reshape', 'blocks.8.attn.permute', 'blocks.8.attn.unbind', 'blocks.8.attn.getitem_3', 'blocks.8.attn.getitem_4', 'blocks.8.attn.getitem_5', 'blocks.8.attn.q_norm', 'blocks.8.attn.k_norm', 'blocks.8.attn.scaled_dot_product_attention', 'blocks.8.attn.transpose', 'blocks.8.attn.reshape_1', 'blocks.8.attn.proj', 'blocks.8.attn.proj_drop', 'blocks.8.ls1', 'blocks.8.drop_path1', 'blocks.8.add', 'blocks.8.norm2', 'blocks.8.mlp.fc1', 'blocks.8.mlp.act', 'blocks.8.mlp.drop1', 'blocks.8.mlp.norm', 'blocks.8.mlp.fc2', 'blocks.8.mlp.drop2', 'blocks.8.ls2', 'blocks.8.drop_path2', 'blocks.8.add_1', 'blocks.9.norm1', 'blocks.9.attn.getattr', 'blocks.9.attn.getitem', 'blocks.9.attn.getitem_1', 'blocks.9.attn.getitem_2', 'blocks.9.attn.qkv', 'blocks.9.attn.reshape', 'blocks.9.attn.permute', 'blocks.9.attn.unbind', 'blocks.9.attn.getitem_3', 'blocks.9.attn.getitem_4', 'blocks.9.attn.getitem_5', 'blocks.9.attn.q_norm', 'blocks.9.attn.k_norm', 'blocks.9.attn.scaled_dot_product_attention', 'blocks.9.attn.transpose', 'blocks.9.attn.reshape_1', 'blocks.9.attn.proj', 'blocks.9.attn.proj_drop', 'blocks.9.ls1', 'blocks.9.drop_path1', 'blocks.9.add', 'blocks.9.norm2', 'blocks.9.mlp.fc1', 'blocks.9.mlp.act', 'blocks.9.mlp.drop1', 'blocks.9.mlp.norm', 'blocks.9.mlp.fc2', 'blocks.9.mlp.drop2', 'blocks.9.ls2', 'blocks.9.drop_path2', 'blocks.9.add_1', 'blocks.10.norm1', 'blocks.10.attn.getattr', 'blocks.10.attn.getitem', 'blocks.10.attn.getitem_1', 'blocks.10.attn.getitem_2', 'blocks.10.attn.qkv', 'blocks.10.attn.reshape', 'blocks.10.attn.permute', 'blocks.10.attn.unbind', 'blocks.10.attn.getitem_3', 'blocks.10.attn.getitem_4', 'blocks.10.attn.getitem_5', 'blocks.10.attn.q_norm', 'blocks.10.attn.k_norm', 'blocks.10.attn.scaled_dot_product_attention', 'blocks.10.attn.transpose', 'blocks.10.attn.reshape_1', 'blocks.10.attn.proj', 'blocks.10.attn.proj_drop', 'blocks.10.ls1', 'blocks.10.drop_path1', 'blocks.10.add', 'blocks.10.norm2', 'blocks.10.mlp.fc1', 'blocks.10.mlp.act', 'blocks.10.mlp.drop1', 'blocks.10.mlp.norm', 'blocks.10.mlp.fc2', 'blocks.10.mlp.drop2', 'blocks.10.ls2', 'blocks.10.drop_path2', 'blocks.10.add_1', 'blocks.11.norm1', 'blocks.11.attn.getattr', 'blocks.11.attn.getitem', 'blocks.11.attn.getitem_1', 'blocks.11.attn.getitem_2', 'blocks.11.attn.qkv', 'blocks.11.attn.reshape', 'blocks.11.attn.permute', 'blocks.11.attn.unbind', 'blocks.11.attn.getitem_3', 'blocks.11.attn.getitem_4', 'blocks.11.attn.getitem_5', 'blocks.11.attn.q_norm', 'blocks.11.attn.k_norm', 'blocks.11.attn.scaled_dot_product_attention', 'blocks.11.attn.transpose', 'blocks.11.attn.reshape_1', 'blocks.11.attn.proj', 'blocks.11.attn.proj_drop', 'blocks.11.ls1', 'blocks.11.drop_path1', 'blocks.11.add', 'blocks.11.norm2', 'blocks.11.mlp.fc1', 'blocks.11.mlp.act', 'blocks.11.mlp.drop1', 'blocks.11.mlp.norm', 'blocks.11.mlp.fc2', 'blocks.11.mlp.drop2', 'blocks.11.ls2', 'blocks.11.drop_path2', 'blocks.11.add_1', 'norm', 'getitem_1', 'fc_norm', 'head_drop', 'head'], ['x', 'patch_embed.getattr', 'patch_embed.getitem', 'patch_embed.getitem_1', 'patch_embed.getitem_2', 'patch_embed.getitem_3', 'patch_embed.eq', 'patch_embed._assert', 'patch_embed.eq_1', 'patch_embed._assert_1', 'patch_embed.proj', 'patch_embed.flatten', 'patch_embed.transpose', 'patch_embed.norm', 'pos_embed', 'cls_token', 'getattr', 'getitem', 'expand', 'cat', 'add', 'pos_drop', 'patch_drop', 'norm_pre', 'blocks.0.norm1', 'blocks.0.attn.getattr', 'blocks.0.attn.getitem', 'blocks.0.attn.getitem_1', 'blocks.0.attn.getitem_2', 'blocks.0.attn.qkv', 'blocks.0.attn.reshape', 'blocks.0.attn.permute', 'blocks.0.attn.unbind', 'blocks.0.attn.getitem_3', 'blocks.0.attn.getitem_4', 'blocks.0.attn.getitem_5', 'blocks.0.attn.q_norm', 'blocks.0.attn.k_norm', 'blocks.0.attn.scaled_dot_product_attention', 'blocks.0.attn.transpose', 'blocks.0.attn.reshape_1', 'blocks.0.attn.proj', 'blocks.0.attn.proj_drop', 'blocks.0.ls1', 'blocks.0.drop_path1', 'blocks.0.add', 'blocks.0.norm2', 'blocks.0.mlp.fc1', 'blocks.0.mlp.act', 'blocks.0.mlp.drop1', 'blocks.0.mlp.norm', 'blocks.0.mlp.fc2', 'blocks.0.mlp.drop2', 'blocks.0.ls2', 'blocks.0.drop_path2', 'blocks.0.add_1', 'blocks.1.norm1', 'blocks.1.attn.getattr', 'blocks.1.attn.getitem', 'blocks.1.attn.getitem_1', 'blocks.1.attn.getitem_2', 'blocks.1.attn.qkv', 'blocks.1.attn.reshape', 'blocks.1.attn.permute', 'blocks.1.attn.unbind', 'blocks.1.attn.getitem_3', 'blocks.1.attn.getitem_4', 'blocks.1.attn.getitem_5', 'blocks.1.attn.q_norm', 'blocks.1.attn.k_norm', 'blocks.1.attn.scaled_dot_product_attention', 'blocks.1.attn.transpose', 'blocks.1.attn.reshape_1', 'blocks.1.attn.proj', 'blocks.1.attn.proj_drop', 'blocks.1.ls1', 'blocks.1.drop_path1', 'blocks.1.add', 'blocks.1.norm2', 'blocks.1.mlp.fc1', 'blocks.1.mlp.act', 'blocks.1.mlp.drop1', 'blocks.1.mlp.norm', 'blocks.1.mlp.fc2', 'blocks.1.mlp.drop2', 'blocks.1.ls2', 'blocks.1.drop_path2', 'blocks.1.add_1', 'blocks.2.norm1', 'blocks.2.attn.getattr', 'blocks.2.attn.getitem', 'blocks.2.attn.getitem_1', 'blocks.2.attn.getitem_2', 'blocks.2.attn.qkv', 'blocks.2.attn.reshape', 'blocks.2.attn.permute', 'blocks.2.attn.unbind', 'blocks.2.attn.getitem_3', 'blocks.2.attn.getitem_4', 'blocks.2.attn.getitem_5', 'blocks.2.attn.q_norm', 'blocks.2.attn.k_norm', 'blocks.2.attn.scaled_dot_product_attention', 'blocks.2.attn.transpose', 'blocks.2.attn.reshape_1', 'blocks.2.attn.proj', 'blocks.2.attn.proj_drop', 'blocks.2.ls1', 'blocks.2.drop_path1', 'blocks.2.add', 'blocks.2.norm2', 'blocks.2.mlp.fc1', 'blocks.2.mlp.act', 'blocks.2.mlp.drop1', 'blocks.2.mlp.norm', 'blocks.2.mlp.fc2', 'blocks.2.mlp.drop2', 'blocks.2.ls2', 'blocks.2.drop_path2', 'blocks.2.add_1', 'blocks.3.norm1', 'blocks.3.attn.getattr', 'blocks.3.attn.getitem', 'blocks.3.attn.getitem_1', 'blocks.3.attn.getitem_2', 'blocks.3.attn.qkv', 'blocks.3.attn.reshape', 'blocks.3.attn.permute', 'blocks.3.attn.unbind', 'blocks.3.attn.getitem_3', 'blocks.3.attn.getitem_4', 'blocks.3.attn.getitem_5', 'blocks.3.attn.q_norm', 'blocks.3.attn.k_norm', 'blocks.3.attn.scaled_dot_product_attention', 'blocks.3.attn.transpose', 'blocks.3.attn.reshape_1', 'blocks.3.attn.proj', 'blocks.3.attn.proj_drop', 'blocks.3.ls1', 'blocks.3.drop_path1', 'blocks.3.add', 'blocks.3.norm2', 'blocks.3.mlp.fc1', 'blocks.3.mlp.act', 'blocks.3.mlp.drop1', 'blocks.3.mlp.norm', 'blocks.3.mlp.fc2', 'blocks.3.mlp.drop2', 'blocks.3.ls2', 'blocks.3.drop_path2', 'blocks.3.add_1', 'blocks.4.norm1', 'blocks.4.attn.getattr', 'blocks.4.attn.getitem', 'blocks.4.attn.getitem_1', 'blocks.4.attn.getitem_2', 'blocks.4.attn.qkv', 'blocks.4.attn.reshape', 'blocks.4.attn.permute', 'blocks.4.attn.unbind', 'blocks.4.attn.getitem_3', 'blocks.4.attn.getitem_4', 'blocks.4.attn.getitem_5', 'blocks.4.attn.q_norm', 'blocks.4.attn.k_norm', 'blocks.4.attn.scaled_dot_product_attention', 'blocks.4.attn.transpose', 'blocks.4.attn.reshape_1', 'blocks.4.attn.proj', 'blocks.4.attn.proj_drop', 'blocks.4.ls1', 'blocks.4.drop_path1', 'blocks.4.add', 'blocks.4.norm2', 'blocks.4.mlp.fc1', 'blocks.4.mlp.act', 'blocks.4.mlp.drop1', 'blocks.4.mlp.norm', 'blocks.4.mlp.fc2', 'blocks.4.mlp.drop2', 'blocks.4.ls2', 'blocks.4.drop_path2', 'blocks.4.add_1', 'blocks.5.norm1', 'blocks.5.attn.getattr', 'blocks.5.attn.getitem', 'blocks.5.attn.getitem_1', 'blocks.5.attn.getitem_2', 'blocks.5.attn.qkv', 'blocks.5.attn.reshape', 'blocks.5.attn.permute', 'blocks.5.attn.unbind', 'blocks.5.attn.getitem_3', 'blocks.5.attn.getitem_4', 'blocks.5.attn.getitem_5', 'blocks.5.attn.q_norm', 'blocks.5.attn.k_norm', 'blocks.5.attn.scaled_dot_product_attention', 'blocks.5.attn.transpose', 'blocks.5.attn.reshape_1', 'blocks.5.attn.proj', 'blocks.5.attn.proj_drop', 'blocks.5.ls1', 'blocks.5.drop_path1', 'blocks.5.add', 'blocks.5.norm2', 'blocks.5.mlp.fc1', 'blocks.5.mlp.act', 'blocks.5.mlp.drop1', 'blocks.5.mlp.norm', 'blocks.5.mlp.fc2', 'blocks.5.mlp.drop2', 'blocks.5.ls2', 'blocks.5.drop_path2', 'blocks.5.add_1', 'blocks.6.norm1', 'blocks.6.attn.getattr', 'blocks.6.attn.getitem', 'blocks.6.attn.getitem_1', 'blocks.6.attn.getitem_2', 'blocks.6.attn.qkv', 'blocks.6.attn.reshape', 'blocks.6.attn.permute', 'blocks.6.attn.unbind', 'blocks.6.attn.getitem_3', 'blocks.6.attn.getitem_4', 'blocks.6.attn.getitem_5', 'blocks.6.attn.q_norm', 'blocks.6.attn.k_norm', 'blocks.6.attn.scaled_dot_product_attention', 'blocks.6.attn.transpose', 'blocks.6.attn.reshape_1', 'blocks.6.attn.proj', 'blocks.6.attn.proj_drop', 'blocks.6.ls1', 'blocks.6.drop_path1', 'blocks.6.add', 'blocks.6.norm2', 'blocks.6.mlp.fc1', 'blocks.6.mlp.act', 'blocks.6.mlp.drop1', 'blocks.6.mlp.norm', 'blocks.6.mlp.fc2', 'blocks.6.mlp.drop2', 'blocks.6.ls2', 'blocks.6.drop_path2', 'blocks.6.add_1', 'blocks.7.norm1', 'blocks.7.attn.getattr', 'blocks.7.attn.getitem', 'blocks.7.attn.getitem_1', 'blocks.7.attn.getitem_2', 'blocks.7.attn.qkv', 'blocks.7.attn.reshape', 'blocks.7.attn.permute', 'blocks.7.attn.unbind', 'blocks.7.attn.getitem_3', 'blocks.7.attn.getitem_4', 'blocks.7.attn.getitem_5', 'blocks.7.attn.q_norm', 'blocks.7.attn.k_norm', 'blocks.7.attn.scaled_dot_product_attention', 'blocks.7.attn.transpose', 'blocks.7.attn.reshape_1', 'blocks.7.attn.proj', 'blocks.7.attn.proj_drop', 'blocks.7.ls1', 'blocks.7.drop_path1', 'blocks.7.add', 'blocks.7.norm2', 'blocks.7.mlp.fc1', 'blocks.7.mlp.act', 'blocks.7.mlp.drop1', 'blocks.7.mlp.norm', 'blocks.7.mlp.fc2', 'blocks.7.mlp.drop2', 'blocks.7.ls2', 'blocks.7.drop_path2', 'blocks.7.add_1', 'blocks.8.norm1', 'blocks.8.attn.getattr', 'blocks.8.attn.getitem', 'blocks.8.attn.getitem_1', 'blocks.8.attn.getitem_2', 'blocks.8.attn.qkv', 'blocks.8.attn.reshape', 'blocks.8.attn.permute', 'blocks.8.attn.unbind', 'blocks.8.attn.getitem_3', 'blocks.8.attn.getitem_4', 'blocks.8.attn.getitem_5', 'blocks.8.attn.q_norm', 'blocks.8.attn.k_norm', 'blocks.8.attn.scaled_dot_product_attention', 'blocks.8.attn.transpose', 'blocks.8.attn.reshape_1', 'blocks.8.attn.proj', 'blocks.8.attn.proj_drop', 'blocks.8.ls1', 'blocks.8.drop_path1', 'blocks.8.add', 'blocks.8.norm2', 'blocks.8.mlp.fc1', 'blocks.8.mlp.act', 'blocks.8.mlp.drop1', 'blocks.8.mlp.norm', 'blocks.8.mlp.fc2', 'blocks.8.mlp.drop2', 'blocks.8.ls2', 'blocks.8.drop_path2', 'blocks.8.add_1', 'blocks.9.norm1', 'blocks.9.attn.getattr', 'blocks.9.attn.getitem', 'blocks.9.attn.getitem_1', 'blocks.9.attn.getitem_2', 'blocks.9.attn.qkv', 'blocks.9.attn.reshape', 'blocks.9.attn.permute', 'blocks.9.attn.unbind', 'blocks.9.attn.getitem_3', 'blocks.9.attn.getitem_4', 'blocks.9.attn.getitem_5', 'blocks.9.attn.q_norm', 'blocks.9.attn.k_norm', 'blocks.9.attn.scaled_dot_product_attention', 'blocks.9.attn.transpose', 'blocks.9.attn.reshape_1', 'blocks.9.attn.proj', 'blocks.9.attn.proj_drop', 'blocks.9.ls1', 'blocks.9.drop_path1', 'blocks.9.add', 'blocks.9.norm2', 'blocks.9.mlp.fc1', 'blocks.9.mlp.act', 'blocks.9.mlp.drop1', 'blocks.9.mlp.norm', 'blocks.9.mlp.fc2', 'blocks.9.mlp.drop2', 'blocks.9.ls2', 'blocks.9.drop_path2', 'blocks.9.add_1', 'blocks.10.norm1', 'blocks.10.attn.getattr', 'blocks.10.attn.getitem', 'blocks.10.attn.getitem_1', 'blocks.10.attn.getitem_2', 'blocks.10.attn.qkv', 'blocks.10.attn.reshape', 'blocks.10.attn.permute', 'blocks.10.attn.unbind', 'blocks.10.attn.getitem_3', 'blocks.10.attn.getitem_4', 'blocks.10.attn.getitem_5', 'blocks.10.attn.q_norm', 'blocks.10.attn.k_norm', 'blocks.10.attn.scaled_dot_product_attention', 'blocks.10.attn.transpose', 'blocks.10.attn.reshape_1', 'blocks.10.attn.proj', 'blocks.10.attn.proj_drop', 'blocks.10.ls1', 'blocks.10.drop_path1', 'blocks.10.add', 'blocks.10.norm2', 'blocks.10.mlp.fc1', 'blocks.10.mlp.act', 'blocks.10.mlp.drop1', 'blocks.10.mlp.norm', 'blocks.10.mlp.fc2', 'blocks.10.mlp.drop2', 'blocks.10.ls2', 'blocks.10.drop_path2', 'blocks.10.add_1', 'blocks.11.norm1', 'blocks.11.attn.getattr', 'blocks.11.attn.getitem', 'blocks.11.attn.getitem_1', 'blocks.11.attn.getitem_2', 'blocks.11.attn.qkv', 'blocks.11.attn.reshape', 'blocks.11.attn.permute', 'blocks.11.attn.unbind', 'blocks.11.attn.getitem_3', 'blocks.11.attn.getitem_4', 'blocks.11.attn.getitem_5', 'blocks.11.attn.q_norm', 'blocks.11.attn.k_norm', 'blocks.11.attn.scaled_dot_product_attention', 'blocks.11.attn.transpose', 'blocks.11.attn.reshape_1', 'blocks.11.attn.proj', 'blocks.11.attn.proj_drop', 'blocks.11.ls1', 'blocks.11.drop_path1', 'blocks.11.add', 'blocks.11.norm2', 'blocks.11.mlp.fc1', 'blocks.11.mlp.act', 'blocks.11.mlp.drop1', 'blocks.11.mlp.norm', 'blocks.11.mlp.fc2', 'blocks.11.mlp.drop2', 'blocks.11.ls2', 'blocks.11.drop_path2', 'blocks.11.add_1', 'norm', 'getitem_1', 'fc_norm', 'head_drop', 'head'])\n",
      "The model will start training with only 624429 trainable parameters out of 86423085.\n",
      "The model should start training with only 589824 trainable lora parameters, plus 0 head params = 589824.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "model = test_lora(freeze=True, lora=True, rank=4)\n",
    "parameters = list(model.parameters())\n",
    "trainable_parameters = list(filter(lambda p: p.requires_grad, parameters))\n",
    "print(\n",
    "    f\"The model will start training with only {sum([int(torch.numel(p)) for p in trainable_parameters])} \"\n",
    "    f\"trainable parameters out of {sum([int(torch.numel(p)) for p in parameters])}.\"\n",
    ")\n",
    "    \n",
    "    \n",
    "def name_is_lora(name):\n",
    "    return (\n",
    "        len(name.split(\".\")) >= 4\n",
    "        and (name.split(\".\")[-4]) == \"parametrizations\"\n",
    "        and name.split(\".\")[-1] in [\"lora_A\", \"lora_B\"]\n",
    "    )\n",
    "\n",
    "def name_is_head(name):\n",
    "    return (name.split(\".\")[1]) == \"classifier\"\n",
    "\n",
    "def get_params_by_name(model, print_shapes=False, name_filter=None):\n",
    "    for n, p in model.named_parameters():\n",
    "        if name_filter is None or name_filter(n):\n",
    "            if print_shapes:\n",
    "                print(n, p.shape)\n",
    "            yield p\n",
    "\n",
    "lora_parameters = get_params_by_name(model, name_filter=name_is_lora)\n",
    "num_lora = sum([int(torch.numel(p)) for p in lora_parameters])\n",
    "head_parameters = get_params_by_name(model, name_filter=name_is_head)\n",
    "num_head = sum([int(torch.numel(p)) for p in head_parameters])\n",
    "print(\n",
    "    f\"The model should start training with only {num_lora} trainable lora parameters, plus {num_head} head params = {num_head+num_lora}.\"\n",
    ")\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.AdamW(\n",
    "#    trainable_parameters,\n",
    "#    lr=1e-3,\n",
    "#)\n",
    "#\n",
    "#device = 'cuda'\n",
    "#model = model.to(device)\n",
    "#for epoch in range(1, 10):\n",
    "#    train(model, criterion, device, train_loader, optimizer, epoch)\n",
    "#    test(model, criterion, optimizer, device, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597524a6-48a5-4958-a41f-0832db043728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
