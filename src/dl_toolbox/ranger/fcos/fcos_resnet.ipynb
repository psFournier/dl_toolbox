{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebba51df-e9c9-461c-b055-977f16969f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelP6P7\n",
    "#from torchvision.models.detection.backbone_utils import LastLevelP6P7\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "INF = 100000000\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from dl_toolbox.callbacks import ProgressBar\n",
    "import gc \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from dl_toolbox.callbacks import ProgressBar\n",
    "from dl_toolbox import datamodules\n",
    "from dl_toolbox import modules\n",
    "import torchvision.transforms.v2 as v2\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from dl_toolbox.callbacks import ProgressBar, Finetuning, Lora, TiffPredsWriter, CalibrationLogger\n",
    "from functools import partial\n",
    "import gc\n",
    "\n",
    "\n",
    "train_tf = v2.Compose(\n",
    "    [\n",
    "        v2.RandomCrop(224),\n",
    "        v2.SanitizeBoundingBoxes(),\n",
    "        v2.Normalize([0.5]*3, [0.5]*3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_tf = v2.Compose(\n",
    "    [\n",
    "        v2.CenterCrop(1000),\n",
    "        v2.Resize(224),\n",
    "        v2.SanitizeBoundingBoxes(),\n",
    "        v2.Normalize([0.5]*3, [0.5]*3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "dm = datamodules.xView(\n",
    "    data_path='/data',\n",
    "    merge='all',\n",
    "    train_tf=train_tf,\n",
    "    test_tf=test_tf,\n",
    "    batch_tf=None,\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    limit_train_batches=5,\n",
    "    limit_val_batches=5,\n",
    "    callbacks=[ProgressBar()]\n",
    ")\n",
    "\n",
    "num_classes = dm.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e24601f-642b-4245-a582-42a6dc91a23e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _compute_centerness_targets(reg_targets):\n",
    "    if len(reg_targets) == 0:\n",
    "        return reg_targets.new_zeros(len(reg_targets))\n",
    "    left_right = reg_targets[:, [0, 2]]\n",
    "    top_bottom = reg_targets[:, [1, 3]]\n",
    "    centerness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\\n",
    "                (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0])\n",
    "    return torch.sqrt(centerness)\n",
    "\n",
    "\n",
    "def _calculate_reg_targets(xs, ys, bbox_targets):\n",
    "    \"\"\" \n",
    "    Very important : what is the format of bbox in targets produced by the dataset ?\n",
    "    \"\"\"\n",
    "    l = xs[:, None] - bbox_targets[:, 0][None] # Lx1 - 1xT -> LxT\n",
    "    t = ys[:, None] - bbox_targets[:, 1][None]\n",
    "    r = bbox_targets[:, 2][None] - xs[:, None]\n",
    "    b = bbox_targets[:, 3][None] - ys[:, None]\n",
    "    return torch.stack([l, t, r, b], dim=2) # LxTx4\n",
    "\n",
    "\n",
    "def _apply_distance_constraints(reg_targets, level_distances):\n",
    "    \"\"\"\n",
    "    reg_targets: LxTx4\n",
    "    level_distances: Lx2\n",
    "    \"\"\"\n",
    "    max_reg_targets, _ = reg_targets.max(dim=2)\n",
    "    return torch.logical_and(max_reg_targets >= level_distances[:, None, 0], \\\n",
    "                             max_reg_targets <= level_distances[:, None, 1])\n",
    "\n",
    "def _match_pred_format(cls_targets, reg_targets, locations):\n",
    "    cls_per_level = []\n",
    "    reg_per_level = []\n",
    "    for level in range(len(locations)):\n",
    "        \n",
    "        cls_per_level.append(torch.cat([ct[level] for ct in cls_targets],\n",
    "                                       dim=0))\n",
    "\n",
    "        reg_per_level.append(torch.cat([rt[level] for rt in reg_targets],\n",
    "                                       dim=0))\n",
    "    # reg_per_level is a list of num_levels tensors of size Bxnum_loc_per_levelx4\n",
    "    return cls_per_level, reg_per_level\n",
    "\n",
    "\n",
    "def _get_positive_samples(cls_labels, reg_labels, box_cls_preds, box_reg_preds,\n",
    "                          centerness_preds, num_classes):\n",
    "    box_cls_flatten = []\n",
    "    box_regression_flatten = []\n",
    "    centerness_flatten = []\n",
    "    labels_flatten = []\n",
    "    reg_targets_flatten = []\n",
    "    for l in range(len(cls_labels)):\n",
    "        box_cls_flatten.append(box_cls_preds[l].permute(0, 2, 3, 1).reshape(\n",
    "            -1, num_classes))\n",
    "        box_regression_flatten.append(box_reg_preds[l].permute(0, 2, 3,\n",
    "                                                               1).reshape(\n",
    "                                                                   -1, 4))\n",
    "        labels_flatten.append(cls_labels[l].reshape(-1))\n",
    "        reg_targets_flatten.append(reg_labels[l].reshape(-1, 4))\n",
    "        centerness_flatten.append(centerness_preds[l].reshape(-1))\n",
    "\n",
    "    cls_preds = torch.cat(box_cls_flatten, dim=0)\n",
    "    cls_targets = torch.cat(labels_flatten, dim=0)\n",
    "    reg_preds = torch.cat(box_regression_flatten, dim=0)\n",
    "    reg_targets = torch.cat(reg_targets_flatten, dim=0)\n",
    "    centerness_preds = torch.cat(centerness_flatten, dim=0)\n",
    "    pos_inds = torch.nonzero(cls_targets > 0).squeeze(1) # dim #loc in all batches where there is one cls to pred not background\n",
    "    print(f'{pos_inds.shape = }')\n",
    "    reg_preds = reg_preds[pos_inds]\n",
    "    reg_targets = reg_targets[pos_inds]\n",
    "    centerness_preds = centerness_preds[pos_inds]\n",
    "    return reg_preds, reg_targets, cls_preds, cls_targets, centerness_preds, pos_inds\n",
    "\n",
    "class LossEvaluator(nn.Module):\n",
    "\n",
    "    def __init__(self, locs_info, num_classes):\n",
    "        super(LossEvaluator, self).__init__()\n",
    "        locs_per_level, bb_sizes_per_level, num_locs_per_level = locs_info\n",
    "        self.centerness_loss_func = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "        # Let's call L the total nb of locs across all feature levels\n",
    "        self.register_buffer('locations', torch.cat(locs_per_level, dim=0)) # Lx2\n",
    "        self.register_buffer('bb_sizes', torch.cat(bb_sizes_per_level, dim=0)) # Lx2\n",
    "        self.num_locs_per_level = num_locs_per_level # list len L\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def _get_cls_loss(self, cls_preds, cls_targets, total_num_pos):\n",
    "        nc = cls_preds.shape[1]\n",
    "        onehot = F.one_hot(cls_targets.long(), nc+1)[:,1:].float()\n",
    "        cls_loss = torchvision.ops.sigmoid_focal_loss(cls_preds, onehot)\n",
    "        return cls_loss.sum() / total_num_pos\n",
    "\n",
    "    def _get_reg_loss(self, reg_preds, reg_targets, centerness_targets):\n",
    "        reg_preds = reg_preds.reshape(-1, 4)\n",
    "        reg_targets = reg_targets.reshape(-1, 4)\n",
    "        reg_losses = torchvision.ops.distance_box_iou_loss(reg_preds, reg_targets, reduction='none')\n",
    "        sum_centerness_targets = centerness_targets.sum()\n",
    "        reg_loss = (reg_losses * centerness_targets).sum() / sum_centerness_targets\n",
    "        return reg_loss\n",
    "\n",
    "    def _get_centerness_loss(self, centerness_preds, centerness_targets,\n",
    "                             total_num_pos):\n",
    "        centerness_loss = self.centerness_loss_func(centerness_preds,\n",
    "                                                    centerness_targets)\n",
    "        return centerness_loss / total_num_pos\n",
    "\n",
    "    def _evaluate_losses(self, reg_preds, cls_preds, centerness_preds,\n",
    "                         reg_targets, cls_targets, centerness_targets,\n",
    "                         pos_inds):\n",
    "        total_num_pos = max(pos_inds.new_tensor([pos_inds.numel()]), 1.0)\n",
    "\n",
    "        cls_loss = self._get_cls_loss(cls_preds, cls_targets, total_num_pos)\n",
    "\n",
    "        if pos_inds.numel() > 0:\n",
    "            reg_loss = self._get_reg_loss(reg_preds, reg_targets,\n",
    "                                          centerness_targets)\n",
    "            centerness_loss = self._get_centerness_loss(centerness_preds,\n",
    "                                                        centerness_targets,\n",
    "                                                        total_num_pos)\n",
    "        else:\n",
    "            reg_loss = reg_preds.sum() # 0 ??\n",
    "            centerness_loss = centerness_preds.sum() # 0 ??\n",
    "\n",
    "        return reg_loss, cls_loss, centerness_loss\n",
    "    \n",
    "    def _prepare_labels(self, targets_batch):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            targets_batch: list of dict of tv_tensors {'labels':, 'boxes':}\n",
    "        outputs:\n",
    "            target bb and cls for each anchor ?\n",
    "        \"\"\"\n",
    "        # nb of locs for bbox in original image size\n",
    "        # L = sum locs per level x 2 : for each loc in all_locs, the max size of bb authorized\n",
    "        xs, ys = self.locations[:, 0], self.locations[:, 1] # L & L\n",
    "        num_locs = sum(self.num_locs_per_level)\n",
    "\n",
    "        all_reg_targets = []\n",
    "        all_cls_targets = []\n",
    "        for targets in targets_batch:\n",
    "            \n",
    "            bbox_targets = targets['boxes'] # Tx4\n",
    "            # bbox targets which format ??? \n",
    "            # Code here expects xyxy, but dataset provides xywh so:\n",
    "            bbox_targets[:, 2] += bbox_targets[:, 0]\n",
    "            bbox_targets[:, 3] += bbox_targets[:, 1]\n",
    "            \n",
    "            cls_targets = targets['labels'] # T\n",
    "            num_targets = cls_targets.shape[0]\n",
    "            print(f'{num_targets =}')\n",
    "\n",
    "            # for each loc in L and each target in T, the reg target\n",
    "            reg_targets = _calculate_reg_targets(xs, ys, bbox_targets) # LxTx4\n",
    "            \n",
    "            # Which locs are contained in which tgt bb\n",
    "            is_in_boxes = reg_targets.min(dim=2)[0] > 0 # min returns values and indices -> LxT\n",
    "            \n",
    "            # BUG ? Now : which pairs (loc, tgt bb) are such that the max value to regress at this loc for this bb \n",
    "            # is inside the bounds associated to this loc;\n",
    "            # Nothing prevents that the pair requires regressing negative vals ??\n",
    "            # NO BUG because the rest is filtered by is_in_boxes\n",
    "            fits_to_feature_level = _apply_distance_constraints(\n",
    "                reg_targets, self.bb_sizes) # LxT\n",
    "            \n",
    "            #bbox_areas = _calc_bbox_area(bbox_targets) # T\n",
    "            bbox_areas = torchvision.ops.box_area(bbox_targets) # compared to above, does not deal with 0dim bb\n",
    "            \n",
    "            # area of each target bbox repeated for each loc with inf where the the loc is not \n",
    "            # in the target bbox or if the loc is not at the right level for this bbox size\n",
    "            locations_to_gt_area = bbox_areas[None].repeat(len(self.locations), 1) # LxT\n",
    "            locations_to_gt_area[is_in_boxes == 0] = INF\n",
    "            locations_to_gt_area[fits_to_feature_level == 0] = INF\n",
    "\n",
    "            # for each loc, area and target idx of the target of min area at that loc\n",
    "            if num_targets>0:\n",
    "                # Here the goal is to associate one target bbox ONLY to each loc/anchor\n",
    "                # So, min over T for each loc to find the smallest tgt bb that:\n",
    "                # - contains the loc\n",
    "                # - and is of size in the limits associated to the loc\n",
    "                loc_min_area, loc_min_idxs = locations_to_gt_area.min(dim=1) # val&idx, size L, idx in [0,T-1]\n",
    "                reg_targets = reg_targets[range(len(self.locations)), loc_min_idxs] # Lx4\n",
    "                cls_targets = cls_targets[loc_min_idxs] # L\n",
    "                cls_targets[loc_min_area == INF] = 0 # 0 is no-obj category ? NO !! XVIEW outputs 0 for first class !!\n",
    "            else:\n",
    "                cls_targets = cls_targets.new_zeros((num_locs,))\n",
    "                reg_targets = reg_targets.new_zeros((num_locs,4))\n",
    "\n",
    "            all_cls_targets.append(\n",
    "                torch.split(cls_targets, self.num_locs_per_level, dim=0))\n",
    "            all_reg_targets.append(\n",
    "                torch.split(reg_targets, self.num_locs_per_level, dim=0))\n",
    "        # all_cls_targets contains B lists of num levels elem of loc_per_levelsx1\n",
    "        locations = torch.split(self.locations, self.num_locs_per_level, dim=0)\n",
    "        return _match_pred_format(all_cls_targets, all_reg_targets, locations)\n",
    "\n",
    "    def __call__(self, out, targets_batch):\n",
    "        \n",
    "        # reg_targets is a list of num_levels tensors of size Bxnum_loc_per_levelx4\n",
    "        cls_targets, reg_targets = self._prepare_labels(targets_batch)\n",
    "        \n",
    "        # box_regression is a list of num_levels tensors of size Bx4xnum_loc_per_level\n",
    "        box_cls, box_regression, centerness = out\n",
    "        reg_p, reg_t, cls_p, cls_t, centerness_p, pos_inds = _get_positive_samples(\n",
    "            cls_targets,\n",
    "            reg_targets,\n",
    "            box_cls,\n",
    "            box_regression,\n",
    "            centerness,\n",
    "            self.num_classes\n",
    "        )\n",
    "        centerness_t = _compute_centerness_targets(reg_t)\n",
    "        losses = {}\n",
    "        reg_loss, cls_loss, centerness_loss = self._evaluate_losses(\n",
    "            reg_p, cls_p, centerness_p, reg_t, cls_t, centerness_t, pos_inds)\n",
    "        losses[\"cls_loss\"] = cls_loss\n",
    "        losses[\"reg_loss\"] = reg_loss\n",
    "        losses[\"centerness_loss\"] = centerness_loss\n",
    "        losses[\"combined_loss\"] = cls_loss + reg_loss + centerness_loss\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd41acc-4e80-43d8-9c45-34a3dd319154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from post_processor import *\n",
    "\n",
    "class FCOS(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        network,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        pred_thresh,\n",
    "        tta=None,\n",
    "        sliding=None,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.network = network\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.map_metric = MeanAveragePrecision(\n",
    "            box_format='xywh', # make sure your dataset outputs target in xywh format\n",
    "            backend='faster_coco_eval'\n",
    "        )\n",
    "        self.sliding = sliding\n",
    "        self.pred_thresh = pred_thresh      \n",
    "\n",
    "        fpn_strides = [8, 16, 32, 64, 128] # Feat size reduction factor for each FPN stage\n",
    "        # BB size to be detected by FPN stages: P3=red factor 8 -> detects bb of size between -1 & 64\n",
    "        bb_sizes = [-1, 64, 128, 256, 512, INF] \n",
    "        # anchors is a list of num_feat_level elem, where each elem indicates the tensor of \n",
    "        # anchors of size Nx2 in the original image corresponding to each location in the feature map at this level\n",
    "        # For ex, the top-left anchor for the first feature level is centered on (4,4) and aims at\n",
    "        # detecting objects of which the bbox sides are not further than 64 pixels, so at max of size 128\n",
    "        anchors, anchor_sizes, num_anchors = self.get_anchors(\n",
    "            network.feat_sizes,\n",
    "            fpn_strides,\n",
    "            bb_sizes\n",
    "        )\n",
    "        self.loss = LossEvaluator(\n",
    "            (anchors, anchor_sizes, num_anchors),\n",
    "            num_classes\n",
    "        )\n",
    "        self.post_processor = FCOSPostProcessor(\n",
    "            locs_info = (anchors, anchor_sizes, num_anchors),\n",
    "            pre_nms_thresh=0.3,\n",
    "            pre_nms_top_n=1000,\n",
    "            nms_thresh=0.45,\n",
    "            fpn_post_nms_top_n=50,\n",
    "            min_size=0,\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        train_params = list(filter(lambda p: p[1].requires_grad, self.named_parameters()))\n",
    "        nb_train = sum([int(torch.numel(p[1])) for p in train_params])\n",
    "        nb_tot = sum([int(torch.numel(p)) for p in self.parameters()])\n",
    "        print(f\"Training {nb_train} params out of {nb_tot}\")\n",
    "        optimizer = self.optimizer(params=[p[1] for p in train_params])\n",
    "        scheduler = self.scheduler(optimizer)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_anchors(cls, feat_sizes, fpn_strides, bb_sizes):\n",
    "        \"\"\"\n",
    "        arguments:\n",
    "            feat_sizes: feature maps sizes\n",
    "            fpn_strides: Feat map size reduction factor for each FPN stage\n",
    "            bb_sizes: bbox \n",
    "        \"\"\"\n",
    "        anchors, anchor_sizes, num_anchors = [], [], []\n",
    "        \n",
    "        def _anchors_per_level(h, w, s):\n",
    "            locs_x = [i for i in range(w)]\n",
    "            locs_y = [i for i in range(h)]\n",
    "            locs_x = [s / 2 + x * s for x in locs_x]\n",
    "            locs_y = [s / 2 + y * s for y in locs_y]\n",
    "            locs = [(y, x) for x in locs_x for y in locs_y]\n",
    "            return torch.tensor(locs)\n",
    "        \n",
    "        for l, (h,w) in enumerate(feat_sizes):\n",
    "            # first level : l=0, stride=8, h=w=28\n",
    "            locs = _anchors_per_level(h, w, fpn_strides[l])\n",
    "            sizes = torch.tensor([bb_sizes[l], bb_sizes[l+1]], dtype=torch.float32)\n",
    "            sizes = sizes.repeat(len(locs)).view(len(locs), 2)\n",
    "            anchors.append(locs)\n",
    "            anchor_sizes.append(sizes)\n",
    "            num_anchors.append(len(locs))\n",
    "        return anchors, anchor_sizes, num_anchors\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def post_process(self, out, images):\n",
    "        predicted_boxes, scores, all_classes = self.post_processor(*out, images.shape[-1])\n",
    "        preds = [{'boxes': bb, 'scores': s, 'labels': l} for bb,s,l in zip(\n",
    "            predicted_boxes, scores, all_classes\n",
    "        )]\n",
    "        return preds\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, targets, paths = batch[\"sup\"] #targets is a list of dict\n",
    "        outputs = self.forward(x) # pred_cls, pred_bb, pred_centerness\n",
    "        losses = self.loss(outputs, targets)\n",
    "        loss = losses[\"combined_loss\"]\n",
    "        self.log(f\"loss/train\", loss.detach().item())\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, targets, paths = batch\n",
    "        outputs = self.forward(x)\n",
    "        losses = self.loss(outputs, targets)\n",
    "        loss = losses[\"combined_loss\"]\n",
    "        self.log(f\"Total loss/val\", loss.detach().item())\n",
    "        preds = self.post_process(outputs, x)\n",
    "        self.map_metric.update(preds, targets)\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        mapmetric = self.map_metric.compute()['map']\n",
    "        self.log(\"map/val\", mapmetric)\n",
    "        print(\"\\nMAP: \", mapmetric)\n",
    "        self.map_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0d01e6b-5b98-4aae-a18b-a87aa7c0f8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.45s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /d/pfournie/dl_toolbox/dl_toolbox/à ranger/fcos/lightning_logs/version_7/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=2.76s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type                 | Params\n",
      "--------------------------------------------------------\n",
      "0 | network        | ResnetDet            | 29.9 M\n",
      "1 | map_metric     | MeanAveragePrecision | 0     \n",
      "2 | loss           | LossEvaluator        | 0     \n",
      "3 | post_processor | FCOSPostProcessor    | 0     \n",
      "--------------------------------------------------------\n",
      "29.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "29.9 M    Total params\n",
      "119.556   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 29888902 params out of 29888902\n",
      "Epoch 16:  40%|█████████████████████████████████████████▌                                                              | 2/5 [00:43<01:04,  0.05it/s, v_num=7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanity Checking DataLoader 0:   0%|                                                                                                     | 0/2 [00:00<?, ?it/s]num_targets =84\n",
      "num_targets =55\n",
      "pos_inds.shape = torch.Size([306])\n",
      "Sanity Checking DataLoader 0:  50%|██████████████████████████████████████████████▌                                              | 1/2 [00:00<00:00, 29.87it/s]num_targets =0\n",
      "num_targets =68\n",
      "pos_inds.shape = torch.Size([100])\n",
      "Sanity Checking DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.25it/s]\n",
      "MAP:  tensor(0.0143)\n",
      "                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:   0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]num_targets =0\n",
      "num_targets =0\n",
      "pos_inds.shape = torch.Size([0])\n",
      "Epoch 16:  20%|████████████████████▊                                                                                   | 1/5 [00:00<00:01,  2.17it/s, v_num=7]num_targets =0\n",
      "num_targets =0\n",
      "pos_inds.shape = torch.Size([0])\n",
      "Epoch 16:  40%|█████████████████████████████████████████▌                                                              | 2/5 [00:00<00:01,  2.14it/s, v_num=7]num_targets =0\n",
      "num_targets =0\n",
      "pos_inds.shape = torch.Size([0])\n",
      "Epoch 16:  60%|██████████████████████████████████████████████████████████████▍                                         | 3/5 [00:01<00:00,  2.47it/s, v_num=7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "from resnet_fcos import ResnetDet\n",
    "\n",
    "network = ResnetDet(256, num_classes)\n",
    "module = FCOS(\n",
    "    num_classes=num_classes,\n",
    "    network=network,\n",
    "    optimizer=partial(torch.optim.Adam, lr=0.001),\n",
    "    scheduler=partial(torch.optim.lr_scheduler.ConstantLR, factor=1),\n",
    "    pred_thresh=0.1,\n",
    ")\n",
    " \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "trainer.fit(\n",
    "    module,\n",
    "    datamodule=dm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39373a1-4d21-4e68-b83e-77acc9fcbad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_venv",
   "language": "python",
   "name": "dl_toolbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
