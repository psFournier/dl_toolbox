{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80247a83-666b-4e1d-9137-785489e8e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from yolos.detector import Detector\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc133752-6f73-432c-a732-ec2b3aa013d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "from torchvision.ops import box_convert, generalized_box_iou\n",
    "\n",
    "@torch.no_grad()\n",
    "#def hungarian_matching(pred_cls, pred_boxes, target_cls, target_boxes):\n",
    "def hungarian_matching(pred_logits, pred_boxes, tgt_cls, tgt_boxes):\n",
    "    \"\"\" \n",
    "    Params:\n",
    "        pred_cls: Tensor of dim [B, num_queries, num_classes] with the class logits\n",
    "        pred_boxes: Tensor of dim [B, num_queries, 4] with the pred box coord\n",
    "        target_cls: list (len=batchsize) of Tensors of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth objects in the target) containing the class labels\n",
    "        target_boxes: list (len=batchsize) of Tensors of dim [num_target_boxes, 4] containing the target box coord\n",
    "\n",
    "    Returns:\n",
    "        A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "            - index_i is the indices of the selected predictions (in order)\n",
    "            - index_j is the indices of the corresponding selected targets (in order)\n",
    "        For each batch element, it holds:\n",
    "            len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Also concat the target labels and boxes\n",
    "    tgt_ids = torch.cat(tgt_cls)\n",
    "    tgt_bbox = torch.cat(tgt_boxes)\n",
    "    \n",
    "    # For each query box in the batch, the output proba of all classes\n",
    "    all_query_probs = pred_logits.flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "    # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "    # but approximate it in 1 - proba[target class].\n",
    "    # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "    # For each query box in the batch, the output prob of the classes of all targets of the batch\n",
    "    cost_class = -all_query_probs[:, tgt_ids] # bs*num_q x tot num targets over batch\n",
    "\n",
    "    # Compute the L1 cost between boxes\n",
    "    out_bbox = pred_boxes.flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "    cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1) # bs*num_q x tot num targets over batch\n",
    "\n",
    "    # Compute the giou cost betwen boxes\n",
    "    out_bbox = box_convert(out_bbox, 'cxcywh', 'xyxy')\n",
    "    tgt_bbox = box_convert(tgt_bbox, 'cxcywh', 'xyxy')\n",
    "    cost_giou = -generalized_box_iou(out_bbox, tgt_bbox)\n",
    "\n",
    "    # Final cost matrix\n",
    "    C = 5 * cost_bbox + cost_class + 2 * cost_giou\n",
    "    B, Q = pred_logits.shape[:2]\n",
    "    C = C.view(B, Q, -1).cpu() # bs x num_q x tot num targets over batch\n",
    "\n",
    "    sizes = [len(bbox) for bbox in tgt_boxes] # num_tgt per img \n",
    "\n",
    "    # Finds the minimum cost detection token/target assignment per img\n",
    "    indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "    int64 = lambda x: torch.as_tensor(x, dtype=torch.int64)\n",
    "    return [(int64(i), int64(j)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ecdce1-f26f-456a-86ba-d79cda74d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_convert, generalized_box_iou\n",
    "import torch.nn as nn\n",
    "\n",
    "class SetCriterion(nn.Module):\n",
    "    def __init__(self, num_classes, eos_coef):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            eos_coef: relative classification weight applied to the no-object category\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eos_coef = eos_coef\n",
    "        self.weight_dict = {\n",
    "            'loss_ce': 1,\n",
    "            'loss_bbox': 5,\n",
    "            'loss_giou': 2\n",
    "        }\n",
    "        empty_weight = torch.ones(self.num_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "    def loss_labels(self, pred_logits, tgt_cls, matches):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            matches: list of batch_size pairs (I, J) of arrays such that output bbox I[n] must be matched with target bbox J[n]\n",
    "        \"\"\"\n",
    "                \n",
    "        # all N tgt labels are reordered following the matches and concatenated  \n",
    "        reordered_labels = [t[J] for t, (_, J) in zip(tgt_cls, matches)]\n",
    "        reordered_labels = torch.cat(reordered_labels) # Nx1\n",
    "        #print(f'{reordered_labels.shape =}')\n",
    "        \n",
    "        # batch_idxs[i] is the idx in the batch of the img to which the i-th elem in the new order corresponds\n",
    "        batch_idxs = [torch.full_like(pred, i) for i, (pred, _) in enumerate(matches)]\n",
    "        batch_idxs = torch.cat(batch_idxs) # Nx1\n",
    "        \n",
    "        # src_idxs[i] is the idx of the preds for img batch_idxs[i] to which the i-th elem in the new order corresponds\n",
    "        pred_idxs = torch.cat([pred for (pred, _) in matches]) # Nx1\n",
    "        \n",
    "        # target_classes is of shape batch_size x num det tokens, and is num_classes (=no_obj) everywhere, except for each token that is matched to a tgt, where it is the label of the matched tgt\n",
    "        target_classes = torch.full(\n",
    "            pred_logits.shape[:2], #BxNdetTok\n",
    "            self.num_classes, #Filled with num_cls\n",
    "            dtype=torch.int64, \n",
    "            device=pred_logits.device\n",
    "        )\n",
    "        target_classes[(batch_idxs, pred_idxs)] = reordered_labels\n",
    "        loss_ce = nn.functional.cross_entropy(\n",
    "            pred_logits.transpose(1, 2), #BxNclsxd1xd2...\n",
    "            target_classes, #Bxd1xd2...\n",
    "            self.empty_weight\n",
    "        )\n",
    "        \n",
    "        ## If we did as follows, then there would be no incentive for the network to output small logits for non-matched tokens\n",
    "        #reordered_pred_logits = pred_logits[(batch_idxs, pred_idxs)] # NxNcls\n",
    "        #other_loss_ce = F.cross_entropy(\n",
    "        #    reordered_pred_logits,\n",
    "        #    reordered_labels\n",
    "        #)\n",
    "        \n",
    "        losses = {'loss_ce': loss_ce}\n",
    "        return losses\n",
    "\n",
    "    def loss_boxes(self, pred_boxes, tgt_boxes, matches, num_boxes):\n",
    "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
    "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
    "        \"\"\"\n",
    "        reordered_target_boxes = [t[i] for t, (_, i) in zip(tgt_boxes, matches)]\n",
    "        reordered_target_boxes = torch.cat(reordered_target_boxes) # Nx4\n",
    "        #print(f'{reordered_target_boxes.shape =}')\n",
    "        \n",
    "        # batch_idxs[i] is the idx in the batch of the img to which the i-th elem in the new order corresponds\n",
    "        batch_idxs = [torch.full_like(pred, i) for i, (pred, _) in enumerate(matches)]\n",
    "        batch_idxs = torch.cat(batch_idxs) # Nx1\n",
    "        \n",
    "        # src_idxs[i] is the idx of the preds for img batch_idxs[i] to which the i-th elem in the new order corresponds\n",
    "        pred_idxs = torch.cat([pred for (pred, _) in matches]) # Nx1\n",
    "        \n",
    "        #print(f'{pred_boxes.shape =}')\n",
    "        reordered_pred_boxes = pred_boxes[(batch_idxs, pred_idxs)] # Nx4\n",
    "        #print(f'{reordered_pred_boxes.shape =}')\n",
    "\n",
    "        losses = {}\n",
    "        loss_bbox = nn.functional.l1_loss(\n",
    "            reordered_pred_boxes,\n",
    "            reordered_target_boxes,\n",
    "            reduction='none'\n",
    "        )\n",
    "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
    "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
    "            box_convert(reordered_pred_boxes, 'cxcywh', 'xyxy'),\n",
    "            box_convert(reordered_target_boxes, 'cxcywh', 'xyxy')))\n",
    "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
    "        return losses\n",
    "\n",
    "    def forward(self, pred_logits, pred_boxes, tgt_cls, tgt_boxes, matches):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "            outputs: dict of tensors, see the output specification of the model for the format\n",
    "            targets: list of dicts, such that len(targets) == batch_size. The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "            matches: list of batch_size pairs (I, J) of arrays such that for pair (I,J) output bbox I[n] must be matched with target bbox J[n]\n",
    "        \"\"\"\n",
    "        # Compute the average (?) number of target boxes accross all nodes, for normalization purposes\n",
    "        #num_boxes = sum(len(boxes) for boxes in tgt_boxes)\n",
    "        #device = next(iter(outputs.values())).device\n",
    "        #num_boxes = torch.as_tensor(\n",
    "        #    [num_boxes],\n",
    "        #    dtype=torch.float,\n",
    "        #    device=device\n",
    "        #)\n",
    "        num_boxes = sum(len(boxes) for boxes in tgt_boxes)\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        losses.update(self.loss_labels(pred_logits, tgt_cls, matches))\n",
    "        losses.update(self.loss_boxes(pred_boxes, tgt_boxes, matches, float(num_boxes)))\n",
    "        losses['combined_loss'] = sum(losses[k] * self.weight_dict[k] for k in losses.keys() if k in self.weight_dict)\n",
    "        return losses   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095dbb98-18bd-4441-bb63-fc35381f7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def post_process(pred_logits, pred_boxes, size):\n",
    "    prob = nn.functional.softmax(pred_logits, -1) # bxNdetTokxNcls\n",
    "    # Most prob cls (except no-obj: NOOBJ is class Ncls ?) and its score per img per token\n",
    "    scores, labels = prob[..., :-1].max(-1) # bxNdetTok\n",
    "    \n",
    "    #pred_boxes = box_convert(pred_boxes, 'cxcywh', 'xyxy')\n",
    "    # and from relative [0, 1] to absolute [0, height] coordinates\n",
    "    #img_h, img_w = target_sizes.unbind(1)\n",
    "    #scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
    "    h,w = size\n",
    "    scale_fct = torch.tensor([w, h, w, h], device=pred_boxes.device)\n",
    "    boxes = pred_boxes * scale_fct[None, None, :]\n",
    "\n",
    "    results = [{'scores': s, 'labels': l, 'boxes': b}\n",
    "               for s, l, b in zip(scores, labels, boxes)]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84fe0f36-e105-4388-9437-ebf86c346111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolos.detector import small, MLP\n",
    "from yolos.misc import NestedTensor, nested_tensor_from_tensor_list\n",
    "from yolos.layers import trunc_normal_\n",
    "\n",
    "class Detector(nn.Module):\n",
    "    def __init__(self, num_classes, pre_trained=None, det_token_num=100, backbone_name='tiny', init_pe_size=[800,1344], mid_pe_size=None, use_checkpoint=False, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        self.det_token_num = det_token_num\n",
    "        self.backbone, hidden_dim = small(pretrained=pre_trained)   \n",
    "        if freeze_backbone:\n",
    "            for n, p in self.backbone.named_parameters():\n",
    "                #print(n)\n",
    "                #if not (n.startswith('det') or n.startswith('class_embed') or n.startswith('bbox_embed')):\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        self.finetune_det(img_size=init_pe_size)\n",
    "        self.class_embed = MLP(hidden_dim, hidden_dim, num_classes + 1, 3)\n",
    "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
    "        \n",
    "    def finetune_det(self, img_size=[800, 1344]):\n",
    "\n",
    "        import math\n",
    "        g = math.pow(self.backbone.pos_embed.size(1) - 1, 0.5)\n",
    "        if int(g) - g != 0:\n",
    "            self.backbone.pos_embed = torch.nn.Parameter(self.backbone.pos_embed[:, 1:, :])\n",
    "\n",
    "        self.backbone.det_token_num = self.det_token_num\n",
    "        self.det_token = nn.Parameter(torch.zeros(1, self.det_token_num, self.backbone.embed_dim))\n",
    "        self.det_token = trunc_normal_(self.det_token, std=.02)\n",
    "        \n",
    "        self.cls_pos_embed = nn.Parameter(self.backbone.pos_embed[:, 0, :][:,None])\n",
    "        \n",
    "        self.det_pos_embed = nn.Parameter(torch.zeros(1, self.det_token_num, self.backbone.embed_dim))\n",
    "        self.det_pos_embed = trunc_normal_(self.det_pos_embed, std=.02)\n",
    "        \n",
    "        patch_pos_embed = self.backbone.pos_embed[:, 1:, :]\n",
    "        patch_pos_embed = patch_pos_embed.transpose(1,2)\n",
    "        B, E, Q = patch_pos_embed.shape\n",
    "        P_H, P_W = self.backbone.img_size[0] // self.backbone.patch_size, self.backbone.img_size[1] // self.backbone.patch_size\n",
    "        patch_pos_embed = patch_pos_embed.view(B, E, P_H, P_W)\n",
    "        H, W = img_size\n",
    "        new_P_H, new_P_W = H//self.backbone.patch_size, W//self.backbone.patch_size\n",
    "        patch_pos_embed = nn.functional.interpolate(patch_pos_embed, size=(new_P_H,new_P_W), mode='bicubic', align_corners=False)\n",
    "        self.patch_pos_embed = nn.Parameter(patch_pos_embed.flatten(2).transpose(1, 2))\n",
    "        \n",
    "        #self.backbone.pos_embed = torch.nn.Parameter(torch.cat((cls_pos_embed, patch_pos_embed, det_pos_embed), dim=1))\n",
    "        \n",
    "        self.backbone.img_size = img_size\n",
    "        self.backbone.has_mid_pe = False\n",
    "        self.backbone.use_checkpoint=False\n",
    "        \n",
    "    def forward_features(self, x):\n",
    "        B, H, W = x.shape[0], x.shape[2], x.shape[3]\n",
    "\n",
    "        x = self.backbone.patch_embed(x)\n",
    "        temp_pos_embed = torch.cat((self.cls_pos_embed, self.patch_pos_embed, self.det_pos_embed), dim=1)\n",
    "        # interpolate init pe\n",
    "        if (temp_pos_embed.shape[1] - 1 - self.det_token_num) != x.shape[1]:\n",
    "            temp_pos_embed = self.backbone.InterpolateInitPosEmbed(temp_pos_embed, img_size=(H,W))\n",
    "\n",
    "        cls_tokens = self.backbone.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        det_token = self.det_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x, det_token), dim=1)\n",
    "        x = x + temp_pos_embed\n",
    "        x = self.backbone.pos_drop(x)\n",
    "\n",
    "        for i in range(len((self.backbone.blocks))):\n",
    "            x = self.backbone.blocks[i](x)\n",
    "        x = self.backbone.norm(x)\n",
    "\n",
    "        return x[:, -self.det_token_num:, :]\n",
    "    \n",
    "    def forward(self, samples: NestedTensor):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        if isinstance(samples, (list, torch.Tensor)):\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "        x = self.forward_features(samples.tensors)\n",
    "        # x = x[:, 1:,:]\n",
    "        outputs_class = self.class_embed(x)\n",
    "        outputs_coord = self.bbox_embed(x).sigmoid()\n",
    "        out = {'pred_logits': outputs_class, 'pred_boxes': outputs_coord}\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab609671-26a6-4c65-975d-7ff2bbf2b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 22806918 params out of 22806918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/src/yolos/backbone.py:423: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrained, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SetCriterion()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Detector(\n",
    "    num_classes=num_classes,\n",
    "    pre_trained='/d/pfournie/YOLOS/deit_small_patch16_224-cd65a155.pth',\n",
    "    det_token_num=100,\n",
    "    backbone_name='small',\n",
    "    init_pe_size=(560,560)\n",
    ")\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "nb_tot = sum([int(torch.numel(p)) for p in model.parameters()])\n",
    "print(f\"Training {n_parameters} params out of {nb_tot}\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = SetCriterion(num_classes, eos_coef=0.1)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962473af-cccc-4875-a7b8-075ae1b67cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_optimizer(model):\n",
    "    if hasattr(model.backbone, 'no_weight_decay'):\n",
    "        skip = model.backbone.no_weight_decay()\n",
    "    head = []\n",
    "    backbone_decay = []\n",
    "    backbone_no_decay = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"backbone\" not in name and param.requires_grad:\n",
    "            head.append(param)\n",
    "        if \"backbone\" in name and param.requires_grad:\n",
    "            if len(param.shape) == 1 or name.endswith(\".bias\") or name.split('.')[-1] in skip:\n",
    "                backbone_no_decay.append(param)\n",
    "            else:\n",
    "                backbone_decay.append(param)\n",
    "    param_dicts = [\n",
    "        {\"params\": head},\n",
    "        {\"params\": backbone_no_decay, \"weight_decay\": 0., \"lr\": 0.0001},\n",
    "        {\"params\": backbone_decay, \"lr\": 0.0001},\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=0.001,\n",
    "                              weight_decay=0.0001)\n",
    "    return optimizer\n",
    "\n",
    "optimizer = build_optimizer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7c09e-9929-4dc5-8f68-d4201a623ed4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset & dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9374da9c-ecba-49c1-97bc-30bf4e820eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/pfournie/dl_toolbox/venv38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pformat\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import gc \n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.transforms import v2 as T\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, RandomSampler\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = read_image(img_path)\n",
    "        mask = read_image(mask_path)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(masks)\n",
    "\n",
    "        # there is only one class: attention différent de fcos, A UNIFORMISER\n",
    "        labels = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = tv_tensors.Image(img)\n",
    "\n",
    "        target = {}\n",
    "        h, w = v2.functional.get_size(img)\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(h,w))\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.Tensor([image_id])\n",
    "        #target[\"area\"] = area\n",
    "        #target[\"iscrowd\"] = iscrowd\n",
    "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "            \n",
    "        return {'image': img, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "from dl_toolbox.transforms import NormalizeBB\n",
    "\n",
    "tf = T.Compose(\n",
    "    [\n",
    "        T.Resize(size=480, max_size=560),\n",
    "        T.RandomCrop(size=(560,560), pad_if_needed=True, fill=0),\n",
    "        T.ToDtype(torch.float, scale=True),\n",
    "        T.SanitizeBoundingBoxes(),\n",
    "        T.ConvertBoundingBoxFormat(format='CXCYWH'),\n",
    "        NormalizeBB(),\n",
    "        #T.ToPureTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "dataset = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "dataset_test = PennFudanDataset('/data/PennFudanPed', tf)\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_set = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "val_set = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def list_of_dicts_to_dict_of_lists(list_of_dicts):\n",
    "    dict_of_lists = defaultdict(list)\n",
    "    for dct in list_of_dicts:\n",
    "        for key, value in dct.items():\n",
    "            dict_of_lists[key].append(value)\n",
    "    res = dict(dict_of_lists)\n",
    "    return res\n",
    "\n",
    "def collate(batch):\n",
    "    batch = list_of_dicts_to_dict_of_lists(batch)\n",
    "    batch['image'] = torch.stack(batch['image'])\n",
    "    return batch\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    dataset=train_set,\n",
    "    sampler=RandomSampler(\n",
    "        train_set,\n",
    "        #replacement=True,\n",
    "        #num_samples=100*2\n",
    "    ),\n",
    "    drop_last=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    dataset=val_set,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41433fa2-d761-48a2-880c-d470c9930003",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer=optimizer,\n",
    "    start_factor=1.,\n",
    "    end_factor=0.01,\n",
    "    total_iters=150*30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa85d72-d0bb-4725-8083-1a9a438170be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unnorm_bounding_boxes(inpt):\n",
    "    bounding_boxes = inpt.as_subclass(torch.Tensor)\n",
    "    in_dtype = bounding_boxes.dtype\n",
    "    bounding_boxes = bounding_boxes.clone() if bounding_boxes.is_floating_point() else bounding_boxes.float()\n",
    "    whwh = torch.Tensor(inpt.canvas_size).repeat(2).flip(dims=(0,)).to(inpt.device) # canvas_size is H,W hence the flip to WHWH\n",
    "    out_boxes = bounding_boxes*whwh \n",
    "    return tv_tensors.wrap(out_boxes.to(in_dtype), like=inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a13c370-b3be-4342-8ab3-7d172f8b3cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▎                                                                                                        | 1/25 [00:00<00:05,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████▊                                                                                       | 5/25 [00:00<00:01, 10.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████████████▌                                                                              | 7/25 [00:00<00:01, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████████████████████████████████▌                                                            | 11/25 [00:00<00:01, 13.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████▊                                           | 15/25 [00:01<00:00, 14.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████████████████████████████████                          | 19/25 [00:01<00:00, 14.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████████████████████████████████████████████████████████████▋                 | 21/25 [00:01<00:00, 14.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:01<00:00, 13.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "valid_loss = 5.998540706634522\n",
      "valid_bbox_loss = 0.6899915099143982\n",
      "valid_giou_loss = 0.9110201692581177\n",
      "valid_ce_loss = 0.7265427708625793\n",
      "{'classes': tensor(0, dtype=torch.int32),\n",
      " 'map': tensor(4.4341e-05),\n",
      " 'map_50': tensor(0.0004),\n",
      " 'map_75': tensor(0.),\n",
      " 'map_large': tensor(4.4341e-05),\n",
      " 'map_medium': tensor(0.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.),\n",
      " 'mar_1': tensor(0.0008),\n",
      " 'mar_10': tensor(0.0030),\n",
      " 'mar_100': tensor(0.0038),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.0043),\n",
      " 'mar_medium': tensor(0.),\n",
      " 'mar_small': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                     | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███▋                                                                                                         | 2/60 [00:00<00:12,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████▎                                                                                                     | 4/60 [00:00<00:09,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████▉                                                                                                  | 6/60 [00:01<00:08,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████████▌                                                                                              | 8/60 [00:01<00:08,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████████                                                                                          | 10/60 [00:01<00:07,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████▌                                                                                      | 12/60 [00:01<00:07,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████████████████████▏                                                                                  | 14/60 [00:02<00:07,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████████████████████▊                                                                               | 16/60 [00:02<00:06,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████▍                                                                           | 18/60 [00:02<00:06,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT INTERPOLATING 1225\n",
      "NOT INTERPOLATING 1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████▍                                                                           | 18/60 [00:03<00:07,  5.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 107\u001b[0m\n\u001b[1;32m    105\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    106\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 107\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m train_bbox_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_bbox\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    109\u001b[0m train_giou_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_giou\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from yolos.box_ops import box_cxcywh_to_xyxy\n",
    "gc.collect()\n",
    "#torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "start_epoch = 0\n",
    "for epoch in range(start_epoch, 150):\n",
    "    time_ep = time.time()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_bbox_loss = 0\n",
    "    valid_giou_loss = 0\n",
    "    valid_ce_loss = 0\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "    with torch.no_grad():\n",
    "        map_metric = MeanAveragePrecision(\n",
    "            box_format='cxcywh', # make sure your dataset outputs target in xywh format\n",
    "            backend='faster_coco_eval'\n",
    "        )\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            image = batch[\"image\"].to(device)\n",
    "            targets = batch['target']\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model(image)\n",
    "            \n",
    "            matches = hungarian_matching(\n",
    "                pred_logits=outputs[\"pred_logits\"],\n",
    "                pred_boxes=outputs[\"pred_boxes\"],\n",
    "                tgt_cls=[tgt[\"labels\"] for tgt in targets],\n",
    "                tgt_boxes=[tgt[\"boxes\"] for tgt in targets]\n",
    "            )\n",
    "            loss_dict = criterion(\n",
    "                pred_logits=outputs[\"pred_logits\"],\n",
    "                pred_boxes=outputs[\"pred_boxes\"],\n",
    "                tgt_cls=[tgt[\"labels\"] for tgt in targets],\n",
    "                tgt_boxes=[tgt[\"boxes\"] for tgt in targets],\n",
    "                matches=matches\n",
    "            )\n",
    "            loss = loss_dict['combined_loss']\n",
    "            \n",
    "            b,c,h,w = image.shape\n",
    "            results = post_process(\n",
    "                pred_logits=outputs[\"pred_logits\"],\n",
    "                pred_boxes=outputs[\"pred_boxes\"],\n",
    "                size=(h,w)\n",
    "            )\n",
    "            for t in targets:\n",
    "                t['boxes'] = unnorm_bounding_boxes(t['boxes'])\n",
    "            map_metric.update(results, targets)\n",
    "            \n",
    "            valid_loss += loss.detach().item()\n",
    "            valid_bbox_loss += loss_dict[\"loss_bbox\"].detach().item()\n",
    "            valid_giou_loss += loss_dict[\"loss_giou\"].detach().item()\n",
    "            valid_ce_loss += loss_dict[\"loss_ce\"].detach().item()\n",
    "        valid_loss /= len(val_dataloader)\n",
    "        valid_bbox_loss /= len(val_dataloader)\n",
    "        valid_giou_loss /= len(val_dataloader)\n",
    "        valid_ce_loss /= len(val_dataloader)\n",
    "        mapmetrics = map_metric.compute()\n",
    "        print(f\"{epoch = }\")\n",
    "        print(f\"{valid_loss = }\")\n",
    "        print(f\"{valid_bbox_loss = }\")\n",
    "        print(f\"{valid_giou_loss = }\")\n",
    "        print(f\"{valid_ce_loss = }\")\n",
    "        print(pformat(mapmetrics))\n",
    "        map_metric.reset()\n",
    "        \n",
    "    train_loss = 0\n",
    "    train_bbox_loss = 0\n",
    "    train_giou_loss = 0\n",
    "    train_ce_loss = 0\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        image = batch[\"image\"].to(device)\n",
    "        outputs = model(image)\n",
    "        targets = batch['target']\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        matches = hungarian_matching(\n",
    "            pred_logits=outputs[\"pred_logits\"],\n",
    "            pred_boxes=outputs[\"pred_boxes\"],\n",
    "            tgt_cls=[tgt[\"labels\"] for tgt in targets],\n",
    "            tgt_boxes=[tgt[\"boxes\"] for tgt in targets]\n",
    "        )\n",
    "        loss_dict = criterion(\n",
    "            pred_logits=outputs[\"pred_logits\"],\n",
    "            pred_boxes=outputs[\"pred_boxes\"],\n",
    "            tgt_cls=[tgt[\"labels\"] for tgt in targets],\n",
    "            tgt_boxes=[tgt[\"boxes\"] for tgt in targets],\n",
    "            matches=matches\n",
    "        )\n",
    "        loss = loss_dict['combined_loss']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        train_loss += loss.detach().item()\n",
    "        train_bbox_loss += loss_dict[\"loss_bbox\"].detach().item()\n",
    "        train_giou_loss += loss_dict[\"loss_giou\"].detach().item()\n",
    "        train_ce_loss += loss_dict[\"loss_ce\"].detach().item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_bbox_loss /= len(train_dataloader)\n",
    "    train_giou_loss /= len(train_dataloader)\n",
    "    train_ce_loss /= len(train_dataloader)\n",
    "    print(f\"{epoch = }\")\n",
    "    #print(f\"lr = {lr_scheduler.get_last_lr()[0]}\"),\n",
    "    print(f\"{train_loss = }\")\n",
    "    print(f\"{train_bbox_loss = }\")\n",
    "    print(f\"{train_giou_loss = }\")\n",
    "    print(f\"{train_ce_loss = }\")\n",
    "    #lr_scheduler.step(epoch)\n",
    "    time_ep = time.time() - time_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea4b18-ae70-4bfd-bbde-4f9f782287f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_toolbox_38",
   "language": "python",
   "name": "dl_toolbox_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
