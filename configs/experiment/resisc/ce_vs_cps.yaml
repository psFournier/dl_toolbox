# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
#  - override /hydra/launcher: my_submitit_slurm
  - override /paths: qdtis056z
  - override /datamodule: resisc_semisup
  - override /module: cps
  - override /module/network@module.network1: efficientnet
  - override /module/network@module.network2: efficientnet
  - override /trainer: gpu
  
name: cps_alpha

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 12345

datamodule:
  batch_size: 32
  num_workers: 32
  prop: 3
  unlabeled_prop: [90, 99]
  train_tf:
    _target_: dl_toolbox.transforms.Compose
    transforms: 
        - _target_: dl_toolbox.transforms.D4
        - _target_: dl_toolbox.transforms.ImagenetNormalize
  val_tf:
    _target_: dl_toolbox.transforms.ImagenetNormalize

module:
  network1:
    weights: IMAGENET1K_V1
  network2:
    weights: IMAGENET1K_V1
  dice_weight: 0
  alpha_ramp:
    start: 10000 # ensures alpha remains constant
  
trainer:
  max_time: "00:02:00:40"
  limit_train_batches: 1.
  limit_val_batches: 1.
  
hydra:
  run:
    dir: ${paths.output_dir}/datamodule:${datamodule.name}/${name}/${now:%Y-%m-%d_%H%M%S}
  sweeper:
    params:
      module.alpha_ramp.start_val: 0,0.1,0.5,1,2,5
  sweep:
    dir: ${paths.output_dir}/datamodule:${datamodule.name}/${name}
    subdir: ${module.alpha_ramp.start_val}/${now:%Y-%m-%d_%H%M%S}
    
callbacks:
  model_checkpoint:
    #dirpath: ${hydra:run.dir}/checkpoints
    dirpath: ${hydra:sweep.dir}/${hydra:sweep.subdir}/checkpoints